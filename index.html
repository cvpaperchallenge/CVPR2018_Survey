<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>cvpaper.challenge</title><link rel="stylesheet" href="css/reveal.css"><link rel="stylesheet" href="css/theme/white.css"><link rel="stylesheet" href="css/layout.css"><link rel="stylesheet" href="lib/css/zenburn.css"></head><script>var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-118576057-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-118576057-1');
</script><body><div class="reveal"><div class="slides"><section id="Modulated_Convolutional_Networks"><div class="paper-abstract"><div class="title">Modulated Convolutional Networks</div><div class="info"><div class="authors">Xiaodi wang , Baochang Zhang</div></div><div class="slide_editor">Kazuki Tsubura</div><div class="item1"><div class="text"><h1>概要</h1><p>・CNNは画像処理の様々なタスクをこなすうえでとても有効だが，ネットワークのストレージにかなりのコストを要求するため，展開が制限される．2値化フィルタを用いたCNNの移植性向上のための新しい変調畳み込みネットワーク(MCNs)を提案する．MCNでは，end-to-endフレームワークにおけるフィルタ損失，中心損失，ソフトマックス損失を考慮した新しい損失関数であるM-フィルタを提案する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Modulated_Convolutional_Networks.png" alt="Modulated_Convolutional_Networks.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・差分</h1><p>・非二項フィルタを復元するために，M-フィルタを導入しネットワークモデルを計算するための新しいアーキテクチャを導出する．MCNは完全精度モデルとは対照的に，畳み込みフィルタの必要な記憶スペースのサイズを32倍に縮小することができ，最先端の2値化モデルよりもはるかに優れた性能を達成した．また，MCNは完全精度のResentsおよびWideResentsと同等のパフォーマンスを達成した．</p></div></div><div class="item4"><div class="text"><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Modulated_Convolutional_Networks_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#1]</div><div class="timestamp">2018.6.20 19:49:11</div></div></section><section id="SplineCNN_Fast_Geometric_Deep_Learning_with_Continuous_B-Spline_Kernels"><div class="paper-abstract"><div class="title">SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels</div><div class="info"><div class="authors">Matthias Fey, Jan Eric Lenssen, Frank Weichert, Heinrich M¨uller</div><div class="conference">CVPR2018</div></div><div class="slide_editor">KazukiTsubura</div><div class="item1"><div class="text"><h1>概要</h1><p>グラフなどの不規則な構造をした幾何学的入力のためのディープニューラルネットワークの変形であるスプラインベースの畳み込みニューラルネットワーク(SplineCNN)．スペクトル領域内でフィルタリングするのではなく，純粋に空間領域で特徴集計をする．SplineCNNを使用することで，手作業による特徴記述子の代わりに入力として幾何学的構造を使用することで，深いアーキテクチャの完全なend-to-endの学習が可能になる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Fast_Geometric_Deep_Learning_with_Continuous_B-Spline_Kernels1.png" alt="Fast_Geometric_Deep_Learning_with_Continuous_B-Spline_Kernels1"><img src="slides/figs/Fast_Geometric_Deep_Learning_with_Continuous_B-Spline_Kernels2.png" alt="Fast_Geometric_Deep_Learning_with_Continuous_B-Spline_Kernels2"></p></div></div><div class="item3"><div class="text"><h1>新規性・差分</h1><p>グラフやmeshesのような不規則な構造をした様々な点で利用でき，空間上における入力の幾何学的関係を発見する．手作業による特徴記述子を使用せずにend-to-endの学習が可能になり，また，最先端の幾何学的な学習と同等である．</p><ul><li><a href="https://arxiv.org/pdf/1711.08920.pdf">論文</a></li><li><a href="https://github.com/rusty1s/pytorch_geometric">Github</a></li></ul></div></div><div class="slide_index">[#2]</div><div class="timestamp">2018.6.18 18:45:52</div></div></section><section id="Learning_and_Using_the_Arrow_of_Time"><div class="paper-abstract"><div class="title">Learning and Using the Arrow of Time</div><div class="info"><div class="authors">Donglai Wei et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>DNN を用いて動画中の時間の流れている方向（Arrow of Time）を学習する研究. 人工的な信号を含むキューは Arrow of Time の学習に悪影響を及ぼすことを示し, それらの影響を取り除いた大規模 dataset を作成した. 評価実験では映画中の逆再生部分を検出するというタスクにおいて人間とほぼ同程度の精度を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Learning-and-Using-the-Arrow-of-Time.png" alt="fukuhara-Learning-and-Using-the-Arrow-of-Time.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Arrow of Time を学習する DNN アーキテクチャとして Temporal Class-Activation Map Network (T-CAM) を提案</li><li>T-CAM は数フレーム分の optical flow を入力から Arrow of Time を推測</li><li>人工的な信号である camera Motion や black framing を含むキューは Arrow of Time の推定を容易にし, ネットワークの学習に悪影響を与えてしまうことを実験により示した</li><li>上記の人工的な信号を取り除いた Arrow of Time を学習するための大規模データセット, Flickr-AoT と Kinetics-AoT を作成</li><li>提案手法を用いて行った映画の逆再生部分を検出する実験では, 人間（80%）とほぼ同等（76%）の結果を達成</li><li>また, Arrow of Time が flow-based の行動認識において self-supervised pre-training に有用であることを示した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://people.csail.mit.edu/donglai/paper/aot18.pdf" target="blank">[論文] Learning and Using the Arrow of Time</a></li><li><a href="https://www.youtube.com/watch?v=1zfZhXkOzCw" target="blank">[動画] YouTube</a></li></ul></div></div><div class="slide_index">[#3]</div><div class="timestamp">2018.5.17 12:19:55</div></div></section><section id="Missing_Slice_Recovery_for_Tensors_Using_a_Low-rank_Model_in_Embedded_Space"><div class="paper-abstract"><div class="title">Missing Slice Recovery for Tensors Using a Low-rank Model in Embedded Space</div><div class="info"><div class="authors">T. Yokota, B. Erem, S. Guler, S.K. Warfield and H. Hontani</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>テンソルがスライス方向に欠けてしまった場合の復元についての論文．このケースでは，よく行われる核ノルム利用やその他正則化手法ではムリ．
遅れ／シフトに不変な構造を捉えることが重要になることから，
「高次元空間への低ランクモデルの埋め込み」を行うことで解決する．
時系列の遅延埋め込みを，テンソルにおける「複数方向遅延埋め込み変換」
を行い，不完全なテンソルを高次不完全ハンケルテンソルへと変換する．
その後，この高次テンソルをタッカー展開の枠組みで低ランク化することで
復元が行われる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/item3image.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>伝統的に行われてきた行列・テンソル解析系の論文．情報学部出身の読者になるべく分かりやすいように丁寧に書いているように見受けられる．
画像で言えば，伝送エラーなどで行の一部分や下半分が吹き飛んでしまった時などに使える復元手法．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>きちんと読み手への導入は行われているものの，読み下すには，テンソル分解程度の数学の知識が必要．ついでに，カオスのような時系列システムも知っているとわかりやすい（図中の説明での事例がそれ）．
まとめ人にとっては数学の復習になったので，ぜひ論文を読んでみていただきたい．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0672.pdf">論文</a></li></ul></div></div><div class="slide_index">[#4]</div><div class="timestamp">2018.6.18 11:01:34</div></div></section><section id="Sim2Real_Viewpoint_Invariant_Visual_Servoing_by_Recurrent_Control"><div class="paper-abstract"><div class="title">Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control</div><div class="info"><div class="authors">Fereshteh Sadeghi et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>ロボットアームを用いたビジュアルサーボについての研究. DNN を用いた視点に依存しないビジュアルサーボの能力を学習する Recurrent Convolutional Neural Network Controller を提案. 様々な視点, 光源環境, 物体の種類や位置に置けるタスクをシミュレーション上で学習することで, 未知の視点において自動でキャリブレーションを行うことが可能. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Sim2Real-Viewpoint-Invariant-Visual-Servoing-by-Recurrent-Control.png" alt="fukuhara-Sim2Real-Viewpoint-Invariant-Visual-Servoing-by-Recurrent-Control.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>コントローラーは目的物体のクエリ画像, 現在の観測画像, 1つ前の行動, 現在の内部状態から次の行動と内部状態を決定する</li><li>LSTM を用いてネットワークが過去の行動の結果を参照できるようにすることで Jacobian (action と motion との関係) についての事前知識無しでの学習を可能とした</li><li>ロス関数にはとった行動によって目的物体との距離がどのように変化したかと, 長期的な行動の価値を学習するための Q-関数 (行動状態価値関数) を用いる</li><li>少数のアノテーション付きシークエンスがあれば, シミュレーション上で学習結果を実際のロボットへ転移することが可能（追加で学習が必要なのは画像特徴の部分のみのため）</li><li>実際のロボットに学習結果を転移して行った評価実験では, 物体へロボットアームを到達させるタスクにおいて, 単一物体の場合は 94.4%, 二つの場合は 70.8% を達成した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.05472" target="blank">[論文] Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control</a></li><li><a href="https://www.youtube.com/watch?v=oLgM2Bnb7fo" target="blank">[動画] YouTube</a></li></ul></div></div><div class="slide_index">[#5]</div><div class="timestamp">2018.6.18 4:18:55</div></div></section><section id="Multi-Oriented_Scene_Text_Detection_via_Corner_Localization_and_Region_Segmentation"><div class="paper-abstract"><div class="title">Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation</div><div class="info"><div class="authors">Pengyuan Lyu, Cong Yao, Wenhao Wu, Shuicheng Yan and Xiang Bai</div><div class="conference">CVPR2018</div><div class="paper_id">982</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>コーナー検出とセグメンテーションを用いた高速かつ高精度なテキスト検出手法．テキスト検出時，ボックスのコーナー点を局所化し，テキスト領域を相対位置でセグメンテーションする．画像を入力すると，DSSDベースのNWで特徴抽出をし，コーナー点検出とコーナー位置に基づくセグメンテーションを出力する．コーナー点はサンプリングおよびグループ化され複数の候補ボックスとなる．セグメンテーション結果とあわせてスコア付けしてNMSする．長いテキストを自然に検出でき，複雑な後処理をする必要もない．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180617_Multi-Oriented_Scene_Text_Detection1.jpg" alt="20180617_Multi-Oriented_Scene_Text_Detection1.jpg"><img src="slides/figs/20180617_Multi-Oriented_Scene_Text_Detection2.jpg" alt="20180617_Multi-Oriented_Scene_Text_Detection2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Deepベースのテキスト検出は，テキストを物体の一種として扱いb-boxの回帰を行うか，テキスト部分を直接抽出する手法である．前者はアスペクト比によっては検出できず，後者は複雑な後処理を必要とする．本手法はその2つを組み合わせて，両者の欠点を補う．SynthText，ICDAR2015，2013，MSRA-TD500，MLTおよびCOCO-Textのデータセットで評価して，ほとんどがSOTAを達成した．とくに，ICDAR2015では84.3%（F-measure），MSRA-TD500では81.5%を達成した．10.4FPSで動作する．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>非常にシンプルながらも高精度なテキスト検出． DSSDのデコーダ部分の特徴マップからセグメンテーションを行う最近よくある手法をテキスト検出に応用している．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1688.pdf">論文</a></li><li><a href="https://arxiv.org/abs/1802.08948">arXiv</a></li></ul></div></div><div class="slide_index">[#6]</div><div class="timestamp">2018.6.17 23:16:27</div></div></section><section id="Low-Latency_Video_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Low-Latency Video Semantic Segmentation</div><div class="info"><div class="authors">Yule Li, et al.</div><div class="paper_id">1804.00389</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>動画によるセマンティックセグメンテーションにおいて、精度を向上させつつ、処理速度を上げる手法の提案。2つのコンポーネントを組み込んだフレームワークで構成している。1つ目は、時間変化に伴って空間的な畳み込み処理を変化させ、特徴を適応させる特徴伝播モジュール。2つ目は、精度予測に基づいて、計算を動的に割り当てるスケジューラ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180617LLVSS.jpg" alt="20180617LLVSS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>動画のセマンティックセグメンテーションには、高スループットやコスト、低遅延などの問題があり、 自律運転などにおいて重要となる。時間的変化に適応させた処理によって精度向上、処理速度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>CityscapesとCamVidにおいて、最新の手法と競合する精度で、遅延を360msから119msに抑えられる結果に。</p><ul><li><a href="https://arxiv.org/pdf/1804.00389.pdf">論文</a></li></ul></div></div><div class="slide_index">[#7]</div></div></section><section id="VirtualHome_Simulating_Household_Activities_via_Programs"><div class="paper-abstract"><div class="title">VirtualHome: Simulating Household Activities via Programs</div><div class="info"><div class="authors">Xavier Puig et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>家の中の環境をシミミュレーションするための仮想環境 VirtualHome を作成した. また, 家の中で典型的に起こる様々な行動を自然言語とプログラムの形式で表現し, それらを仮想環境上でシミミュレーションした動画を組みにした VirtualHome Activity Dataset を公開した. 加えて, LSTM を用いて動画やテキストからプログラム形式の表現を生成する手法を提案した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-VirtualHome-Simulating-Household-Activities-via-Programs.png" alt="fukuhara-VirtualHome-Simulating-Household-Activities-via-Programs.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>VirtualHome には様々な種類の間取りや物体（平均357個）があり, Agent も複数の種類が用意されている</li><li>dataset では家の中で行われる様々な行動に対して, 名前と自然言語形式での行動の説明と行動をプログラムの形式が与えられている</li><li>VirtualHome 上でプログラムをシミュレーションすることで作成された動画には, Agent の姿勢やフロー, 物体のクラスなど様々な情報が与えられている</li><li>LSTM を用いた encoder-decoder 型のネットワークに強化学習を適用し, 動画やテキストからプログラム形式の表現を生成する手法を提案</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.pdf" target="blank">[論文] VirtualHome: Simulating Household Activities via Programs</a></li><li><a href="http://www.virtual-home.org/" target="blank">[Project page] VirtualHome: Simulating Household Activities via Programs</a></li></ul></div></div><div class="slide_index">[#8]</div><div class="timestamp">2018.6.16 16:40:55</div></div></section><section id="Visual_Question_Generation_as_Dual_Task_of_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Visual Question Generation as Dual Task of Visual Question Answering</div><div class="info"><div class="authors">Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang and Xiaogang Wang</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像に関する質問に答えるVisual Question Answering(VQA)と与えられた答えになる質問を作るVisual Question Generation(VQG)を同時に扱うInvertible Question Answering Network(iQAN)を提案した。質問が与えられている場合は答えを、答えが与えられている場合は質問を推定することで学習をする。
その際、2つのタスクを独立した問題ではなく逆問題であると考え、質問と答え及びそれぞれを表現する特徴量間の変換に使用する重みを共有する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Visual_Question_Generation_as_Dual_Task_of_Visual_Question_Answering.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>VQAに関しては、従来手法と比べて精度を向上することが可能となった。また、VQGによって生成した質問と答えのペアをVQAの学習に使用すると精度が向上することが分かり、VQGによってデータ数を増やすことが可能であると結論付けた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://cvboy.com/publication/cvpr2018_iqan/">著者ホームページ</a></li></ul></div></div><div class="slide_index">[#9]</div><div class="timestamp">2018.6.5 00:51:56</div></div></section><section id="Teaching_Categories_to_Human_Learners_with_Visual_Explanations"><div class="paper-abstract"><div class="title">Teaching Categories to Human Learners with Visual Explanations</div><div class="info"><div class="authors">Oisin Mac Aodha, Shihan Su, Yuxin Chen, Pietro Perona and Yisong Yue</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像に写っているもののカテゴリをコンピュータが人間に教えるためのシステムEXPLAINを提案。カテゴリを分類する上でどこに注目すればいいのか(例：蝶の種類を見分けるにはどこに注目すれば良いか）を提示することで人間がカテゴリを学習することを支援する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Teaching_Categories_to_Human_Learners_with_Visual_Explanations.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来の手法ではカテゴリを表すラベルを提示するのみであったが、重要領域を提示することでより効率的に人間が学習することを可能とした。ユーザースタディにより人に学習してもらった内容に関するテストをしたところ、EXPLAINの方が短い時間で高い正答率を出すという結果を得られた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.06924">link1</a></li></ul></div></div><div class="slide_index">[#10]</div><div class="timestamp">2018.6.10 23:58:13</div></div></section><section id="Face_Aging_with_Identity-Preserved_Conditional_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Face Aging with Identity-Preserved Conditional Generative Adversarial Networks</div><div class="info"><div class="authors">Zongwei Wang, Xu Tang, Weixin Luo and Shenghua Gao</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人間の年齢変化顔を合成するIdentity-Preserved Conditional Generative Adversarial Networks (IPCGANs)を提案。合成画像が満たすべき特徴を、(1)目的の年齢に近づいている(2)変化前の人物と同一人物か(3)リアルな画像かの3つとした。
(1)(2)については、Generatorによって生成した画像を年齢推定及び同一人物性を評価するネットワークによって評価する。
(3)はDiscriminatorにリアルかどうかを判定させることで最適化を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Face_Aging_with_Identity-Preserved_Conditional_Generative_Adversarial_Networks.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ユーザースタディにより、Image Quality, Age Classification, Face Verificationの3つの観点を評価し、DNNベースの手法と比較してFace VerificationとImage Qualityの2つの観点で高い評価を得た。VGG-faceによりinception scoreを求め、比較対象の手法より高いスコアを得た。
また、計算時間についても劇的に良化した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Face_Aging_With_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#11]</div><div class="timestamp">2018.6.15 18:52:34</div></div></section><section id="Emotional_Attention_A_Study_of_Image_Sentiment_and_Visual_Attention"><div class="paper-abstract"><div class="title">Emotional Attention: A Study of Image Sentiment and Visual Attention</div><div class="info"><div class="authors">Shaojing Fan, Zhiqi Shen, Ming Jiang, Bryan L. Koenig, Juan Xu, Mohan S. Kankanhalli and Qi Zhao</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像に潜んでいる感情と注目を集める領域の関連を調査した。アイトラッキングのデータと、画像中に写っている感情に関連する物体(笑顔など)をアノテーションしたEMOtional attention dataset(EMOd)を構築した。
また、画像中の注目領域を抽出するDNNモデルであるCASNetを提案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Emotional_Attention_A_Study_of_Image_Sentiment_and_Visual_Attention.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>EMOdを用いて分析した結果、感情に関連する物体の方が人々の視線を集めることが判明した。その中でも、人間が関連する（笑顔など）場合がより視線を集めることが分かった。
従来のSaliencyを求める手法よりもCASNetの方が多くの指標で高いスコアを獲得した。
また、感情に関連する物体の方がより注目を集めるという結果を出力したことからEMOdの分析結果を反映していることを確認した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://nus-sesame.top/emotionalattention/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#12]</div><div class="timestamp">2018.6.5 12:28:58</div></div></section><section id="Categorizing_Concepts_with_Basic_Level_for_Vision-to-Language"><div class="paper-abstract"><div class="title">Categorizing Concepts with Basic Level for Vision-to-Language</div><div class="info"><div class="authors">Hanzhang Wang, Hanli Wang and Kaisheng Xu</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Vision and Languageのタスクに、Cognition分野で提唱されているbasic levelという概念を基にしたBasic Concept(BaC)を導入した。basic levelとは人間が幼少期に行う抽象化であり、本研究では物体のクラスを類似したもの同士を1つにまとめる。
始めに、MSCOCOのキャプションとImageNetのクラスをマッチングすることで、Salient Concept(SaC)というBaCに候補を決定する。
続いて、物体のクラス分類におけるConfusion Matrixを求め、混同されるクラス同士を1つにまとめることでBaCを決定する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Categorizing_Concepts_with_Basic_Level_for_Vision-to-Language.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Vision and Languageのタスクとして、Image CaptioningとVQAによって検証を行った。Image Captioningについては、ベースラインと比較してほとんどの指標において精度が向上し、向上しなかった指標についてもベースラインと大差ない数値を記録した。
VQAについては、ObjectとLocationについて精度の向上を確認した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Categorizing_Concepts_With_CVPR_2018_paper.html">論文</a></li></ul></div></div><div class="slide_index">[#13]</div><div class="timestamp">2018.6.15 16:40:44</div></div></section><section id="Multi-Level_Fusion_Based_3D_Object_Detection_From_Monocular_Images"><div class="paper-abstract"><div class="title">Multi-Level Fusion Based 3D Object Detection From Monocular Images</div><div class="info"><div class="authors">Bin Xu et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>一枚のRGB画像から３次元物体認識を行う研究. region-based な２次元の物体検出器を３次元に拡張する一般的なフレームワークを提案し, end-to-end のネットワークで２次元と３次元の物体位置と物体のクラスを同時に推定することが可能. KITTI dataset を用いた評価実験では state-of-the-art の結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Multi-Level-Fusion-Based-3D-Object-Detection-From-Monocular-Images.png" alt="fukuhara-Multi-Level-Fusion-Based-3D-Object-Detection-From-Monocular-Images.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>end-to-end のネットワークで単一のRGB画像から物体のクラスと２次元, ３次元の物体位置, ３次元の物体の方向などを同時に推定</li><li>RGB画像に MonoDepth を用いて推定した Depth 画像を連結したものを CNN に入力し, Faster-RCNN と同様の方法で Region Proposal を生成</li><li>また, Depth 画像から Point Cloud (XYZ Map)を推定</li><li>上記の２つを連結したものを全結合層に通して, 物体位置と物体のクラスの推定を行う</li><li>KITTI dataset を用いた評価実験では　Mono3D, 3DOP, Deep3DBox などと比較して優位な結果を達成した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2380.pdf" target="blank">[論文] Multi-Level Fusion Based 3D Object Detection From Monocular Images</a></li></ul></div></div><div class="slide_index">[#14]</div><div class="timestamp">2018.6.15 1:01:55</div></div></section><section id="Conditional_Probability_Models_for_Deep_Image_Compression"><div class="paper-abstract"><div class="title">Conditional Probability Models for Deep Image Compression</div><div class="info"><div class="authors">Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, Luc Van Gool</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像復元の問題は復元エラー（distortion）とエントロピー（rate）とのトレードオフであるが、本論文ではこのトレードオフをできる限り解消し、画像圧縮を行うAutoEncoderを提案する。著者らはコンテキストモデルから直接的に潜在表現のエントロピーを復元するモデルを考案して同問題に取り組んだ。AutoEncoderには条件付き確率モデルを学習した3D-CNNを適用。実験ではSSIMを用いて従来の畳み込みによるAutoEncoderモデルよりも良好な精度を実現した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180614DeepImageCompression.png" alt="180614DeepImageCompression"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>3D-CNNにより条件付き学率モデルを学習したAutoEncoderモデルを考案したことが新規性であり、JPEG(2000)などよりも良い圧縮法であることを示し、Rippel&Bourdevらのモデルと同等レベルの精度を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>画像圧縮、超解像の違いがいまいちよくわからなくなってきた。評価方法の違い？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mentzer_Conditional_Probability_Models_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://relational.github.io/">著者</a></li></ul></div></div><div class="slide_index">[#15]</div><div class="timestamp">2018.6.14 08:55:34</div></div></section><section id="Improved_Lossy_Image_Compression_With_Priming_and_Spatially_Adaptive_Bit_Rates_for_Recurrent_Networks"><div class="paper-abstract"><div class="title">Improved Lossy Image Compression With Priming and Spatially Adaptive Bit Rates for Recurrent Networks</div><div class="info"><div class="authors">Nick Johnston, Damien Vincent, David Minnen, Michele Covell, Saurabh Singh, Troy Chinen, Sung Jin Hwang, Joel Shor, George Toderici</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Recurrent/Convolutional Neural Networks（RNN/CNN）を用いた非可逆画像圧縮の手法を提案し、BPG(4:2:0), WebP, JPEG2000, JPEGよりも性能のよいものを提案した。3つの改善、(1)ニューラルネットにより空間的分散を効果的に捉えて情報量の劣化を防ぐ、(2)エントロピーコーディングの上に空間適応的ビット配置アルゴリズムを適用して効率的な画像圧縮とする、(3)SSIMによりピクセルごとの損失を計算して最適化することで圧縮数値を改善する、を加えて圧縮方法を提案。KodakやTecnickのカメラを用いてコーデックの評価を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180614SpatiallyAdaptiveBitRates.png" alt="180614SpatiallyAdaptiveBitRates"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来の圧縮方法であるBPG(4:2:0), WebP, JPEG2000, JPEGなどよりも効率の良い圧縮方法を提案した。また、手法的にもCNN/RNNを応用し、さらに後処理として画質を改善するSpatially Adaptive Bit Rate (SABR)を提案したことが評価された。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>（数十年前からある問題という意味で）過去の問題と現在の手法が合わさって新規性を出している論文。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Johnston_Improved_Lossy_Image_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#16]</div><div class="timestamp">2018.6.14 08:27:59</div></div></section><section id="Deep_Density_Clustering_of_Unconstrained_Faces"><div class="paper-abstract"><div class="title">Deep Density Clustering of Unconstrained Faces</div><div class="info"><div class="authors">Wei-An Lin, Jun-Cheng Chen, Carlos D. Castillo, Rama Chellappa</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>unconstrainedな顔に対してクラスタリングを行うDeep Density Clustering(DDC)を提案。顔画像をDNNによって単位超級面空間に射影する。続いて、各サンプル2点の類似度を測定する際に、
その2点の近傍に位置するサンプルを考慮することでクラスタの密度を推定することが可能となるため、これに基づいてクラスタリングを行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Density_Clustering_of_Unconstrained_Faces.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>YTF, LFW, IJB-Bデータセットを使用して評価。それぞれのデータセットには同一人物の画像が複数枚もつ。</li><li>評価指標はBCubed precision、Bcubed F-measure、NMIで評価。 </li><li>提案手法と同等の精度を持つ既存手法のJULE、DEPICTはクラスタ数を指定する必要があるが、提案手法ではクラスタ数を指定する必要がない。</li><li>クラスタリングの際の閾値の変更に対して、既存手法に比べてクラスタ数の変動が小さい。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3208.pdf">論文</a></li><li> <a href=" http://openaccess.thecvf.com/content_cvpr_2018/Supplemental/3208-supp.pdf">Supplementary material</a></li></ul></div></div><div class="slide_index">[#17]</div></div></section><section id="Pose-Guided_Photorealistic_Face_Rotation"><div class="paper-abstract"><div class="title">Pose-Guided Photorealistic Face Rotation</div><div class="info"><div class="authors">Yibo Hu, Xiang Wu, Bing Yu, Ran He, Zhenan Sun</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>入力顔画像に対して任意の画像を生成するネットワークを提案。顔向きのコンディションとしてランドマークのヒートマップを与え、U-Netによって画像を生成し、2つのdiscriminatorを用いることで画像を生成。
1つ目のdiscriminatorは入力画像をコンディションとして生成画像or正解画像を識別し、
2つ目のdiscriminatorはランドマークのヒートマップをコンディションとして生成画像or正解画像を識別する。
また人物IDを保存するためにLight CNNによる特徴量によるロスをとる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Pose-Guided_Photorealistic_Face_Rotation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ランドマークのヒートマップ、2つのdiscriminator、IDを保存するロスを用いて入力顔画像を任意の向きに回転させた画像を生成。</li><li>337IDそれぞれに対して20の照明環境と15種類の顔向きをもつMulti-PIEで検証。</li><li>トレーニングには使用していないLFWで画像を生成したところ、既存手法による画像よりも見た目の良い画像が得られた。</li><li>face verification、face recognitionにおいてSoTAを達成。</li><li>ablation studyの結果、IDのロスがface recognitionに最も影響が高いことを確認。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>既存手法のように顔向きの角度を使うのではなくヒートマップを与えることでU-netの学習がしやすい、という上手い方法。</li><li>IDのロスに使用する特徴量が最後のFC層に加えてプーリング層からも取得されておりIDについてはMS-Celeb-1Mでプリトレインした後Multi-PIEへとファインチューニングしているなど、かなり微調整を感じる論文。</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1097.pdf">論文</a></li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/Supplemental/1097-supp.pdf">Supplementary material</a></li></ul></div></div><div class="slide_index">[#18]</div></div></section><section id="Unsupervised_Training_for_3D_Morphable_Model_Regression"><div class="paper-abstract"><div class="title">Unsupervised Training for 3D Morphable Model Regression</div><div class="info"><div class="authors">Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, Daniel Vlasic, William T. Freeman</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>それぞれ単独の実画像データセットと3D Morphable Model(3DMM)データセットを使用し、画像から3DMMを生成する手法を提案。トレーニングには実画像データセットVGG-Face、3DMMデータセットBasel Face 3DMMを使用。
IDが保たれることを念頭にネットワークを構築。Batch Distribution Lossでは、
Basel Face 3DMMのパラメタ分布が平均０、標準偏差1のガウス分布であるため、
実画像によって生成される3DMMのシェイプ、テクスチャパラメタがどちらも平均0、標準偏差1となるようにロスをとる。
Loopback Lossは画像/生成された3DMMのdecoderによる特徴量の差分を取り、よりリアルな3DMMかつ、
より現実的な3DMMパラメタを得ることを目的としている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Unsupervised_Training_for_3D_Morphable_Model_Regression.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>画像、3DMMの対応がないデータセットを用いて、教師なしで画像から3DMMを生成する手法を提案。</li><li>Batch Distribution Loss、Loopback Loss、Multi-view Identity Lossを学習することで教師なしであることを緩和している。</li><li>MICC Florence 3D Faceデータセットで検証し、Mean error、Faceクラスタリング、Earth mover’s distanceによる実画像と生成3DMMの顔類似度のそれぞれにおいてSoTA。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>Basel Face 3DMMのパラメタ分布が平均０、標準偏差1のガウス分布という仮定はどこから来ている？</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1349.pdf">論文</a></li></ul></div></div><div class="slide_index">[#19]</div></div></section><section id="Aligning_Infinite-Dimensional_Covariance_Matrices_in_Reproducing_Kernel_Hilbert_Spaces_for_Domain_Adaptation"><div class="paper-abstract"><div class="title">Aligning Infinite-Dimensional Covariance Matrices in Reproducing Kernel Hilbert Spaces for Domain Adaptation</div><div class="info"><div class="authors">Zhen Zhang, Mianzhi Wang, Yan Huang, Arye Nehorai</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ソースドメイン(SD)とターゲットドメイン(TD)のそれぞれのreproducing kernel Hilbert space(RKHS)における共分散を最適化することでdomain adaptation(DA)を行う手法。
既存のカーネルベースのDAはSDとTDのRKHS上の統計的分布の類似度に大きく依存することに着目。
共分散を最適化する方法としてkernel whitening-coloring map(KWC)とkernel optimal 
transport map(KOT)があり、これをRKHS上で計算で可能なように式変形を行うことでDAを行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Aligning_Infinite-Dimensional_Covariance_Matrices_in_Reproducing_Kernel_Hilbert_Spaces_for_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>SDとTDのRKHS上の共分散を最適化することでDAを行う。</li><li>複数のDAのベンチマークデータセットにおいてKWC、KOTのいずれかがSoTAを達成。</li><li>SoTAと比較して実行時間が短く、KWCは4分の１、KOTは10分の1程度。</li><li>Out-of-Sampleによる推定においてもSoTAを達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>248パターンのDAを検証しており、本論文に載っていたのは34パターン</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3383.pdf">論文</a></li><li> <a href="http://openaccess.thecvf.com/content_cvpr_2018/Supplemental/3383-supp.pdf">Supplementary material</a></li></ul></div></div><div class="slide_index">[#20]</div></div></section><section id="Cross-Dataset_Adaptation_for_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Cross-Dataset Adaptation for Visual Question Answering</div><div class="info"><div class="authors">Wei-Lun Chao, Hexiang Hu, Fei Sha</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>VQAのデータセットにおけるバイアスを調査した上で、VQAにおけるdomain adaptation(DA)を提案。提案手法では選択肢の中から解答を選択するVQAを扱う。VQAデータセットは画像、質問、解答選択肢＝正解＋誤答の要素からなる。
それぞれの要素を組み合わせた入力を用いて、その入力がどのデータセットに所属しているのかを調査した結果、
画像はほぼ無相関であることがわかり、質問と解答によってデータセット間にバイアスが生じていることを確認。
この結果に基づき、以下のようにDAを提案。ターゲットドメイン(TD)に質問/解答選択肢のみがある場合、
ソースドメイン(SD)の質問/正解(誤答は任意性があるため使用しない)の特徴量が持つ分布とTDの質問のDNNによる
特徴量が持つ分布のJensen-shannon Divergence(JSD)が小さくなるように学習。TDが質問と正解(＋誤答)を持つ場合、
SDが持つ質問・正解の特徴量分布とTDの質問・正解のDNNによる特徴料が持つJSDが小さくなるように学習。
さらにSDで事前学習を行った質問-正解識別をTDでfine-tuningを行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Cross-Dataset_Adaptation_for_Visual_Question_Answering.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>事前実験より与える情報によって、入力データがどちらのデータセットに所属しているかの識別率の変化を確認。画像、質問、正解解答、解答群(正解+不正解)を与え、与える要素を増やすほど識別率が高くなった。
この結果から、データセットによってバイアスがあることを確認。</li><li>質問に対する正答率を複数のデータセットにおいて既存手法であるADDA、CORALと比較した結果SoTAを達成。TDが解答選択肢のみ、質問と正解を持つ場合において高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集 </h1><ul><li>TDの正解、誤答のみを使用し質問を使用せずにDAを行った方が高い状況がいくつも確認できる。これはつまり質問と解答の相関がすでにSDで学習できており、SDの質問がノイズになってしまっているとを示唆している。</li><li>VQAをDAしてみた、という実験的な論文であり比較している手法もDAのベンチマークの手法なので、まだまだ新規性を出すことができそう。</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/4249.pdf">論文</a></li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/Supplemental/4249-supp.pdf">Supplementary material</a></li></ul></div></div><div class="slide_index">[#21]</div></div></section><section id="Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Monocular_Video_Using_3D_Geometric_Constraints"><div class="paper-abstract"><div class="title">Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints</div><div class="info"><div class="authors">Reza Mahjourian et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>教師なし学習で単眼の動画から Depth と Ego-Motion の推定を行う研究. 連続するフレーム間における 3D Geometry の一貫性を教師信号の代わりに利用して学習を行う. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuahra-Unsupervised-Learning-of-Depth-and-Ego-Motion-from-Monocular-Video-Using-3D-Geometric-Constraints.png" alt="fukuahra-Unsupervised-Learning-of-Depth-and-Ego-Motion-from-Monocular-Video-Using-3D-Geometric-Constraints.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>連続するフレーム間における 3D Geometry の一貫性を用いることで, 教師なし学習で単眼の動画から Depth と Ego-Motion の推定を行うことを可能とした</li><li>連続するフレームから推定された Point Cloud に対して Iterative Closest Point (ICP) を計算し, その Residual と Transform の大きさを 3D Loss として課す</li><li>3D Loss に加えて推定された Depth の滑らかさと, 推定結果を用いて復元した画像の誤差 (2種類) も Loss として課す</li><li>KITTI dataset と mobile phone カメラで撮影した動画を用いて行った評価実験では　Trajectory と Depth の両方において先行研究よりも優位な結果を達成した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.05522" target="blank">[論文] Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints</a></li></ul></div></div><div class="slide_index">[#22]</div><div class="timestamp">2018.5.28 18:59:55</div></div></section><section id="A_Network_Architecture_for_Point_Cloud_Classification_via_Automatic_Depth_Images_Generation"><div class="paper-abstract"><div class="title">A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation</div><div class="info"><div class="authors">RiccardoRoveri et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Point Cloud データのクラス分類についての研究. 順序不定の 3D Point Cloud データを 2D Depth 画像に変換し, ResNet でクラス分類を行う. 評価実験では PointNet より優位な結果となった.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-A-Network-Architecture-for-Point-Cloud-Classification-via-Automatic-Depth-Images-Generation.png" alt="fukuhara-A-Network-Architecture-for-Point-Cloud-Classification-via-Automatic-Depth-Images-Generation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Network は３つのモジュールで構成されており, joint training が可能</li><li>１つ目のモジュールは PointNet を用いて PointCloud から有用な view direction を推定する</li><li>２つ目のモジュールは Gausiaan Interporation （Roveri＋18 の拡張版）によって推定された view direction からの Depth 画像を生成する</li><li>３つ目のモジュールは ResNet50 を用いて Depth 画像から Image Based Classification を行う</li><li>ModelNet40 benchmark を用いて行った shape のクラス分類の評価実験では instance-based accuracy と class average accuracy の両方で PointNet よりも優位な結果となった</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3120.pdf" target="blank">[論文] A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation</a></li><li>3D の問題を既によく研究されている 2D 画像のクラス分類へと帰着させることで, 既存の強力な手法を用いる戦略</li></ul></div></div><div class="slide_index">[#23]</div><div class="timestamp">2018.6.13 5:54:55</div></div></section><section id="GraphBit_Bitwise_Interaction_Mining_via_Deep_Reinforcement_Learning"><div class="paper-abstract"><div class="title">GraphBit: Bitwise Interaction Mining via Deep Reinforcement Learning</div><div class="info"><div class="authors">Yueqi Duan et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Deep binary descriptor においてバイナリを生成する際に0と1の境界に位置する曖昧なビット (ambiguous bit) の問題に取り組んだ研究. 強化学習によって学習したビット間の implicit な関係性を付加することで曖昧性を緩和する GraphBit を提案. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-GraphBit-Bitwise-Interaction-Mining-via-Deep-Reinforcement-Learning.png" alt="fukuhara-GraphBit-Bitwise-Interaction-Mining-via-Deep-Reinforcement-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Binary descriptor における曖昧なビット (ambiguous bit) の問題を緩和するためにビット間の関係性を付加した GraphBit を提案</li><li>CNNからの出力された正規化された特徴量（binary descriptor）に対して Grpah 構造を付加する</li><li>ビット間の相互関係をマイニングする過程をマルコフ過程として定式化し, 強化学習（Policy Gradient）で学習</li><li>State は現在の Graph の構造</li><li>Atction は GraphBit に新しいエッジを１つ追加するか, 既存のエッジを１つ削除</li><li>Reward は t ステップと t+1 ステップにおけるロス関数の減少度合いから計算</li><li>CIFAR-10, Brown, HPatches dataset を用いた評価実験では mean average precision (mAP) の評価尺度でそれぞれ平均 9.64%, 8.84%, 3.22% の精度の向上を達成した </li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Duan_GraphBit_Bitwise_Interaction_CVPR_2018_paper.pdf" target="blank">[論文] GraphBit: Bitwise Interaction Mining via Deep Reinforcement Learning</a></li></ul></div></div><div class="slide_index">[#24]</div><div class="timestamp">2018.6.13 2:43:55</div></div></section><section id="Deep_Progressive_Reinforcement_Learning_for_Skeleton-based_Action_Recognition"><div class="paper-abstract"><div class="title">Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition</div><div class="info"><div class="authors">Yansong Tan et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Skeleton-based action recognition の研究. 強化学習によって与えられた動画から最適な keyframe の組を選択する frame distillation network (FDNet) と graph-based convolution によって keyframe の skeleton 情報から行動認識を行う Graph-based CNN (GCNN) を提案. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Deep-Progressive-Reinforcement-Learning-for-Skeleton-based-Action-Recognition.png" alt="fukuhara-Deep-Progressive-Reinforcement-Learning-for-Skeleton-based-Action-Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>与えられた動画のシークエンスから最適な keyframe の組を選択する過程をマルコフ過程として定式化し, 強化学習 (policy gradient) を適用した</li><li>State として Skeleton 動画全体と現在選択されてる keyframe の組の情報を使用</li><li>Action は各 keyframe を1フレーム前後にずらすか, そのままかの３つ</li><li>Reward は学習済みの GCNN を用いて計算</li><li>また, keyframe から行動認識を行う際は gggraph-based convolution を用いることによって人間の関節の依存関係を考慮している</li><li>NTU, SYSU, UT dataset を用いて評価実験では state-of-the-art とほぼ同等か, 優位な結果を示した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1736.pdf" target="blank">[論文] Deep Progressive Reinforcement Learning for Skeleton-based Action Recognition</a></li></ul></div></div><div class="slide_index">[#25]</div><div class="timestamp">2018.6.12 13:53:55</div></div></section><section id="Learning_Superpixels_with_Segmentation-Aware_Affinity_Loss"><div class="paper-abstract"><div class="title">Learning Superpixels with Segmentation-Aware Affinity Loss</div><div class="info"><div class="authors">Wei-Chih Tu, Ming-Yu Liu, Varun Jampani, Deqing Sun, Shao-Yi Chien, Ming-Hsuan Yang, Jan Kautz</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>superpixel segmentationのためにピクセルの類似性(pixel affinities)を学習するdeep learningベースの手法を提案。pixel affinitiesが同一物体に属する2つの隣接画素の尤度を測る。これまで、groundtruthがないこと、superpixelsのインデックスが交換可能であること、superpixelsの手法は微分不可であることからdeep learningベースのsuperpixelアルゴリズムは試みられていなかった。論文では、segmentation誤差から類似性を学習するsegmentation-aware loss(SEAL)と、pixel affinitiesを出力するPixel Affinity Net(PAN)を提案し、superpixelsとdeep learningを統合する。既存の手法より物体境界を保持したままsuperpixelsを計算することが可能になった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Superpixels_with_Segmentation_Aware_Affinity_Loss.PNG" alt="Learning_Superpixels_with_Segmentation_Aware_Affinity_Loss.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>superpixels + deep learningが新しい。実験では単純なpretrained modelによる特徴量や、edge検出によるsuperpixelsとの統合はうまくいかないことを示している。手法に関しては、superpixelsを直接出力するのではなく、pixel affinitiesを計算、graph-basedのアルゴリズム(ERS)を経由し出力、そしてSEALを計算する。これにより、pixel affinitiesを出力するPANへ誤差を逆伝播することができる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>より効果的に細部の情報をsuperpixelsとして保持することができるため、semantic segmentationの改善や計算量の削減につながるだろう。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Tu_Learning_Superpixels_With_CVPR_2018_paper.pdf">paper</a></li><li><a href="https://sites.google.com/site/wctu1009/cvpr18_superpixel">proj_page</a></li></ul></div></div><div class="slide_index">[#26]</div><div class="timestamp">2018.6.12 12:14:08</div></div></section><section id="Generating_Synthetic_X-ray_Images_of_a_Person_from_the_Surface_Geometry"><div class="paper-abstract"><div class="title">Generating Synthetic X-ray Images of a Person from the Surface Geometry</div><div class="info"><div class="authors">Brian Teixeira, Vivek Singh, Terrence Chen, Kai Ma, Birgi Tamersoy, Yifan Wu, Elena Balashova and Dorin Comaniciu</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>人間の三次元輪郭形状から，見えない体の内側を解析してしまおうという話．本論文では，X線画像を生成する．
さらに，X線画像はパラメタライズしておくことで，体のキーポイントの調節によるマニピュレーションも可能．</p><p>構造的には，2つのネットワークからなる．(1)部分画像といくつかのパラメータから，画像全体を生成するように学習，
(2)全体画像が得られるような(1)のパラメータの推定．
これら2つのネットワークを，一貫性が出てくるように反復的に学習させる．</p><p>生成した画像を使ってみて，画像補間に使ってみた．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Generating_Synthetic_X-ray_Images_of_a_Person_from_the_Surface_Geometry.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>体表面を計測しておくなどして，体表面形状のデータがあれば，X線画像をある程度任意に生成できる．逆に，体表面形状をいじることでそれに対応したX線画像も作れる．
学習データとして活用することができる可能性がある．</p><p>構造はGAN風だが，いい感じに変形している感じがウケているかもしれない．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>この時点での一番の貢献は，それっぽいX線画像が自動生成できる事だろう．<a href="http://smpl.is.tue.mpg.de/">SMPL</a>と組み合わせていろいろやることを想定しているだろうか．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0500.pdf">論文</a></li></ul></div></div><div class="slide_index">[#27]</div><div class="timestamp">2018.6.12 10:10:47</div></div></section><section id="Fully_Convolutional_Adaptation_Networks_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Fully Convolutional Adaptation Networks for Semantic Segmentation</div><div class="info"><div class="authors">Yiheng Zhang, Zhaofan Qiu, Ting Yao, Dong Liu, Tao Mei</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>スタイル特徴量を用いて画像の見た目を変換するネットワークとドメイン間で不変な特徴量を得るネットワークを用いて、domain adaptationを行うことで教師無しでセマンティックセグメンテーションを行うFully Convolutional 
Adaptation Networks (FCAN)を提案。画像の見た目を変換するAppearance Adaptation Networks (AAN)では
ホワイトノイズから画像を生成し、ソースドメインの特徴量マップ、ターゲットドメインのもつ<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf">スタイル特徴量</a>が小さくなるように学習を行うことで、画像をもう一方のドメインの見た目になるように変換する。
ドメイン間で不変な特徴量を得るRepresentation Adaptation Networks (RAN)ではsemantic classificationと、
それぞれのドメインにから得られた特徴量マップに対するadversarial lossと、
ASPPによって得られた特徴量マップに対してピクセルごとにadversarial lossを適用。
ドメインとして実画像とゲーム画像で検証している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Fully_Convolutional_Adaptation_Networks_for_Semantic_Segmentation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>style transferと同様の考え方でドメイン間の画像変換を行いsemantic classification、特徴量マップ、dilated convolutional layerから得られた特徴量マップに対する各ピクセルに対してadversarial lossをとることで教師無しでセマンティックセグメンテーションを行う。</li><li>GTA5とcity spaceを用いて、セマンティックセグメンテーションの精度をstate-of-the-artと比較した結果、19クラスのうち17クラスで最も高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1761.pdf">論文</a></li></ul></div></div><div class="slide_index">[#28]</div></div></section><section id="Re-weighted_Adversarial_Adaptation_Network_for_Unsupervised_Domain_Adaptation"><div class="paper-abstract"><div class="title">Re-weighted Adversarial Adaptation Network for Unsupervised Domain Adaptation</div><div class="info"><div class="authors">Qingchao Chen, Yang Liu, Zhaowen Wang, Ian Wassell, Kevin Chetty</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>Unsupervised Domain Adaptationを行うため、ドメイン間の特徴量分布を一致させるoptimal transportベースのEM distanceを導入し、ターゲットドメイン(T)のラベル分布をソースドメイン(S)のラベル分布に対してラベルごとに重み付けした分布で表現する手法を提案。
domain discriminatorをOTベースのEM distanceをロス関数とすることでドメイン間の特徴量分布を近づける。
一方でベイズの定理より、ドメイン間のラベルの事前分布と特徴量の事後分布は比例関係にありラベルは低次元かつ離散的であるので
ドメイン間で類似度が高いと仮定し、Tにおけるラベルの事前分布をSのラベルの事前分布の重みを変更したもので表す。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Re-weighted_Adversarial_Adaptation_Network_for_Unsupervised_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ドメイン間で特徴量分布をOTベースのEM distanceの学習で、Tのラベル分布をSのラベル分布の重みを変更したもので表現することで、それぞれのdomain shiftを解消する手法を提案。</li><li>手書き文字データセットMNIST、USPS、SVHN、MINST-Mデータセット、19のラベルを持つ実画像、デプス画像のドメインを持つNYU-Dデータセットで検証。state-of-the-artと比較した結果、多くの状況で最も高い精度を達成。</li><li>Sのラベル分布の重みの変更による有効性、ラベルごとの特徴量が分離できているかどうかも議論している。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1224.pdf">論文</a></li></ul></div></div><div class="slide_index">[#29]</div></div></section><section id="Unsupervised_Deep_Generative_Adversarial_Hashing_Network"><div class="paper-abstract"><div class="title">Unsupervised Deep Generative Adversarial Hashing Network</div><div class="info"><div class="authors">Kamran Ghasedi Dizaji, Feng Zheng, Najmeh Sadoughi, Yanhua Yang, Cheng Deng, Heng Huang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>教師無しで画像をバイナリに符号化するハッシュ関数であるHashGANを提案。ハッシュ関数が満たすべき条件は画像が変換されて同じハッシュ値を返すこと、異なる画像には異なるハッシュ値を与えることである。
既存の教師無しハッシュ関数は過学習のために精度がよくなかった。提案手法であるHashGANはgenerator、discriminator、
encoderからなる。学習はGAN loss、encoderによって生成されるハッシュ値のエントロピーが小さくなるように、
出現するハッシュ値が同じになるように、画像の変換によるハッシュ値が不変となるように、画像ごとのハッシュ値が固有となるように、
合成画像をエンコードした際のハッシュ値のL2ロス、実画像と合成画像を入力とした際のdiscriminatorの最後の層に対して
feature matchingを行う。またdiscriminatorはデータ固有の情報を識別し、encoderはデータ固有の情報を抽出しようとするため、
両者の目的が一致しているのでパラメタを共有して学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Unsupervised_Deep_Generative_Adversarial_Hashing_Network.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>GAN、discriminatorとパラメタを共有しているencoder、ハッシュ関数が満たすべきロス関数を導入したHashGANを提案。</li><li>image retrieval、image clusteringで手法の優位性を検討。image retrievalでは既存のunsupervised hash functionとの比較を行い、最も高い精度を達成。image clusteringではstate-of-the-artと同等の精度を達成。</li><li>ablation testにより、特にadversarial loss, feture matching, L2ロス、画像変換によるハッシュの不変性の考慮の影響が大きいことがわかった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>教師無し学習でもタスク特化の手法であり、ハッシュ関数の性質をよく考察した上でモデルを設計している。</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0909.pdf">論文</a></li></ul></div></div><div class="slide_index">[#30]</div></div></section><section id="Supervision-by-Registration_An_Unsupervised_Approach_to_Improve_the_Precision_of_Facial_Landmark_Detectors"><div class="paper-abstract"><div class="title">Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors</div><div class="info"><div class="authors">Xuanyi Dong, Shoou-I Yu, Xinshuo Weng, Shih-En Wei, Yi Yang, Yaser Sheikh,</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ランドマークのGT有り顔画像とラベルなし顔動画を用いて、現在フレームに対して直接推定されたランドマークと、トラッキングによって前フレームから推定されたランドマークの位置の誤差を学習することで顔画像に対してランドマークを推定する手法を提案。
人間によるランドマークのアノテーションは正確でないため、この誤差が学習や推定精度に影響を与えてしまう。
これに対して本論文ではランドマークの推定器に最適化によって計算されるオプティカルフローを教師情報として与える
Supervision by Registration(SBR)を提案。ランドマーク位置を推定するCNNに対して、
Lukas-Kanade法によるトラッキング結果とランドマークの推定位置が同じになるように学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Supervision-by-Registration_An_Unsupervised_Approach_to_Improve_the_Precision_of_Facial_Landmark_Detectors.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>人間のアノテーションよりも、より正確であるオプティカルフローを教師情報として使用することで顔画像に対するランドマークの推定手法を提案。</li><li>300-W、AFLWにおいてランドマーク推定手法であるCPMのアルゴリズムをSBRで学習させると、SBRを使用しない場合よりも精度が向上。</li><li>動画に対するランドマーク推定はstate-of-the-artに及ばなかった。ターゲットとなる人物をデータセットに含んでおくPersonalized Adaptation Modeling(PAM)を行うことで、state-of-the-artと同等の精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>画像のランドマークを推定するために動画から得られるオプティカルフローを使用する、という発想の飛躍が面白い！最適化による正確な教師情報とCNNによる合わせ技。</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0191.pdf">論文</a></li></ul></div></div><div class="slide_index">[#31]</div></div></section><section id="EnvironmentUpgrade_Reinforcement_Learning_for_Non-differentiable_Multi-stage_Pipelines"><div class="paper-abstract"><div class="title">Environment Upgrade Reinforcement Learning for Non-differentiable Multi-stage Pipelines</div><div class="info"><div class="authors">Shuqin Xie et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>微分不可能な multi-stage pipline において joint optimization を可能にする environment upgrade reinforcement learning (EU-RL) を提案. ２段階の Instance segmentation と pose estimation のタスクで評価実験を行い, どちらも優位な結果を示した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Environment-Upgrade-Reinforcement-Learning-for-Non-differentiable-Multi-stage-Pipelines.png" alt="fukuhara-Environment-Upgrade-Reinforcement-Learning-for-Non-differentiable-Multi-stage-Pipelines.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>微分不可能な multi-stage pipline の学習において問題であった上流への feedback が出来ないという点と end-to-end な最適化が出来ない点に取り組んだ研究</li><li>強化学習の agent が下流の出力を受けて上流の出力に変更を与える, environment upgrade reinforcement learning (EU-RL) を提案</li><li>強化学習の手法として actor-critic を Temporal Difference　(TD) learning で学習</li><li>State として１段階目（例えば物体認識）からの出力と２段階目からの出力（例えば semantic segmentation）を使用</li><li>Action として１段階目からの出力結果を変更する操作の集合を使用（物体認識ならBounding Boxの位置の変更やスケールなど）</li><li>Reward は２段目の出力の精度の向上度合いによって計算</li><li>Instance segmentation と pose estimation のタスクで評価実験を行い, どちらも優位な結果を示した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/1643.html" target="blank">[論文] Environment Upgrade Reinforcement Learning for Non-differentiable Multi-stage Pipelines</a></li><li>強化学習の応用先としても, アイデアとしても面白い. 今回の論文では２段階の pipeline についてのみ議論が行われていたが, 今後は３段以上の pipeline でも同様の議論が行われていく？</li></ul></div></div><div class="slide_index">[#32]</div><div class="timestamp">2018.5.11 23:32:55</div></div></section><section id="Deep_Reinforcement_Learning_of_Region_Proposal_Networks_for_Object_Detection"><div class="paper-abstract"><div class="title">Deep Reinforcement Learning of Region Proposal Networks for Object Detection</div><div class="info"><div class="authors">Aleksis Pirinen and Cristian Sminchisescu</div><div class="conference">CVPR2018</div><div class="paper_id">872</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Region proposal network（RPN）と深層強化学習（DRL）を組み合わせたdrl-RPNを提案する．通常のRPNがRoIを貪欲に選択するのに対し，DRLで学習されたsequential attention mechanismを用いて選択することで，最終検出タスクに最適化される．また，時間経過とともにクラス固有の特徴を蓄積し，分類スコアに良い影響を与えて検出精度が高めることを示す．また，学習をいつ停止するか自動的に判断する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180611_drlRPN.jpg" alt="20180611_drlRPN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>RPNにDRLを導入して，attentionに即したRoIを選択できるようにした．VOC2007を用いた評価では，通常のRPNがmAP74.2%なのに対し，drl-RPNは76.4%を達成した．MSCOCOでも各指標・各セットで数%の精度向上が見られた．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>またまた高精度なRoIを検出するタイプの手法．ついにRLまで使うことになった．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Pirinen_Deep_Reinforcement_Learning_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#33]</div><div class="timestamp">2018.6.11 23:18:20</div></div></section><section id="A_Closer_Look_at_Spatiotemporal_Convolutions_for_Action_Recognition"><div class="paper-abstract"><div class="title">A Closer Look at Spatiotemporal Convolutions for Action Recognition</div><div class="info"><div class="authors">Du Tran et al.</div><div class="conference">CVPR2018</div><div class="paper_id">1711.11248</div></div><div class="slide_editor">Takumu Ikeya</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>動画解析のための時空間畳み込みの各手法が行動解析に及ぼす影響を調査した．</li><li>Residual learningのフレームワークでは3D CNNsが2D CNNsよりも精度において優れていることを実験的に示した．</li><li>3D Convolution filterを空間と時間へ分割することで精度が向上することを示した．</li><li>新たな時空間畳み込みブロックの構造として”R(2+1)D”を提案した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Closer_Look_at_Spatiotemporal_Convolutions_for_Action_Recognition.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新規の畳み込みブロックとして時空間の畳み込みブロックを時間と空間に分割する"R(2+1)D"を提案した．</li><li>"R(2+1)D"はSports-1M，Kinetics,UCF101,HMDB51のデータセットでSOTAを達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1711.11248.pdf">論文</a></li></ul></div></div><div class="slide_index">[#34]</div><div class="timestamp">2018.6.11 19:39:34</div></div></section><section id="GeoNet_Geometric_Neural_Network_for_Joint_Depth_and_Surface_Normal_Estimation"><div class="paper-abstract"><div class="title">GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation</div><div class="info"><div class="authors">Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasu andJiaya Jia</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>単眼の画像から深さ(depth)と表面の法線マップ(surface normal maps)を同時に予測する幾何ニューラルネットワーク(GeoNet)を提案．NYU v2 dataset、ではGeoNetが幾何学的に一貫した深度マップと法線マップを予測できることを確認．surface normal maps推定でSOTA、また既存のdepth推定方法と同等の精度を達成．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180609_GeoNet1.png" alt="1"><img src="slides/figs/20180609_GeoNet2.png" alt="2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>GeoNetは2つのストリームのCNNの上に構築されており、depthとsurface normal maps間の幾何学的な関係を構築．これによってdepthとsurface normal mapsを効率的に予測するための基礎となるモデルを構築し，高い一貫性と一致精度を達成することが可能．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://xjqi.github.io/">著者</a></li><li><a href="http://www.cs.toronto.edu/~rjliao/papers/CVPR_2018_GeoNet.pdf">論文</a></li></ul></div></div><div class="slide_index">[#35]</div><div class="timestamp">2018.6.9 13:54:32</div></div></section><section id="MiCT_Mixed_3D2D_Convolutional_Tube_for_Human_Action_Recognition"><div class="paper-abstract"><div class="title">MiCT: Mixed 3D/2D Convolutional Tube for Human Action Recognition</div><div class="info"><div class="authors">Yizhou Zhou, Xiaoyan Sun, Zheng-Jun Zha and Wenjun Zeng</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>2D CNNと3D CNNの畳み込みモジュールを統合した行動認識のためのネットワークMixed Convolutional Tube(MiCT)を提案．3つの有名なベンチマークデータセット(UCF101，Sport1M，HMDB-51)においてMiCT-Netが元の3D CNNのみの手法より著しく優れていることを確認．UCF101とHMDB51での行動認識でSOTAの手法と比較し、MiCT-Netは最高の性能を発揮．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180609_MiCTNet1.png" alt="1"><img src="slides/figs/20180609_MiCTNet2.png" alt="2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>2D CNNにおける手法を十分にリスペクトし，3D Convと融合した新規のネットワークを構築</li><li>MiCT-Netによって時空間融合の各ラウンドにおける学習の複雑さを軽減しつつ、より深くより有益な特徴マップを生成可能</li><li>UCF101とHMDB51においてSOTA<img src="slides/figs/20180609_MiCTNet3.png" alt="3"></li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://www.microsoft.com/en-us/research/uploads/prod/2018/05/Zhou_MiCT_Mixed_3D2D_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#36]</div><div class="timestamp">2018.6.9 15:14:28</div></div></section><section id="Jerk-Aware_Video_Acceleration_Magnification"><div class="paper-abstract"><div class="title">Jerk-Aware Video Acceleration Magnification</div><div class="info"><div class="authors">Shoichiro Takeda, Kazuki Okami, Dan Mikami, Megumi Isogai and Hideaki Kimata</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>高速で大きな動きに対して加速度法の出力を頑健にするための、ジャーク(振動，ぶれ)の新規利用方法について言及．微小な変化は時間的スケールでの高速な大きな動きよりも滑らかであるという観点・観測に基づき、高速で大きな動きの下でのみ微妙な変化を通過させるジャークフィルタを設計．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180609_jerkVAM1.png" alt="1"><img src="slides/figs/20180609_jerkVAM3.png" alt="3"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ジャークフィルタを加速度法に適用することで、最先端のものより優れた結果を確認．<img src="slides/figs/20180609_jerkVAM2.png" alt="2"></p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2767.pdf">link1</a></li></ul></div></div><div class="slide_index">[#37]</div><div class="timestamp">2018.6.9 17:37:53</div></div></section><section id="Recurrent_Pixel_Embedding_for_Instance_Grouping"><div class="paper-abstract"><div class="title">Recurrent Pixel Embedding for Instance Grouping</div><div class="info"><div class="authors">Shu Kong, Charless Fowlkes</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>Instance segmentationのような画素単位のグループ分け問題を行うEnd-to-Endで学習可能な枠組みを提案。同じグループの画素はcosine similarityが高くなるように、異なるグループはmargin以下の値になるように超球面上に回帰(Spherical Embedding Module)し、そこでRNNによるMean-shift clusteringを実行すること(Recurrent Grouping Module)で実現。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Recurrent_Pixel_Embedding_for_Instance_Grouping.PNG" alt="Recurrent_Pixel_Embedding_for_Instance_Grouping.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>既存のregion proposalやbboxによる組み合わせたinstance segmentationの手法とは大きく異なり新しい。またこれをRNNでMean-shift clusteringを表現することで実現し、End-to-Endな学習を可能としている。加えてhyperparameterの設定に関する理論的分析も提供。instance segmentationやsemantic segmentationだけでなく、様々なpixel-levelのドメインタスクへ応用可能。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>手法もシンプルでかつ効果的で応用先も広い。Fig.11の結果からsemantic segmentationにおいてもinstanceの情報が効果的に利用できそうで試してみたい。</p><ul><li><a href="https://arxiv.org/abs/1712.08273">arxiv</a></li><li><a href="https://www.ics.uci.edu/~skong2/SMMMSG.html">project_page</a></li><li><a href="https://github.com/aimerykong/Recurrent-Pixel-Embedding-for-Instance-Grouping">GitHub</a></li></ul></div></div><div class="slide_index">[#38]</div><div class="timestamp">2018.6.11 07:49:04</div></div></section><section id="Learning_a_Discriminative_Feature_Network_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Learning a Discriminative Feature Network for Semantic Segmentation</div><div class="info"><div class="authors">Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>Semantic Segmentationにおけるintra-class inconsistencyとinter-class indistinctionの問題を、Discriminative Feature Network(DFN)によって対処。intra-class inconsistencyは図の牛の一部を馬と誤認識するような現象。inter-class indistinctionは、図のコンピュータのように外見が似ている対象の区別することが難しい現象。前者の問題をmulti-scaleかつglobal contextな情報を抽出するChannel Attention Block(CAB)を持つSmooth Networkにより、後者の問題をbottom-upなBorder Networkにより緩和する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_a_Discriminative_Feature_Network_for_Semantic_Segmentation.PNG" alt="Learning_a_Discriminative_Feature_Network_for_Semantic_Segmentation.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Semantic Segmentationをpixel単位のラベル付けだけではなく、物体の1つのカテゴリに対して一貫したセマンティックラベル付けをするタスクとして考えた。それゆえのBorder Networkと考える。上記の2つの問題は、必要な情報が異なるゆえ、対処の仕方をCABとU-Net構造に似たSmooth NetworkとBottom-upなBorder Networkとうまく分解している。PASCAL VOC 2012でmean IoU 86.2%、Cityscapesで80.3%を達成。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>実験で各モジュールの効果を検証していたが何が効いているのかよくわからない。直感的にはBorder NetworkとSmooth Networkの分離は良いアイデアと感じたが、この分離による効果は1%未満。</p><ul><li><a href="https://arxiv.org/abs/1804.09337">arxiv</a></li></ul></div></div><div class="slide_index">[#39]</div><div class="timestamp">2018.6.11 07:32:22</div></div></section><section id="SemStyle_Learning_to_Generate_Stylised_Image_Captions_using_Unaligned_Text"><div class="paper-abstract"><div class="title">SemStyle: Learning to Generate Stylised Image Captions using Unaligned Text</div><div class="info"><div class="authors">A.Mathew, L.Xie and X.He</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1805.07030</div></div><div class="slide_editor">Kota Yoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>書面上のコミニュケーションをする上で文書のスタイルは魅力と明快さに影響する．同一の画像からスタイルの異なるキャプションを生成するという研究．様々なスタイルの単語の選択肢とは異なる構文をもつ文章をデコードするための統一された言語モデルを開発した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SemStyle_Learning_to_Generate_Stylised_Image_Captions_using_Unaligned_Text.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Semanticな用語を用いて文章の柔軟性を備えたキャプションの生成</li><li>スタイルと記述両方のコーパスを用いて文章レベルのスタイルを模倣するための学習</li><li>SemStyleのキャプションが画像の意味を保持し、記述的で、スタイルもシフトできていることを示した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>連続する写真からより豊富なキャプションを生成できる可能性を秘める</li><li><a href="https://arxiv.org/abs/1805.07030">Paper</a></li></ul></div></div><div class="slide_index">[#40]</div><div class="timestamp">2018.6.10 14:13:49</div></div></section><section id="Reinforcement_Cutting-Agent_Learning_for_Video_Object_Segmentation"><div class="paper-abstract"><div class="title">Reinforcement Cutting-Agent Learning for Video Object Segmentation</div><div class="info"><div class="authors">Junwei Han et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Video Object Segmentation (VOS) を強化学習によって行う研究. Object Segmentation では主に物体の領域とそれらの(周辺との)関係性が重要であるという推量に基づいて, VOS をマルコフ過程として定式化し, Deep Q-Learning を適用した. 評価実験では, state-of-the-art とほぼ同等の結果を達成した. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Reinforcement-Cutting-Agent-Learning-for-Video-Object-Segmentation.png" alt="fukuhara-Reinforcement-Cutting-Agent-Learning-for-Video-Object-Segmentation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Video Object Segmentation (VOS) をマルコフ過程 (MDP) として定式化した </li><li>State は動画の現在のフレームの特徴量と過去 k（論文では k=4） フレーム分の action のヒストリーを使用</li><li>Action は object searching (9次元) と context embedding (3次元) を使用</li><li>Reward は ground truth のマスクと推定されたマスクの IoU の差で評価</li><li>強化学習は Deep Q-Learning (DQN) を使用</li><li>DAVIS dataset と YouTube-Objects dataset を用いた評価実験では, state-of-the-art とほぼ同等の結果を達成した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/3342.html" target="blank">[論文] Reinforcement Cutting-Agent Learning for Video Object Segmentation</a></li><li> <a href="https://davischallenge.org/" target="blank">[Dataset] DAVIS dataset</a></li><li><a href="https://data.vision.ee.ethz.ch/cvl/youtube-objects/" target="blank">[Dataset] YouTube-Objects dataset</a></li><li>Future work として同様の手法が　Semantic Segmentation, Object Localization, Saliency Estimation, 3D Shape Learning などに適用できる可能性を示唆</li></ul></div></div><div class="slide_index">[#41]</div><div class="timestamp">2018.6.9 17:29:55</div></div></section><section id="SeedNet_Automatic_Seed_Generation_with_Deep_Reinforcement_Learning_for_Robust_Interactive_Segmentation"><div class="paper-abstract"><div class="title">SeedNet: Automatic Seed Generation with Deep Reinforcement Learning for Robust Interactive Segmentation</div><div class="info"><div class="authors">Gwangmo Song et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>インタラクティブセグメンテーションに強化学習を適用した研究. 入力画像と初期 seed から自動で新しい seed を順次生成する SeedNet を提案. 評価実験では state-of-the-art の結果を達成すると共に, 教師あり手法と比較しても優位な結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-SeedNet-Automatic-Seed-Generation-with-Deep-Reinforcement-Learning-for-Robust-Interactive-Segmentation.png" alt="fukuhara-SeedNet-Automatic-Seed-Generation-with-Deep-Reinforcement-Learning-for-Robust-Interactive-Segmentation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Interactive Segmentation のタスクをマルコフ過程として定式化し, 強化学習（Deep Q-Learning）を用いて学習を行った</li><li>State には入力画像の画素情報と seed の位置とラベル,　mask 画像を用いる (seed の位置を state に陽に加えることによって, 生成される mask が seed 位置の変化についてロバストになるらしい)</li><li>Action は state の情報から新しい seed の位置とラベルの決定（自由度を削減するために 20x20 のグリッド上から位置を選択, seed の数が10点になった段階で終了）</li><li>Reward は生成された Mask と Ground Truth の Mask の IoU（exp 型を提案）に加えて, SeedNet によって追加された新 seed のラベルと位置が適切かの２点を考慮して決定</li><li>MSRA10K dataset を用いた評価実験では state-of-the-art の結果に加えて, 初期の seed 位置についてロバストであることが確認された</li><li>また, 教師あり学習を用いた手法 [Long+15], [Xu+16] と比較しても優位性が確認された</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_SeedNet_Automatic_Seed_CVPR_2018_paper.pdf" target="blank">[論文] SeedNet: Automatic Seed Generation with Deep Reinforcement Learning for Robust Interactive Segmentation</a></li><li>強化学習を新タスクに適用してみました系列の論文</li><li>他の同系列の論文に見られる傾向と同じく, MDPによる定式化と Reward の計算方法を主な貢献としている</li><li>特に本論文は, 教師ありでは学習するのが難しい問題を上手く見つけている（seed の打ち方は user によって千差万別なのでトレーニングデータを作るのが難しい）</li></ul></div></div><div class="slide_index">[#42]</div><div class="timestamp">2018.6.10 21:50:55</div></div></section><section id="Adversarial_Complementary_Learning_for_Weakly_Supervised_Object_Localization"><div class="paper-abstract"><div class="title">Adversarial Complementary Learning for Weakly Supervised Object Localization</div><div class="info"><div class="authors">Xiaolin Zhang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>弱教師ありの Object Localization の研究. 2つの Classifier を並列に配置し, 片方の classifier で注目された領域を他方の入力から取り除いておくことで, それぞれが異なる領域に反応するような構造となっている. 評価実験では ILSVRC dataset の localization　のタスクで 45.15% (new state-of-the-art) の誤差率を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Adversarial-Complementary-Learning-for-Weakly-Supervised-Object-Localization.png" alt="fukuhara-Adversarial-Complementary-Learning-for-Weakly-Supervised-Object-Localization.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>全結合層の最後に畳み込み層を1つ追加することで, CAM [Zhou＋16] と同等の object localization maps を事後処理無しで得られることを数式で示した</li><li>画像から畳み込み層によって抽出した特徴量を, 並列に配置した　classifier に入力する</li><li>片方の classifier から出力された object localization map で注目されていた領域を消去したものを, 他方の入力とすることで両方の classifier を異なる領域に反応させる</li><li>ILSVRC dataset 等を用いて行った評価実験では Localization と Classification の両タスクにおいて, state-of-the-art [Zhou+16, Singh+17] と同等か優位な結果を達成した</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.06962" target="blank">[論文] Adversarial Complementary Learning for Weakly Supervised Object Localization</a></li></ul></div></div><div class="slide_index">[#43]</div><div class="timestamp">2018.6.9 00:32:55</div></div></section><section id="Feature_Selective_Networks_for_Object_Detection"><div class="paper-abstract"><div class="title">Feature Selective Networks for Object Detection</div><div class="info"><div class="authors">Yao Zhai, Jingjing Fu, Yan Lu, Houqiang Li</div><div class="conference">CVPR2018</div><div class="paper_id">538</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体検出時に用いるRegion-of-Interest（RoI）を，sub-regionとアスペクト比の差を用いて再構成するFeature selective netsを提案．画像全体に対してsub-regionのattention bank（すべてのattention mapを記憶するbank）とアスペクト比のattention bankを生成する．Attention mapはbankから選択的にpoolされ，RoIの改善に使用される．処理の手順は(1)CNNから得られた特徴マップをRPNに入力しRoIを得て，(2)特徴マップのチャンネル数を削減してRoIプーリングを行い，圧縮されたRoI特徴を得る．(3)削減される前のRoIをregion-wise attention生成モジュールに入力する．特徴マップを用いてアスペクト比attention bankとsub-region attention bankを得る．(4)各bankにselective RoIプーリングを行う．そして，(2)と(4)で得られたRoI特徴と各attention mapを結合して検出サブネットワークに入力する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180610_FeatureSelectiveNets1.jpg" alt="20180610_FeatureSelectiveNets1.jpg"><img src="slides/figs/20180610_FeatureSelectiveNets2.jpg" alt="20180610_FeatureSelectiveNets2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>RoIをattentinを用いて補正する．VGGだけではなくGoogLeNetやResNetにも適用可能である．VOC2007を用いた評価では，mAP: 82.9%, 76.8%, 74.3% （Res101, GoogLe, VGG-16）を達成し，Faster R-CNNの78.8%, 74.8%, 73.2%（上記と同順）よりも高精度である．さらに，検出サブネットワークをシンプルにしているため，Faster R-CNNよりも高速な検出が可能である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Attentionを用いた物体検出が増えてきている．Mask R-CNNみたいにRoIに注目する手法も多い？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2123.pdf">論文</a></li><li><a href="https://arxiv.org/abs/1711.08879">arXiv</a></li></ul></div></div><div class="slide_index">[#44]</div><div class="timestamp">2018.6.10 23:42:54</div></div></section><section id="Pseudo_Mask_Augmented_Object_Detection"><div class="paper-abstract"><div class="title">Pseudo Mask Augmented Object Detection</div><div class="info"><div class="authors">Xiangyun Zhao, Shuang Liang, Yichen Wei</div><div class="conference">CVPR2018</div><div class="paper_id">530</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Bounding boxでの物体検出でグラフカットを用いて擬似的なマスク（セグメンテーション）のrefinementを行う．インスタンスセグメンテーションの学習を行うことで擬似的な物体マスクを推定できるようにネットワークパラメータを最適化する．フレームワークは検出ネットワークと擬似的なマスクのrefinementを行うグラフカットベースのモジュールからなる．RoIを入力として，ベースネットワークの特徴マップからインスタンスセグメンテーションを行い，それをグラフカットモジュールに入力して擬似的なマスクを得る．インスタンスセグメンテーションの結果はbounding boxの修正にも用いられる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180610_PseudoMaskAug1.jpg" alt="20180610_PseudoMaskAug1.jpg"><img src="slides/figs/20180610_PseudoMaskAug2.jpg" alt="20180610_PseudoMaskAug2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>流行りの物体検出＋セグメンテーションの手法．マスクを単に特徴マップから得て終わりではなく，グラフカットでrefineする部分は新しいところ．グラフカットを数iter行うことで，よりきれいなマスクを得ることができる．VOC2007/2012を用いた物体検出の精度はmAP74.4%（VGG-16）で，Faster R-CNN（70.4%）やHyperNet（71.4）よりも良い．VOC2012SDSを用いたセグメンテーションの精度は58.5/67.6(マスクレベルスコア/物体検出スコア)%であり，iterを繰り返すことで精度が向上することが確認されている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>セグメンテーションタスクの精度向上のためグラフカットでマスクのrefineを繰り返し行うのは面白いと思った．Iter0とiter3でマスクの結果を比較するとかなりきれいになっている．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Pseudo_Mask_Augmented_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://arxiv.org/abs/1803.05858">arXiv</a></li></ul></div></div><div class="slide_index">[#45]</div><div class="timestamp">2018.6.10 21:35:27</div></div></section><section id="Scalable_Dense_Non-Rigid_Structure-From-Motion_A_Grassmannian_Perspective"><div class="paper-abstract"><div class="title">Scalable Dense Non-Rigid Structure-From-Motion: A Grassmannian Perspective</div><div class="info"><div class="authors">Suryansh Kumar, Anoop Cherian, Yuchao Dai, Hongdong Li</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>複数画像を使用した非剛体のSfM (Non-Rigid Structure-from-Motion)に関する研究である。右図は非剛体の表面形状復元結果の一例であり、顔のように時系列的に変化する形状を、多様体の概念をSfMに導入することにより問題解決を図っている。非剛体の形状変化を、空間的・時間的な部分空間としてすいていすることでSfMを実行する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180610NRSfM.png" alt="180610NRSfM"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>非剛体物体の表面形状復元に関するSfM問題を、グラスマン多様体（Grassman Manifold）の問題と捉えて解決している点が新規性として挙げられる。柔軟に表面形状復元ができている様子は動画にて確認可能である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>DynamicFusionからこの手の問題は出て来たのだが、どのような違いがある/どのように展開されているのか？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Kumar_Scalable_Dense_Non-Rigid_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://suryanshkumar.github.io/">著者</a></li><li><a href="https://www.youtube.com/watch?v=KSLrDnOI2m4">YouTube</a></li></ul></div></div><div class="slide_index">[#46]</div><div class="timestamp">2018.6.10 16:43:01</div></div></section><section id="A_Papier-Mch_Approach_to_Learning_3D_Surface_Generation"><div class="paper-abstract"><div class="title">A Papier-Mâché Approach to Learning 3D Surface Generation</div><div class="info"><div class="authors">Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>2次元画像、もしくは3次元点群からメッシュや分解構造を生成し、テクスチャありのメッシュや3次元プリント物体を出力する。この枠組みはAtlasNetと呼ばれ、同タスクのPrecision向上と一般化の面で性能改善を行い、3次元形状を集めたデータベースであるShapeNet上で形状をAuto-Encoding、単眼画像からの形状復元を行った。その他、AtlasNetを用いてモーフィング、パラメトライゼーション、超解像、形状マッチング、共セグメンテーションを実施した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180610AtlasNet.png" alt="180610AtlasNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>3D表面形状生成器であるAtlasNetを構築したことが最も大きな新規性である。形状に関するパラメータを学習可能にした。さらに、AtlasNetをGitHub上で公開して使用できる形式にしている。復元したメッシュ形状も、提案手法がもっともノイズが少なく、良好な復元結果となった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>数年前は型崩れの多い3次元形状を出力するGeneratorであったが、徐々によくなりつつある。この研究もまだ過程にしか過ぎない？</p><ul><li><a href="https://arxiv.org/pdf/1802.05384.pdf">論文</a></li><li><a href="http://imagine.enpc.fr/~groueixt/atlasnet/">Project</a></li><li><a href="https://github.com/ThibaultGROUEIX/AtlasNet">GitHub</a></li></ul></div></div><div class="slide_index">[#47]</div><div class="timestamp">2018.6.10 16:03:52</div></div></section><section id="Improving_Occlusion_and_Hard_Negative_Handling_for_Single-Stage_Pedestrian_Detectors"><div class="paper-abstract"><div class="title">Improving Occlusion and Hard Negative Handling for Single-Stage Pedestrian Detectors</div><div class="info"><div class="authors">Junhyug Noh, et al.</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>歩行者検出におけるオクルージョンやハードネガティブを改善するための提案。本提案手法は、シングルステージ物体検出手法に適応可能。オクルージョン処理のために、ベースモデルの出力テンソルを更新してパートスコアを推定し、オクルージョン認識スコアを算出する。ハードネガティブの混同を軽減するために、 average grid classifiersをpost-refinement classifiersとして導入。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180690IOHN.jpg" alt="20180690IOHN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>SqueezeDetやYOLOv2、SSD、DSSDを含むシングルステージ物体検出手法に適応でき、オクルージョンやハードネガティブを改善する。本論文では歩行者検出におけるオクルージョンにフォーカスを当てているが、一般物体検出にも適応できる可能性がある。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>CaltechPedestrianとCityPersonsデータセットで評価。4つのモデルのパフォーマンス向上を確認。重度のオクルージョン設定において、最良のパフォーマンス。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0620.pdf">論文</a></li></ul></div></div><div class="slide_index">[#48]</div></div></section><section id="Iterative_Learning_with_Open-set_Noisy_Labels"><div class="paper-abstract"><div class="title">Iterative Learning with Open-set Noisy Labels</div><div class="info"><div class="authors">Yisen Wang, et al.</div><div class="paper_id">1804.00092</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ノイズのあるラベルを含んだデータセットを使い、CNN学習を高精度に行うための新しい反復学習フレームワークの提案。反復的なノイズラベル検出、特徴学習、および再重み付けの3段階のフレームワークでノイズの多いラベルを検出しつつ、識別器を反復的に学習。再重みづけでは、クリーンなラベルの学習を重視し、ノイズの場合には低減させる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180690NoisyLabels.jpg" alt="20180690NoisyLabels.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>綺麗なラベルアノテーション付き大規模データセットによる学習は非常に重要だが、人の手間がかなりかかる他、ヒューマンエラーを含む可能性が否めない。本研究では、あえてノイジーなデータセットに挑戦することで、これらの問題を解決する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>データセットの収集コストや信頼性の問題に伴って、自ら良いデータを選択して学習する需要が高まっている印象。</p><ul><li><a href="https://arxiv.org/pdf/1804.00092.pdf">論文</a></li></ul></div></div><div class="slide_index">[#49]</div></div></section><section id="Hand_PointNet_3D_Hand_Pose_Estimation_using_Point_Sets"><div class="paper-abstract"><div class="title">Hand PointNet: 3D Hand Pose Estimation using Point Sets</div><div class="info"><div class="authors">Liuhao Ge, et al.</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>正規化されたポイントクラウドを入力として、複雑な手構造を捕捉し、手の姿勢の低次元表現を正確に回帰させることができるHand PointNetの提案。Oriented Bboxでポイントクラウドを正規化し、ネットワーク入力をよりロバストにする。その後、階層的なPointNetに入力し特徴抽出。PointNetを細分化することにより、指先に対する推定精度を向上させる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180610HandPointNet.jpg" alt="20180610HandPointNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>CNNを用いた従来の奥行き画像における3次元手姿勢推定手法とは異なり、本研究では三次元点群に着目している。データは、奥行き画像をポイントクラウドデータに変換してから使用している。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>3つのハンドポーズデータセットにて実験し、リアルタイム性に優れていることを示唆。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ge_Hand_PointNet_3D_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#50]</div></div></section><section id="Toward_Driving_Scene_Understanding_A_Dataset_for_Learning_Driver_Behavior_and_Causal_Reasoning"><div class="paper-abstract"><div class="title">Toward Driving Scene Understanding:A Dataset for Learning Driver Behavior and Causal Reasoning</div><div class="info"><div class="authors">Vasili Ramanishka, et al.</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>自動車の運転シーン理解のためのデータセットであるHonda Research Institute Driving Dataset(HDD)の提案。本データセットはサンフランシスコ・ベイエリアにて、様々なセンサーを備えた自動車を人間が運転したデータが104時間分含まれる。センサはグラスホッパーカメラ、LiDAR、ダイナミックモーションアナライザ、Vehicle Controller Area Network (CAN)の4つ。これらのデータから運転者の行動を基にアノテーションを付加している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180610HDD.jpg" alt="20180610HDD.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>様々なセンサを用いて、大規模データを収集しただけでなく、ヒューマンファクタや認知科学に基づいてアノテーションを行っている。アノテーションは、Goal-oriented action, Stimulus-driven action, Cause, Attentionの4つ。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>LSTMを用いたベースラインにおいて、センサを増やすことによって表現力の向上が見られた。評価が難しいアノテーションデータが含まれ、チャレンジングなデータセット。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3855.pdf">論文</a></li></ul></div></div><div class="slide_index">[#51]</div></div></section><section id="A_High-Quality_Denoising_Dataset_for_Smartphone_Cameras"><div class="paper-abstract"><div class="title">A High-Quality Denoising Dataset for Smartphone Cameras</div><div class="info"><div class="authors">Abdelrahman Abdelhamed, Stephen Lin, Michael S. Brown</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>スマートフォンで撮影したノイズの多い画像で構成したデータセットSmartphone Image Denoising Dataset (SIDD)の提案。 5つの代表的なスマホカメラを使用し、様々な照明条件下で約30,000枚のノイズの多い画像を収集。ノイズの多い画像だけでなく、ノイズを除去した画像をground truthとして提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180610SIDD.jpg" alt="20180610SIDD.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>過去10年間で、撮影される画像は一眼レフやコンデジから、スマートフォンに切り替わったことに着目。しかし、口径やセンサ―サイズが小さいため、スマホの写真はノイズを多く含んでいる。このような、ノイズを多く含んだスマホ画像を集めることで新たなデータセットを提案する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>やはりノイズを含むスマホ画像でのトレーニングよりも、高品質な画像でトレーニングした方が、CNNで高い精度を得た。現在のタスクにおいて「スマホの画像だから精度が出ない」というのはあまり考えにくいが、日常的なアプリケーションには有用なデータセットではないか。</p><ul><li><a href="http://www.cse.yorku.ca/~mbrown/pdf/sidd_cvpr2018.pdf">論文</a></li></ul></div></div><div class="slide_index">[#52]</div></div></section><section id="Fast_and_Furious_Real_Time_End-to-End_3D_Detection_Tracking_and_Motion_Forecasting_with_a_Single_Convolutional_Net"><div class="paper-abstract"><div class="title">Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net</div><div class="info"><div class="authors">Wenjie Luo, Bin Yang, Raquel Urtasun</div><div class="conference">CVPR2018</div><div class="paper_id">437</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>3Dセンサで得られた点群から3D物体検出や追跡を行う新しいDNN「Fast and Furious（FaF）」を提案．検出と追跡，さらに短期の経路予測を同時に推論でき，Sparse dataやオクルージョンに頑健な検出ができる．3D点群と時間の4Dテンソルを入力として，空間と時間に対して3D畳み込みを行う．4DテンソルはEarly FusionまたはLate Fusion（図中ではLater）で時間情報を結合している．これらは精度と効率のトレードオフ関係にある．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180610_FaF1.jpg" alt="20180610_FaF1.jpg"><img src="slides/figs/20180610_FaF2.jpg" alt="20180610_FaF2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>物体検出から追跡，さらに経路予測までend-to-endで行えるモデル．全体の検出時間はわずか30ms以下である．約55万フレームからなるLiDARのデータセットを作成し，車両に3D bboxとトラッキング用IDをラベリングして学習および評価に用いる．物体検出の結果はSSDのIoU 77.92mAPを上回る83.10mAPである（Late Fusionを用いることで1.4mAP向上している）．追跡もHungarianと同等以上の性能で，経路予測もL2距離0.33メートル未満で10フレーム予測可能である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>タイトルが某カーアクション映画みたいでカッコいい．内容も名前負けしておらずよく作り込まれておりOralで採択されている．インパクトのあるタイトルは大切．</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3013.pdf">論文</a></li></ul></div></div><div class="slide_index">[#53]</div><div class="timestamp">2018.6.10 03:15:09</div></div></section><section id="Low-Shot_Learning_from_Imaginary_Data"><div class="paper-abstract"><div class="title">Low-Shot Learning from Imaginary Data</div><div class="info"><div class="authors">Yu-Xiong Wang, et al.</div><div class="paper_id">1801.05401</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>人間の想像力に着目することで、メタ学習におけるLow-Shot Learningを可能にするアーキテクチャの提案。コンピュータビジョンに幻覚(想像)を抱かせることで、少ないデータから新しい視覚的概念を学習させる。アプローチとしては、メタ学習を取り入れており、 meta-learnertとhallucinator(幻覚者)を組み合わせて共同で最適化。hallucinatorは、通常のトレインセットとノイズベクトルから幻覚トレーニングセットを出力する。通常のトレーニングセットに加えて、幻覚トレーニングセットを学習することで精度向上を図る。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180609imagin.jpg" alt="20180609imagin.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>人間は新しい視覚的情報を素早く学習できる。これは、「物体がさまざまな視点から見たときにどのように見えるかを想像できるから」と仮定。そのうえで、人間の想像力をモデルとし、システムに組み込むことでLow-Shot Learningを可能にしている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>AIに幻覚を見せられる時が来た模様。さまざまなメタ学習手法に組み込むことができ、精度を向上させられるらしい。</p><ul><li><a href="https://arxiv.org/pdf/1801.05401.pdf">論文</a></li></ul></div></div><div class="slide_index">[#54]</div></div></section><section id="Multi-View_Harmonized_Bilinear_Network_for_3D_Object_Recognition"><div class="paper-abstract"><div class="title">Multi-View Harmonized Bilinear Network for 3D Object Recognition</div><div class="info"><div class="authors">Tan Yu, Jingjing Meng, Junsong Yuan</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>3次元物体認識を実行するMulti-view Harmonized Bilinear Network (MHBN)を提案する。異なるビューの特徴量を学習するために基本的にはパッチベースでマッチングを行う。Polynomial Kernel/Bilinear Poolingの関係性を記述するために、畳み込みによる3次元物体表現とBilinear Poolingを実行する。MHBNの枠組みはEnd-to-Endでの学習が可能である。構造は右図のように示され、畳み込みにより特徴マップ（3次元物体表現）を生成、最後にBilinear Poolingを通り抜けて識別を実行。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609MultiviewHarmonized.png" alt="180609MultiviewHarmonized"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>3次元物体認識の場面においてSoTA。ModelNet40, ModelNet10ではそれぞれ94.7 (Instance)/93.1 (Class), 95.0 (Instance)/95.0 (Class)である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>3次元物体認識ではホントの意味での大規模DBはないのだろうか？ModelNetにしてもShapeNetにしてもCADをベースにしている？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Multi-View_Harmonized_Bilinear_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://cse.buffalo.edu/~jsyuan/">著者</a></li><li><a href="http://vis-www.cs.umass.edu/mvcnn/">MVCNN</a></li></ul></div></div><div class="slide_index">[#55]</div><div class="timestamp">2018.6.9 22:40:37</div></div></section><section id="Disentangled_Person_Image_Generation"><div class="paper-abstract"><div class="title">Disentangled Person Image Generation</div><div class="info"><div class="authors">Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, Mario Fritz</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>アピアランス/ビューポイント/背景など、分解された（Disentangled）人物画像の生成を行うための研究である。この目的のため、2ステージの生成手法を考案した（右図を参照）。1ステージ目はリアルの埋め込み特徴（Embedding Features）を獲得する学習を行い、前景/背景や姿勢などを表現。次に2ステージ目は敵対的学習により生成的特徴学習を行いガウシアンノイズから中間表現にマッピング、特徴変換を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609DisentangledPersonImageGeneration.png" alt="180609DisentangledPersonImageGeneration"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>姿勢ベースの人物画像を生成し、人物再同定（Person Re-Identification; ReID）の学習に適用。人物画像生成自体も誤差が少なく、ReIDのためのにおいても良好な精度を実現した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>学習画像がコントロールできるということで注目される技術。ある程度の知見を学習しておけば、そのうちリアル画像のデータがいらない時代になる？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Ma_Disentangled_Person_Image_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://github.com/charliememory/Disentangled-Person-Image-Generation">GitHub</a></li></ul></div></div><div class="slide_index">[#56]</div><div class="timestamp">2018.6.9 21:59:40</div></div></section><section id="Learning_Pose_Specific_Representations_by_Predicting_Different_Views"><div class="paper-abstract"><div class="title">Learning Pose Specific Representations by Predicting Different Views</div><div class="info"><div class="authors">Georg Poier, David Schinagl, Horst Bischof</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>異なるビューポイントの距離画像入力から、低次元の潜在表現を利用して手部領域追跡の学習を実行する研究である。ビューポイント推定の誤差をフィードバックして、教師なしでも手部の姿勢推定に必要な潜在表現を獲得する。これにより、必要なのは対象となるビューポイントではなく、第二のビューポイントのみであり、ラベルあり/ラベルなしの場合においても効果的に学習することができる（Semi-supervised Learningの枠組みで学習可能）。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609PoseSpecificRepresentation.png" alt="180609PoseSpecificRepresentation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>あるビューポイントの距離画像が手に入れば、異なるビューポイントに関する手部領域の姿勢推定が可能になるSemi-supervised Learningを提案。異なるビューポイントの低次元潜在表現を学習し、3Dの関節位置を推定することができる。NYU-CS dataset/MV-hands datasetにてState-of-the-artな精度を達成。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>中間表現（本論文の場合には低次元潜在空間）を学習して、異なるドメイン間の学習に応用したい。このような問題は意外と簡単にできるのだろうか？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Poier_Learning_Pose_Specific_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#57]</div><div class="timestamp">2018.6.9 19:19:49</div></div></section><section id="Fine-grained_Video_Captioning_for_Sports_Narrative"><div class="paper-abstract"><div class="title">Fine-grained Video Captioning for Sports Narrative</div><div class="info"><div class="authors">Huanyu Yu, Shuo Cheng, Bingbing Ni, Minsi Wang, Jian Zhang, Xiaokang Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><Kazushige>okayasu</Kazushige></div><div class="item1"><div class="text"><h1>概要</h1><p>Fine-grainedなスポーツ動画キャプショニング</p></div></div><div class="item2"><div class="text"><p>img(src=`${figpath}Fine-grained_Video_Captioning_for_Sports_Narrative.png`,alt="Fine-grained_Video_Captioning_for_Sports_Narrative")</p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>youtubeから2Kのスポーツ動画とキャプションからなるFine-grained Sports Narrative dataset(FSN)の提案</li><li>スポーツビデオのキャプショニングの新しい評価指標Fine-grained Captioning Evaluation(FCE)の提案</li><li>スポーツビデオのキャプショニングの新しいフレームワークの提案(骨格情報とオプティカルフローで詳細な動作のエンコード，オプティカルフローと選手のローカライズ結果で人物間のインタラクションをエンコードそれらのエンコードされたベクトルを階層的RNNで言語化)</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0668.pdf">論文</a></li></ul></div></div><div class="slide_index">[#58]</div><div class="timestamp">2018.3.24 13:04:44</div></div></section><section id="GANerated_Hands_for_Real-Time_3D_Hand_Tracking_From_Monocular_RGB"><div class="paper-abstract"><div class="title">GANerated Hands for Real-Time 3D Hand Tracking From Monocular RGB</div><div class="info"><div class="authors">Franziska Mueller, Florian Bernard, Oleksandr Sotnychenko, Dushyant Mehta, Srinath Sridhar, Dan Casas, Christian Theobalt</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>RGBのみの動画入力からリアルタイムに3次元手部関節位置推定を実行する手法を提案。YouTubeのようなコントロールされていない場面においても3次元手部関節位置推定を行うことができる。本論文では3次元のハンドモデルとCNNを組み合わせることによりトラッキングを実行しており、GANによる生成ベース（手の3次元合成データをリアルに変換していることに相当）の手法によりオクルージョンやビューポイントの違いに頑健である。GANはAdversarial LossとCycle-consistency Loss、さらには幾何学的な整合性を保つためにGeometric Consistency Lossを最適化するよう学習。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609GANeratedHand.png" alt="180609GANeratedHand"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>GANをベースとして合成データからリアル画像を生成、同データで学習したモデルは、RGB-onlyな3次元ハンドトラッキングにおいてState-of-the-artである。敵対的学習を用いたデータ生成手法、YouTube等のあまり校正されていないデータにおいても良好な精度を実現していることが採択された理由であると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>3Dデータを自由に生成できることは、次世代のアイディアを実現するための大きなポイントである。3次元トラッキングのみならず面白いこと考えたい。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mueller_GANerated_Hands_for_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#59]</div><div class="timestamp">2018.6.9 18:52:55</div></div></section><section id="A_Certifiably_Globally_Optimal_Solution_to_the_Non-Minimal_Relative_Pose_Problem"><div class="paper-abstract"><div class="title">A Certifiably Globally Optimal Solution to the Non-Minimal Relative Pose Problem</div><div class="info"><div class="authors">Jesus Briales, Laurent Kneip, Javier Gonzalez-Jimenez</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>キャリブレーション済みの２カメラにおける相対姿勢の推定問題を解くための全体最適化法（Globally Optimal Solution）を提案する。局所最適解ではなく、グローバルな最適化が計算できることが新規性である。本論文では、凸最適化の問題においてあらかじめ定義された問題（Shor's Convex Relaxation）としてQuadratically Constrained Quadratic Program (QCQP)を扱うことを実施する。ここに対して、理論的かつ実験的な解答法を提示したことが本論文の貢献である。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609GloballyOptimalSolution.png" alt="180609GloballyOptimalSolution"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>２カメラの相対姿勢問題の解決のために従来の凸最適化手法を適用して、理論的かつ実験的に解決できることを示したことが新規性であり、CVPRに採択された理由である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>（あまり深く読めていないのと、知識が足りなくて自信がないです。。）</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Briales_A_Certifiably_Globally_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#60]</div><div class="timestamp">2018.6.9 17:36:01</div></div></section><section id="LiDAR-Video_Driving_Dataset_Learning_Driving_Policies_Effectively"><div class="paper-abstract"><div class="title">LiDAR-Video Driving Dataset: Learning Driving Policies Effectively</div><div class="info"><div class="authors">Yiping Chen, et al. </div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>LiDERで取得したポイントクラウド、車載カメラ映像、および一般ドライバーの運転動作からなるLiDAR-Videoデータセットの提案。運転動作は、ハンドルの傾きと自動車の走行速度情報によるもの。また、これらのデータを使い、自律走行における運転手段を決定するためのPolicy Learningを提案。 これは、DNN+LSTMで構成されるアーキテクチャである。3種類のデータの対応時間を登録することでどのように運転するかをベンチマークする。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180690LiDERVIDIO.jpg" alt="20180690LiDERVIDIO.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>自律走行において、これまではカメラとレーザースキャナー、運転動作を組み合わせたデータやアプローチがなかった。本論文ではデータベースを構築したうえで、自律走行に対するアプローチを提案している。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>単一のデータよりも3つのデータを組み合わせることで精度が向上していることを示唆。また、DNN単体よりも長いtermで処理できるDNN+LSTMの方が精度向上につながることも示唆。</p><ul><li><a href="https://drive.google.com/file/d/1d9OlwHOAz0EgoiMTFsNWaievOArKO5EC/view">論文</a></li></ul></div></div><div class="slide_index">[#61]</div></div></section><section id="Collaborative_and_Adversarial_Network_for_Unsupervised_domain_adaptation"><div class="paper-abstract"><div class="title">Collaborative and Adversarial Network for Unsupervised domain adaptation</div><div class="info"><div class="authors">Weichen Zhang, Wanli Ouyang, Wen Li, Dong Xu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>CNNの浅い層ではドメイン固有の特徴量を、深い層ではドメインに不変な特徴量を取得することでdomain adaptationを行うCollaborative and Adversarial Network(CAN)を提案。
従来の<a href="http://jmlr.org/papers/volume17/15-239/15-239.pdf">Domain Adversarial Training of Neural Network(DANN)</a>ではドメインに不変な特徴量を学習することができるものの、ターゲットドメイン固有の特徴量を得ることが難しいという問題があった。
提案手法では、CNNの浅い層では低次の特徴量を、深い層では高次の特徴量を取得することができることに着目し、
CNNのそれぞれのブロックに対するdomain discriminatorに対して、浅いブロックではソースドメインとターゲットドメインを識別可能となるように、
深いそうでは識別が不可能となるように学習を行う。ソースドメインに対してはクラスの識別も行う。
またテストデータに対してpseudo labelingを行うIncremental CAN(iCAN)も提案。
ターゲットドメインのサンプルのうち、高いconfidenceでソースドメインであると判定され、
かついずれかのラベルに対するconfidenceが高いものに対してpseudo labelingを行うことで、データセットを拡張しdomain shiftを解消する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Collaborative_and_Adversarial_Network_for_Unsupervised_domain_adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>CNNの浅いブロックで得られる特徴量に対してはドメイン識別が可能なように、深いブロックで得られる特徴量に対してはドメイン識別が不可能なように学習を行うCANを提案。
またターゲットドメインに対してpseudo labeingを行うiCANも提案。</li><li>実験で使用したのはpretrained RenNet50であり、10層目、22層目、40層目、49層目のそれぞれに対してdomain discriminatorを適用。41~49層からなるブロックからドメインに不変な特徴量を得るように学習を行った。</li><li>Office31、ImageCLEF-DAを用いたクラス識別においてstate-of-the-artと比較した結果、最も高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>シンプルな発想だが面白い手法！似たアイディアで画像の生成もできないだろうか？</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Collaborative_and_Adversarial_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#62]</div></div></section><section id="Look_at_Boundary_A_Boundary-Aware_Face_Alignment_Algorithm"><div class="paper-abstract"><div class="title">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</div><div class="info"><div class="authors">Author</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔の境界線を事前分布として使用することで、顔のランドマークを推定する手法を提案。既存手法でジゼ情報として使用されている顔のパーツは情報が離散的であり、
顔に対するセマンティックセグメンテーションであるface parsingは鼻に対する精度が良くない。
一方で顔の境界線は定義がはっきりしており、かつ顔の形状から推定することが可能。
提案手法では顔の境界線をstacked hourglassをベースとして、オクルージョンに対して頑健になるようにmessage passing layer、
推定精度の向上のためにadversarial netを導入している。推定された顔の境界線を元に、顔のランドマークを推定する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Look_at_Boundary_A_Boundary-Aware_Face_Alignment_Algorithm.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>事前実験によって顔の境界線を用いたランドマーク推定がstate-of-the-artよりも優っていることを確認した上で手法を提案。</li><li>300W, COFW, AFLWなどのデータセットにおいてstate-of-the-arttと比較した結果、全ての場合において提案手法が優位となった。また境界線のGTを使用したランドマーク推定をOracleとして示しており、
Oracleによる推定精度が最も高くなった。</li><li>WIDER FaceデータセットをベースにしたWider Facial Landmarks in-the-wild(WFLW)データセットを構築しており、10000枚の画像に対して98点のランドマーク、オクルージョン、メイク、照明環境、ブラー、表情のアノテーションを持つ。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>事前実験やOracleによって精度向上の理由が明確になっていルため、手法の優位性がはっきりと伝わってくる。</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1330.pdf">論文</a></li><li><a href="https://wywu.github.io/projects/LAB/LAB.html">Project page(Supplementary material, Demo, Code)</a></li></ul></div></div><div class="slide_index">[#63]</div></div></section><section id="Revisiting_knowledge_transfer_for_training_object_class_detectors"><div class="paper-abstract"><div class="title">Revisiting knowledge transfer for training object class detectors</div><div class="info"><div class="authors">Jasper Uijlings, Stefan Popov, Vittorio Ferrari</div><div class="paper_id">1708.06128</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ソースクラスのBBoxアノテーションを使って、弱教師付きのトレーニング画像からターゲットの物体検出器を学習する知識転移手法の提案。まず、ソーストレインセットでproposal generatorをトレーニングし、それをターゲットトレインセットに適用。次に、画像のクラスラベル(Bboxなし)を使用し、知識転移でMultiple Instance Learning(MIL)を実行。 MILによって、物体検出器をトレーニングするために使用する、ターゲットクラス用のBBoxを生成。最後に、ターゲットの物体検出器をターゲットテストセットに適用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180609MKT.jpg" alt="20180609MKT.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>物体候補とクラスを段階的に知識伝達していくフレームワーク。これにより、固有のクラスやジェネリックなクラスに渡る、広い知識伝達を可能にすることができる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>段階的な知識伝達によって、良質な物体候補を出力できる。</p><ul><li><a href="https://arxiv.org/pdf/1708.06128.pdf">論文</a></li></ul></div></div><div class="slide_index">[#64]</div></div></section><section id="Fight_Ill-Posedness_With_Ill-Posedness_Single-Shot_Variational_Depth_Super-Resolution_From_Shading"><div class="paper-abstract"><div class="title">Fight Ill-Posedness With Ill-Posedness: Single-Shot Variational Depth Super-Resolution From Shading</div><div class="info"><div class="authors">Bjoern Haefner, Yvain Quéau, Thomas Möllenhoff, Daniel Cremers</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>距離空間/距離画像の超解像を行う（Super-Resolution）を行う技術を提案。従来はShape-from-shadingにより行って来たが、形状の複雑性（誤りを含む）が存在していたため、これを改善する手法を提案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609FightIllPosed.png" alt="180609FightIllPosed"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>距離画像における超解像を行うための最適化手法を提案した。結果は図に示すとおりである。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Haefner_Fight_Ill-Posedness_With_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://github.com/YapengTian/Single-Image-Super-Resolution">Single-Image-Super-Resolution</a></li></ul></div></div><div class="slide_index">[#65]</div><div class="timestamp">2018.6.9 13:58:31</div></div></section><section id="Multistage_Adversarial_Losses_for_Pose-Based_Human_Image_Synthesis"><div class="paper-abstract"><div class="title">Multistage Adversarial Losses for Pose-Based Human Image Synthesis</div><div class="info"><div class="authors">Chenyang Si, Wei Wang, Liang Wang, Tieniu Tan</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人物の姿勢を事前情報として、ある視点の人物画像の入力からビューポイントを変更した人物画像を合成する手法を提案する。右図では3ステージのフレームワークについて示しており、最初のステージでは角度情報を挿入した姿勢変換、次のステージでは角度変化した人物にアピアランスを挿入、最後に背景を自然に挿入するステージ、という感じで変換が進んで行く。どう枠組みを実行するため、特にステージ２ではAdversarial Lossが、ステージ３ではForeground/Global Adversarial Lossを適用して誤差を計算する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609PoseHumanSynthesis.png" alt="180609PoseHumanSynthesis"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>評価は生成した画像のPSNR（シグナル・ノイズ比）、正解値との誤差SSIMを計算して、提案手法がもっとも優れた数値を出していることを明らかにした（SSIM: 0.72, PSNR: 20.62）。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>データセットの環境が固定だからできる？背景モデルの空間が非常に小さいので変換した際にもテクスチャが崩れずに生成できる？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Si_Multistage_Adversarial_Losses_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#66]</div><div class="timestamp">2018.6.9 13:47:06</div></div></section><section id="Cross-Modal_Deep_Variational_Hand_Pose_Estimation"><div class="paper-abstract"><div class="title">Cross-Modal Deep Variational Hand Pose Estimation</div><div class="info"><div class="authors">Adrian Spurr, Jie Song, Seonwook Park, Otmar Hilliges</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>2次元画像と3次元手部モデルを同様の空間で扱うことができるCross-modal latent spaceを提案して、手部姿勢推定を実行する。別々にクラスタリングするのではなく、同一の空間で扱う（2DRGB-3D空間関係なく、同じ姿勢は同じような空間位置に投影される）方がマッチングの際にも便利。この特徴空間を学習するためにVariational Auto-Encoder（VAE）の枠組みで、Cross-modalのKL-divergenceを学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609CrossModalLatentSpace.png" alt="180609CrossModalLatentSpace"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>2D-3Dの共通空間を学習することで、2D画像からダイレクトに手部の3D関節点推定に成功した。距離画像との単一空間も学習可能とした。同一空間上で扱えるようにして、かつ従来法よりも精度向上が見られたため、CVPRに採択された。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>異なるモダリティを同一の枠組みで行ってしまう（2d-3dを同じ空間で）学習は他にもありそう？</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Spurr_Cross-Modal_Deep_Variational_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#67]</div><div class="timestamp">2018.6.9 13:24:52</div></div></section><section id="Progressive_Attention_Guided_Recurrent_Network_for_Salient_Object_Detection"><div class="paper-abstract"><div class="title">Progressive Attention Guided Recurrent Network for Salient Object Detection</div><div class="info"><div class="authors">Xiaoning Zhang, et al.</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>マルチレベルのコンテクスト情報を選択的に統合する、顕著性のためのProgressive Attention Guided Recurrent Networkの提案。Attention Moduleを複数組み込み、その出力をステップ形式で統合していく。高レベルのfeatureを使って、低レベルのfeatureをガイドするイメージ。また、ネットワーク全体を最適化するためのmulti-path recurrent feedbackを提案。これにより、上部の畳み込み層からのセマンティック情報を、浅い層に転送することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180609PAGRN.jpg" alt="20180609PAGRN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>顕著性推定のための学習方法の提案。 従来のFCNベースの方法では、情報を区別せずに多レベルの畳み込み特徴を直接適用してしまうため、精度が上がらないと指摘。複数の層、複数のAttention Module出力を使い、コンテキスト情報を統合するので強力な特徴を抽出できる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>6種類のデータベースで精度評価。従来手法と比較して、ほぼ全てで最良の結果。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1235.pdf">論文</a></li></ul></div></div><div class="slide_index">[#68]</div></div></section><section id="Scale-Transferrable_Object_Detection"><div class="paper-abstract"><div class="title">Scale-Transferrable Object Detection</div><div class="info"><div class="authors">Peng Zhou, et al.</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>マルチスケールに対応した物体検出器であるScale-Transferrable Object Detection(STDN)の提案。STDNは DenseNet-169をベースとし、複数の物体スケールに対応するためのsuper-resolution layersを搭載。このsuper-resolution layersによってアップサンプリングすることで高解像度のfeature mapを得られるので小さな物体に対応し、大きな物体にはpooling層で対応する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180690STDN.jpg" alt="20180690STDN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来の物体検出手法では、様々なサイズのfeature mapを組み合わせるなどして、スケールに対応していたが、やはり小さな物体は苦手。本手法では、super-resolution layersという新たな手法によって改善を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>PASCAL VOCやMS COCOなどで精度向上を示している。個人的には、物体検出が苦手とする小さな物体に着目したデータセットなどを用意したうえで精度を比較してみたい。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1376.pdf">論文</a></li></ul></div></div><div class="slide_index">[#69]</div></div></section><section id="Weakly_and_Semi_Supervised_Human_Body_Part_Parsing_via_Pose-Guided_Knowledge_Transfer"><div class="paper-abstract"><div class="title">Weakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer</div><div class="info"><div class="authors">Hao-Shu Fang, Guansong Lu, Xiaolin Fang, Jianwen Xie, Yu-Wing Tai, Cewu Lu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人物姿勢推定において「似たような姿勢はほぼ同じセグメント結果を保有する」という前提で弱教師付き/半教師あり学習を実行する。ある対象画像が入力された際にはほぼ同じ姿勢のデータをDBから検索して知識を転用（Pose-guided Knowledge Transfer）学習を実行する。その際に姿勢による拘束条件（Morphological Constraints）を入れ込むことでピクセルベースの姿勢のセグメンテーションを実行。モデルは全層畳み込みネット（Fully Convolutional Networks; FCN)を適用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180609WeakSemiPoseParsing.png" alt="180609WeakSemiPoseParsing"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>弱教師付き学習（類似の姿勢を検索して対応づける）/半教師付き学習（少量のデータがあれば学習を実行）、いずれの手法でも姿勢学習を実行することができる。その上でデータ量を確保することに成功し、PASCAL-Part datasetにてmAPが3ポイント向上した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>より少量のアノテーションで、かつ複数の枠組みで（本論文の場合は弱教師付き学習/半教師あり学習）学習が実行できる枠組みが増えてきた。そればかりか、教師あり学習のみよりも精度の高いものができあがりつつある。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Fang_Weakly_and_Semi_CVPR_2018_paper.pdf">論文</a></li><li><a href="http://www.stat.ucla.edu/~jxie/">著者</a></li><li><a href="whttps://github.com/MVIG-SJTU/WSHPww">GitHub</a></li></ul></div></div><div class="slide_index">[#70]</div><div class="timestamp">2018.6.9 09:39:15</div></div></section><section id="Occluded_Pedestrian_Detection_Through_Guided_Attention_in_CNNs"><div class="paper-abstract"><div class="title">Occluded Pedestrian Detection Through Guided Attention in CNNs</div><div class="info"><div class="authors">Shanshan Zhang, et al.</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>オクルージョンに頑健な、Faster R-CNNベースの歩行者検出手法の提案。歩行者検出について解析することで、CNN特徴の各チャンネルがそれぞれ異なる身体部分を活性化していることに着目。(実際にチャンネルごとにアテンションを取ることで確認)各チャンネルが異なる身体部位を表現しているならば、オクルージョン発生時に身体部位の特定の組み合わせを定式化することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180608OPDTGA.jpg" alt="20180608OPDTGA.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>歩行者検出器におけるCNN特徴について解析することで、歩行者に特化した物体検出を可能にしている。Faster R-CNNにAttention Networkを追加したアーキテクチャを提案。これにより、上位featureの重みパラメータを調節。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>アーキテクチャをあまり複雑化せずに精度を向上させている。動物や虫などでも、CNNチャンネルごとに異なる身体部位を表現しているのだろうか。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2198.pdf">論文</a></li></ul></div></div><div class="slide_index">[#71]</div></div></section><section id="FaceID-GAN_Learning_a_Symmetry_Three-Player_GAN_for_Identity-Preserving_Face_Synthesis"><div class="paper-abstract"><div class="title">FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis</div><div class="info"><div class="authors">Yujun Shen, Ping Luo, Junjie Yan, Xiaogang Wang, Xiaoou Tang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>IDを保った任意の顔向き画像をGANで生成するために、実画像ドメインと合成画像ドメインのそれぞれのIDを識別するclassifierを導入したFaceID-GANを提案。従来のGANではgeneratorとdiscriminatorが競い合うだけでclassifierは補助的な機能を果たしていたが、
提案手法におけるclassifierは実画像に対しては実画像ドメインのID番号を、
合成画像に対しては合成画像ドメインのID番号を識別させる、というようにデータセットに含まれるN個のラベルに対して、
2Nのラベル識別を行う。
他にも実画像のIDを表す特徴量と合成画像のIDを表す特徴量のコサイン類似度をロス関数として使用することで、
異なるドメインに属する特徴量の類似度を高める。generatorには顔の形状特徴量、顔向き特徴量、ランダムノイズを入力とする。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/FaceID-GAN_Learning_a_Symmetry_Three-Player_GAN_for_Identity-Preserving_Face_Synthesis.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>実画像、合成画像のそれぞれのドメインにおいてID識別を行うclassifierをGANに導入することで、generator VS. discriminator & classifier の構図を持つFaceID-GANを提案。</li><li>CASIA-WebFace494414枚(10575人のID)の画像でトレーニングを行い、LFW, IJB-A, CelebA, CFPで検証した。</li><li>state-of-the-artと横顔を入力とした正面顔画像生成、水平方向の視点移動、face verificationの精度を比較した結果、最も高い精度を達成した。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_FaceID-GAN_Learning_a_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=EDahz2ppTYY">Demo</a></li></ul></div></div><div class="slide_index">[#72]</div></div></section><section id="Unsupervised_Sparse_Dirichlet-Net_for_Hyperspectral_Image_Super-Resolution"><div class="paper-abstract"><div class="title">Unsupervised Sparse Dirichlet-Net for Hyperspectral Image Super-Resolution</div><div class="info"><div class="authors">Ying Qu, Hairong Qi, Chiman Kwan</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>高解像度かつ短いスペクトルバンド幅で撮影された画像であるhyper resolution hyperspectral image(HR HSI)を、HR HSIの正解データなしで、広いスペクトルバンド幅で撮影された高解像度画像(HR MSI)と、短いスペクトルバンド幅で撮影された低解像度画像(LR HSI)を用いて生成する手法を提案。
高解像度かつ短いスペクトルバンド幅で写真を撮影することはハードウェア的に困難であり、データセットの構築も難しい。
提案手法ではHR MSIとLR HSIをトレーニングデータとして2つのencoder-decoderを用いる。
HR MSIとLR HSIにはそれぞれ独立のエンコーダーが適用されるが、LR HSIから得られるスペクトル情報を共有するため、
デコーダーは共有する。またスペクトル係数の総和は1という物理的な制約を実現するために潜在変数がディリクレ分布に従うようにする。
また推定されたスペクトルに対し得てスペクトル空間上の角度の差が小さくなるように学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Unsupervised_Sparse_Dirichlet-Net_for_Hyperspectral_Image_Super-Resolution.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>CAVE、Harvardデータセットにて検証を行い、state-of-the-artとRMSE、SAM(スペクトル空間のベクトル類似性)比較して最も高い精度を達成。</li><li>教師無し学習が行えた理由として、古くから取り扱われている問題設定であったため、問題の性質をよく知っていたことがあげられる。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.05042">論文</a></li></ul></div></div><div class="slide_index">[#73]</div></div></section><section id="_3D_Semantic_Segmentation_with_Submanifold_Sparse_Convolutional_Networks"><div class="paper-abstract"><div class="title">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</div><div class="info"><div class="authors">Benjamin Graham, Laurens van der Maaten, Martin Engelcke</div><div class="conference">CVPR 2018</div><div class="paper_id">1248</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>スパース性が持ったデータ(ポイントクラウドなど)をより効率的で畳み込むsparse convolutional operationsを提案した．また，提案operationsを用いて新たな高次元スパースデータを有効的に処理できるsubmanifold sparse convolutional networks(SSCNs)を提案した．</li><li>従来の問題点：従来のCNNをsparse dataに用いたら計算及びメモリーの効率が良くない問題点がある．また，従来のスパースデータのためのネットワークは主に”full convolution”を行うためスパースデータをdilateしてしまう問題点がある．また，従来のCNNは層が深まることにより，active sitesが大幅に増加してしまうような“submanifold dilation problem”がある．</li><li>以上の様々な問題から，“ネットワークの異なる層で同じレベルのactive sitesのスパース性を保つ”をベースな考えとした新たなconvolution operations:SSCを提案した．こういうような性質から，SSCを用いたらより深い層構造持ったネットワークの学習を可能にした</li><li>具体的なssc：①プーリーングとstrided畳み込み操作と合併②入力のactive sitesだけに対して畳み込みし，active sitesを出力．Ground stateの入力を0と取り扱い畳み込みを廃棄のような設定がある</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SSCN-3D-SemanticSegmentation.png" alt="SSCN-3D-SemanticSegmentation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>提案のSSCがスパース性持ったデータの高効率CNNを可能にした．また，計算量とメモリー消耗の大幅削減及び深い層ネットワークの構築などに用いられる．</li><li>ShapeNetデータセットにおいて，SSCNを用いた3Dシーン及び物体パーツセマンティックセグメンテーションが従来手法(PointCNN,PointNet,Pd-Networkなど含め)より良い精度を達成した．更に，SSCNsの計算効率がより良い</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>論文がとても読みやすかった．しかし想像力が貧乏なので，うまくまとめられない．発表ビデオやコードで具体的なsparse convolutional operations操作を勉強したい</p></li><li><p>ポイントクラウドのようなスパースデータに相性が良いので，SSCNsを用いて３次元処理を行う文章がこれから出てきそう</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3108.pdf">論文</a></p></li><li><p><a href="https://github.com/facebookresearch/SparseConvNet">コード</a></p></li></ul></div></div><div class="slide_index">[#74]</div><div class="timestamp">2018.6.7 19:19:22</div></div></section><section id="Im2Struct_Recovering_3D_Shape_Structure_from_a_Single_RGB_Image"><div class="paper-abstract"><div class="title">Im2Struct: Recovering 3D Shape Structure from a Single RGB Image</div><div class="info"><div class="authors">Chengjie Niu, Jun Li, Kai Xu</div><div class="conference">CVPR 2018</div><div class="paper_id">578</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>1枚のRGB画像から3次元形状構造(直方体で物体パーツを表示し，構造をパーツ間の接続性や対称性などの関係で表す)を復元するネットワーク構造を提案した．</li><li>従来1枚のRGB画像からボリューメトリックの復元が広く研究されている．しかし従来の様々な手法より復元された物体はトポロジーや構造が崩れる問題点が多く存在する（特に入力モデルの構造欠損がある場合）．提案手法は画像から形状構造復元を行うため，従来の体積復元の更なる精度向上や3次元形状構造の編集や高レベル画像編集など様々なところに応用できる．</li><li>提案手法のネットワークは①構造マスクを推定するネットワーク②再帰的オートエンコーダーを用いた直方形階層の構造復元ネットワークで構成される．具体的①はskip連結付きなマルチスケールCNNを用いた．②は①の抽出特徴及び元画像の特徴から再帰的なデコーダーを用いた．学習データは3D CADモデルからレンダリング及び構造抽出により作成した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Im2Struct.png" alt="Im2Struct"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>提案手法が初めての1枚RGB画像から詳細3次元形状構造を復元する手法と指摘した．</li><li>提案の形状構造復元手法がパーツ間の連結や対称性など関係の復元を学習するので，復元された形状の構造の妥当性と汎用性が保証できる．</li><li>構造駆動型3次元体積補間及び構造awareなインタラクティブ画像編集の2つのアプリを開発し，提案手法により復元された形状構造の有効性および妥当性を示した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>画像からの3次元形状構造復元がvolume復元と比べパラメータ数が圧倒的少ないので，問題自体の難しさも低い．しかし実応用を考えると，構造復元がかなり応用場面が多いと思う．問題設定がとても良いと思う</p></li><li><p>逆に今までどうしてやる人がなかったのが分からない</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3941.pdf">論文</a></p></li></ul></div></div><div class="slide_index">[#75]</div><div class="timestamp">2018.6.7 16:59:09</div></div></section><section id="_3D-RCNN_Instance-level_3D_Object_Reconstruction_via_Render-and-Compare"><div class="paper-abstract"><div class="title">3D-RCNN: Instance-level 3D Object Reconstruction via Render-and-Compare</div><div class="info"><div class="authors">Abhijit Kundu, Yin Li, James Rehg</div><div class="conference">CVPR 2018</div><div class="paper_id">436</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>RGB画像からインスタンスレベルの物体full3次元形状及び姿勢を行う”inverse graphics”なend-to-endなネットワーク構造の提案． 物体のカテゴリ検出の結果が与えられたことを仮定し，画像中の物体2次元観測から物体の3次元パラメータの推定を行う．</li><li>提案手法の主な貢献としては①3次元表示：物体の3次元形状がクラス内で共通性が高いことから，大量なCADモデルから低次元なclass-specificな形状priorsを学習する．②2D-3Dマッピングを効率的行える新たなshape,poseの表示を提案した．(例:egocentricではなくallocentric視点を用いるなど)③提案手法を2D監督信号で学習可能にする予測した3次元形状を2次元にレンダリングし2次元のgtと比較することをベースとしたRender-Compareロス関数を提案した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/3D-RCNN-3D-Object-Reconstruction.png" alt="3D-RCNN-3D-Object-Reconstruction"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来のシーン理解は主にシーンに対しセマンティックセグメンテーションや物体検出などを行う．3次元空間のreasoningなどのタスクにおいては3次元のrepresentationが必要となる．また，従来の画像から3次元情報復元に関する研究は主に簡単なシーンから一つの物体に対し推定を行う．提案手法はより複雑なシーンの2次元画像から全部の物体インスタンスに対し3次元情報を推定できるため，自動運転の車・人の3次元情報推定などの様々な複雑なタスクに用いられる．</li><li>ジョイント物体検出と姿勢推定、バウンディングボクス領域内の物体三次元姿勢推定の2つのタスクにおいて，Pascal 3D+,KITTIデータセットでstate-of-the-artな精度を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>今後”analysis by synthesis”,”inverse graphics”などの概念の引用が増やしそう</p></li><li><p>かなり様々なところで工夫をしている．</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Kundu_3D-RCNN_Instance-Level_3D_CVPR_2018_paper.pdf">論文</a></p></li></ul></div></div><div class="slide_index">[#76]</div><div class="timestamp">2018.6.7 16:52:32</div></div></section><section id="Optimizing_Video_Object_Detection_via_a_Scale-Time_Lattice"><div class="paper-abstract"><div class="title">Optimizing Video Object Detection via a Scale-Time Lattice</div><div class="info"><div class="authors">Kai Chen et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>動画中の物体検出において精度とコストの柔軟な trade-off が可能となる Scale-Time Lattice を提案. Propagation and Refinement Unit を用いて時間とスケールについての upsampling を階層的に行う. ImageNet VID dataset を用いた評価実験では先行研究と同等の精度の結果を Realtime で得られた.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Optimizing-Video-Object-Detection-via-a-Scale-Time-Lattice.png" alt="fukuhara-Optimizing-Video-Object-Detection-via-a-Scale-Time-Lattice.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Propagation and Refinement Unit は入力された 2つのフレームの中間の時間のフレームでの推定結果を Motion History Image [Bobick+ 2001] を用いて推定し, その結果をもとにより大きなスケールでの推定を行う.</li><li>Propagation と Refinement を２段階行ったあとは, 残りの全フレームに対して線形補間を行う.</li><li>1段階目の入力となる Keyframe は, まず粗く一様にサンプリングした後, Keyframe 間の Propagation　の容易さ（物体の大きさが小さく, 動きが早いほど難しい）を評価し閾値を超えたら新しい中割りの Keyframe を動的に追加する.</li><li>ImageNet VID dataset を用いた評価実験の結果は 20fps のとき 79.6mAP, 62fps のとき 79.0 fps と先行研究([Feichtenhofer+ 17]が 5fps で 79.8mAP)と同等の高い推定精度を維持したまま Realtime での動作も可能であることが確認された.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.05472" target="blank">[論文] Optimizing Video Object Detection via a Scale-Time Lattice</a></li><li><a href="http://mmlab.ie.cuhk.edu.hk/projects/ST-Lattice/" target="blank">[Project page] Optimizing Video Object Detection via a Scale-Time Lattice</a></li></ul></div></div><div class="slide_index">[#77]</div><div class="timestamp">2018.6.3 14:41:55</div></div></section><section id="Distort-and-Recover_Color_Enhancement_using_Deep_Reinforcement_Learning"><div class="paper-abstract"><div class="title">Distort-and-Recover: Color Enhancement using Deep Reinforcement Learning</div><div class="info"><div class="authors">Jongchan Park et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>強化学習(DQN)を用いて automatic color enhancement を行う研究. 編集後の画像のみを利用して学習を行う方法（distort-and-recover scheme）を提案し, この学習方法の場合は従来の教師あり学習の手法よりも, 強化学習を用いる方が適していることを検証した. また, 評価実験では先行研究と同等か優位な結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Distort-and-Recover-Color-Enhancement-using-Deep-Reinforcement-Learning.png" alt="fukuhara-Distort-and-Recover-Color-Enhancement-using-Deep-Reinforcement-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>color enhancement の工程をマルコフ過程としてモデル化し, 強化学習(DQN)を用いて解いた.</li><li>従来手法のように編集前後の画像の組では無く, 編集後の画像のみを利用して学習を行う方法（distort-and-recover scheme）を提案.</li><li>action は様々な色調整の操作, reward は教師画像に特徴量がどれだけ近づいたかによって計算.</li><li>MIT-Adobe FiveK dataset を用いた評価実験やユーザースタディーでは先行研究と同等か優位な結果を達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.04450" target="blank">[論文] Distort-and-Recover: Color Enhancement using Deep Reinforcement Learning</a></li><li><a href="https://sites.google.com/view/distort-and-recover/" target="blank">[Project Page] Distort-and-Recover: Color Enhancement using Deep Reinforcement Learning</a></li></ul></div></div><div class="slide_index">[#78]</div><div class="timestamp">2018.5.29 16:50:55</div></div></section><section id="W2F_A_Weakly-Supervised_to_Fully-Supervised_Framework_for_Object_Detection"><div class="paper-abstract"><div class="title">W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection</div><div class="info"><div class="authors">Yongqiang Zhang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>弱教師ありの物体認識の学習を使用して, 教師あり物体認識を学習を行う研究. 弱教師ありの物体認識は物体中の最も特徴的な領域や, 複数の領域を抽出してしまう傾向があるが, それらの結果から教師データとして最もらしい Pseudo ground-truth を生成する方法を提案.  PASCAL VOC 2007 と 2012 を用いた評価実験では先行研究よりも優位な結果となった.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-W2F-A-Weakly-Supervised-to-Fully-Supervised-Framework-for-Object-Detection.png" alt="fukuhara-W2F-A-Weakly-Supervised-to-Fully-Supervised-Framework-for-Object-Detection.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>WSDNN [Bilen+ 16] の結果を OICR [Tang+ 17] を用いて改善したものを弱教師ありの物体認識の結果として使用. </li><li>上の結果に対して Pseudo ground-truth excavation (PGE) というアルゴリズムを適用することで, 物体全体を囲う Bounding Box を生成する.</li><li>更に, region proposal network [Ren+ 15] を用いて上の結果を改善したものを　Pseudo ground-truth とする.</li><li>Pseudo ground-truth を用いて, Fast RCNN [Girshick 15] や faster RCNN [Ren+ 15] などの教師あり物体認識の手法の学習を行う.</li><li>PASCAL VOC 2007, 2012 を用いて行った評価実験では先行研究 [Tang+ 17] [Krishna+ 16] と比較して mAP に置いて 5% 程度優位な結果となった.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://ivul.kaust.edu.sa/Documents/Publications/2018/W2F%20A%20Weakly-Supervised%20to%20Fully-Supervised%20Framework.pdf" target="blank">[論文] W2F: A Weakly-Supervised to Fully-Supervised Framework for Object Detection</a></li></ul></div></div><div class="slide_index">[#79]</div><div class="timestamp">2018.6.1 23:39:55</div></div></section><section id="Learning_Descriptor_Networks_for_3D_Shape_Synthesis_and_Analysis"><div class="paper-abstract"><div class="title">Learning Descriptor Networks for 3D Shape Synthesis and Analysis</div><div class="info"><div class="authors">Jianwen Xie, Zilong Zheng</div><div class="conference">CVPR 2018</div><div class="paper_id">1093</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>3次元ボリュームデータの形状特徴をモデリングできる深層畳み込みエネルギーベースなdescriptorネットワークを提案した．</li><li>提案の3D DescriptorNetがvoxelized形状の3D形状特徴を抽出できる．具体的には，voxelized形状のprobability density functionを定義した．また，3次元形状を特徴にマッピングできるボトムアップなボリューメトリックConvNetで特徴の統計またはエネルギー関数を定義した．</li><li>提案手法の貢献としては①ボリュームベースな3次元形状特徴をモデリングできる3D DescriptorNetを提案．②提案手法の学習プロセスをモードseeking,shiftingと解釈した．③形状検索に用いられるconditional 3D DescriptorNetを提案した．④3D形状生成モデルの新たな評価メトリクスを提案した．⑤3D GANを代替できる3D cooperative training schemeを提案した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning-descriptor-for-3D-volumetric.png" alt="Learning-descriptor-for-3D-volumetric"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来あまり提案されていないエネルギーベースな3次元形状descriptorを提案した．</li><li>提案の3D DescriptorNetを3次元形状生成，3次元形状検索，3次元形状スーパー解像度，3次元物体認識などタスクにおいて実験を行った．それぞれstate-of-the-artな性能を得られた．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>コードで実際のネットワーク構造を確認したい．</p></li><li><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xie_Learning_Descriptor_Networks_CVPR_2018_paper.pdf">論文</a></p></li></ul></div></div><div class="slide_index">[#80]</div><div class="timestamp">2018.6.7 13:16:29</div></div></section><section id="PointGrid_A_Deep_Network_for_3D_Shape_Understanding"><div class="paper-abstract"><div class="title">PointGrid: A Deep Network for 3D Shape Understanding</div><div class="info"><div class="authors">Truc Le, Ye Duan</div><div class="conference">CVPR 2018</div><div class="paper_id">1246</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>3D CNNに用いられる新たな3次元データの表示方法(volumetric grid及びpoints表示をコンバインした表示方法)及び3DCNNネットワークPointGridを提案した．提案の3次元データ表示方法は畳み込みができるregular構造でありながら，ポイントクラウドのローカル幾何情報を抽出できる．</li><li>提案PointGridの処理ポロセスは：①ポイントクラウドを-1,1の区間のユニットボクスに正規化する②cellでユニットボックスを分割し，cellごとのポイント数をKまたは0にダウンサンプリング（増強の場合もある），cell内のKポイントのx,y,zを3チャンネルの特徴として取り扱う．③前述した処理後の表示を3D encoderまたは3D U-Netにより物体識別、パーツセマンティックセグメンテーションに適用する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PointGrid.png" alt="PointGrid"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来の3次元表示方法の①occupacy gridやdistance fieldなどはレギュラー構造であるが，3次元形状の近似方法の特徴により低レベルの3次元局所情報しか表示できない，高レベルの特徴を表示するには高解像度が必要だが，CNNに用いたら処理・メモリ―コストが極めて高くなる．②PointNetがポイントクラウドを直接CNN処理を行えるが，max poolingだけでグローバル特徴の抽出を行っているので，局所的な情報抽出が弱い．以上の問題点から， CNN処理を行えるグリッドとポイント表示をコンバインした構造を提案し，occupacy gridより低解像度で豊かな情報を表示でき， PointNetより局所的情報の抽出が強いPointGridを提案した．</li><li>低解像度で有効的に3次元情報を表示できる．例：16，16，16解像度で良い性能を得られる（従来は64，64，64のボリューメトリックグリッド）</li><li>Modelnet-40, shape-netで物体識別及びパーツセグメンテーションの2つのタスクで従来の手法と相当レベルの精度を得られた（ボリューメトリックグリッド方法で最もメモリー消耗が少ない）．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>PointNetの考え方を従来のボリューメトリック方法の解像度削減に利用し，16，16，16解像度でも良い性能を得られるのが魅力的</li><li>提案のPointGridが構造的簡潔でほかのネットワークにも前処理の一部として用いられそう</li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Le_PointGrid_A_Deep_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#81]</div><div class="timestamp">2018.6.7 13:10:24</div></div></section><section id="Hybrid_Camera_Pose_Estimation"><div class="paper-abstract"><div class="title">Hybrid Camera Pose Estimation</div><div class="info"><div class="authors">Federico Camposeco, Andrea Cohen, Marc Pollefeys, Torsten Sattler</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>キャリブレーション済みのピンホールカメラにおいてカメラ姿勢推定問題を解く。例としてStructure-from-Motion (SfM)の2D-3Dマッチングを2D-2Dマッチングのように行う問題である。従来は構造ありの2D-3Dマッチングを解く絶対的なカメラ姿勢推定（absolute pose approaches）か、構造なしのテスクチャベースで2D-2Dマッチング（relative pose approaches）を行なっていたが、両者のいいとこ取りをする。本稿では新規にRANSACベースの手法を提案することで繰り返し最適化を行い、同問題の解決に取り組んだ。提案手法は、2D-3D/2D-2Dマッチングを同時にRANSACの要領で繰り返し最適化することができる（図を参照）。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180607HybridCameraPoseEstimation.png" alt="180607HybridCameraPoseEstimation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Structure-based/Structure-lessなマッチング（それぞれ2D-3D/2D-2Dに対応）を同時に解決する手法であるHybrid-RANSACを提案して、SfMの問題に対して適用した。両者のマッチングを単一の枠組みで実装しただけでなく、両者のいいとこ取りができる手法として完成させた。CVPRオーラルとして採択された。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>SfMのことはそこまで詳しくないのだが文章から「凄さ」が伝わってくる論文だった。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Camposeco_Hybrid_Camera_Pose_CVPR_2018_paper.pdf">論文</a></li></ul></div></div><div class="slide_index">[#82]</div><div class="timestamp">2018.6.7 09:00:34</div></div></section><section id="MegDet_A_Large_Mini-Batch_Object_Detector"><div class="paper-abstract"><div class="title">MegDet:A Large Mini-Batch Object Detector</div><div class="info"><div class="authors">Chao Peng, et al.</div><div class="paper_id">1711.07240</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>16~256のような大きなバッチサイズでも学習することができる、物体検出手法MegDetの提案。ミニバッチ数を上げられることから、GPUを効率的に使用することができ、学習速度を向上。複数のGPUからうまくバッチ正規化を行う、Cross-GPU Batch Normalizationを提案。これにより、33時間の学習を4時間に短縮、かつ高精度にうまいこと学習できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180606MegDet.jpg" alt="20180606MegDet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>2018年現在の著名な物体検出アルゴリズム(Faster R-CNNやMask R-CNNなど)は、全体のフレームワークやロスの設計に力を入れている。本研究では、手薄と思われるバッチサイズに着目し，新しいアプローチで精度向上を図っている。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>GPUの性能(メモリ数)の向上に伴って、この研究は生きてくる可能性がある。学習速度を上げながらCOCO2017一位はすごい。</p><ul><li><a href="https://arxiv.org/pdf/1711.07240.pdf">論文</a></li></ul></div></div><div class="slide_index">[#83]</div></div></section><section id="Rotation_Averaging_and_Strong_Duality"><div class="paper-abstract"><div class="title">Rotation Averaging and Strong Duality</div><div class="info"><div class="authors">Anders Eriksson, Carl Olsson, Fredrik Kahl, Tat-Jun Chin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>本稿では非凸問題の一種であるRotation Averagingに対してLagrangian Dualityを用いる。3次元再構成問題において、その画像群が「どこで、どのカメラ角度で、いつ撮影されたか？」に依存して再構成されるモデルが局所最適解に陥るという問題がRotation Averagingである（<a href="http://www.cs.cornell.edu/~bindel//blurbs/rotavg.html">Rotation averaging</a>）。図のようにカメラの移動軌跡やそのカメラアングルが変化した状態だと3次元再構成の局所解は大きく異なる（3次元再構成が表面のみ捉えていることに依存する）。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180607RotationAveraging.png" alt="180607RotationAveraging"><img src="slides/figs/180607RotationAveraging2.png" alt="180607RotationAveraging2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Structure-from-Motion (SfM)の重要タスクであるRotation Averagingの問題解決についてLagrangian Dualityを用いた全体最適化（局所最適解をできる限りの場面で脱することができた）を行ったことがもっとも大きな新規性である。シンプル/スケーラブルなアルゴリズムであり、大規模空間に対するSfMにも応用可能である。結果は下の図の通りであり、局所最適解を脱してより詳細な形状復元を行うことに成功した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ディープラーニングを使っていない側の問題！SfMの未解決問題？であるRotation Averagingを高いレベルで改善している。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Eriksson_Rotation_Averaging_and_CVPR_2018_paper.pdf">論文</a></li><li><a href="http://www.cs.cornell.edu/~bindel//blurbs/rotavg.html">Rotation Averaging</a></li></ul></div></div><div class="slide_index">[#84]</div><div class="timestamp">2018.6.7 08:11:11</div></div></section><section id="An_Unsupervised_Learning_Model_for_Deformable_Medical_Image_Registration"><div class="paper-abstract"><div class="title">An Unsupervised Learning Model for Deformable Medical Image Registration</div><div class="info"><div class="authors">Guha Balakrishnan, Amy Zhao, Mert R. Sabuncu, John Guttag, Adrian V. Dalca</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>脳の平均3D形状である脳アトラスの各ボクセルが患者の脳3次元データのどの位置に対応するか、という画像位置合わせ(image registration)をUnetを用いて正解データ無しの教師無し学習で行う手法を提案。
既存手法は最適化ベースだったが、学習ベースの画像位置合わせを初めて提案。トレーニング、検証で使用されているのは脳のMRIデータだが、
他のデータに対する画像位置合わせにも適用することが可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/An_Unsupervised_Learning_Model_for_Deformable_Medical_Image_Registration.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>U-netを用いた学習ベースの3次元画像における画像位置合わせ手法を提案。</li><li>比較は最適化ベースの手法である<a href="https://www.sciencedirect.com/science/article/pii/S1361841507000606">SyN</a>と行った。
SyNと同等の精度を達成し、一方で実行時間はCPU上では約160倍、GPU上では更にその156倍の速度で実行可能。</li><li>教師無し学習のため出力された脳アトラスの全体的な形状は異なっているが、各器官の位置はかなり高い精度で推定できていることが驚き。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>選択分野の勝利？手法に新規性は無く、検証で比較した手法も2008年のものとかなり古いが、それでも同等の精度で実行時間が速くなれば、それはCV分野としてはOKと判断されたのか？</li><li><a href="https://arxiv.org/abs/1802.02604">論文</a></li><li><a href="https://github.com/voxelmorph/voxelmorph">GitHub</a></li></ul></div></div><div class="slide_index">[#85]</div></div></section><section id="Recurrent_Scene_Parsing_with_Perspective_Understanding_in_the_Loop"><div class="paper-abstract"><div class="title">Recurrent Scene Parsing with Perspective Understanding in the Loop</div><div class="info"><div class="authors">Shu Kong, Charless Fowlkes</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>固定解像度で処理する画像認識システムでは、遠近感を持つシーンの画像において物体が任意のスケールを持つことが問題となる。(距離によって物体のスケールが変わる。カメラから遠いほど物体は小さく、近いほど大きい。)これ解決するために、物体のスケール(Depthに反比例)によってPoolingサイズを可変にするdepth-aware pooling moduleを提案。遠くの物体の細部は保持され、近くの物体は大きな受容野を持つことができる。
Depth画像は与えられるか直接RGB画像から推定され、Depth情報と意味的予測を利用するRecurrent Refinement Moduleにより、Semantic Segmentationを反復的に精錬する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Recurrent_Scene_Parsing_with_Perspective_Understanding_in_the_Loop.PNG" alt="Recurrent_Scene_Parsing_with_Perspective_Understanding_in_the_Loop.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>受容野のサイズを変化させるためにDepth情報を利用しこれを自然にCNNに組み込んだこと(geometricな情報を利用する先行研究はあり)。またこのDepth予測をSemantic Segmentationと互いに補い合う用にRecurrent Refinement Moduleを組み込んだこと。NYU-depth-v2の単眼深度推定においてstate-of-the-artな性能とSemantic Segmentationの性能改善を確認。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Recurrent refinement moduleのLoopにより物体の事前情報を捉えることができるが、Loopによる精度変化が小さい。Curriculum Learningと組み合わせるとおもしろそう。ResNetから得られる特徴はすでにスケールを考慮した特徴が抽出できているようにも思え、depth-aware pooling moduleが活かされているかというと疑問。</p><ul><li><a href="https://arxiv.org/abs/1705.07238">論文</a></li><li><a href="https://www.ics.uci.edu/~skong2/recurrentDepthSeg">Project Page</a></li><li><a href="https://github.com/aimerykong/Recurrent-Scene-Parsing-with-Perspective-Understanding-in-the-loop">GitHub</a></li></ul></div></div><div class="slide_index">[#86]</div><div class="timestamp">2018.6.6 19:36:41</div></div></section><section id="Mobile_Video_Object_Detection_with_Temporally-Aware_Feature_Maps"><div class="paper-abstract"><div class="title">Mobile Video Object Detection with Temporally-Aware Feature Maps</div><div class="info"><div class="authors">Mason Liu and Menglong Zhu</div><div class="conference">CVPR2018</div><div class="paper_id">698</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>モバイルや組み込み機器上で低消費電力かつリアルタイムに動作する物体検出のオンラインモデル．Single-Shotベースの物体検出モデルとLSTMを組み合わせたモデルである．また，通常のLSTMよりも計算コストを大幅に削減できるBottleneck-LSTMを提案する．Bottleneck-LSTMは，NチャンネルのBottleneck特徴マップ（Bt）を計算してすべてのゲートの入力をBtに置き換える．これによるゲート内の計算が減る．LSTM自体をDeepな構成にしても標準LSTMより効率的な計算が可能である．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180606_Bottleneck-LSTM1.jpg" alt="20180606_Bottleneck-LSTM1.jpg"><img src="slides/figs/20180606_Bottleneck-LSTM2.jpg" alt="20180606_Bottleneck-LSTM2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のVideo object detectionはフレームごとの検出に依存しているため，時間的情報を利用することができなかったが，本研究では検出器の速度を犠牲にせず時間的な情報を組み込んだ．ImageNet VID データセットでmobilenet-SSDよりも高精度（54.4mAP）に検出可能でありながら，モバイルCPU（Qualcomm Snapdragon 835, Xperia XZ Premiumなどに搭載）で15FPSの速さで検出できる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Googleでのインターン成果とのこと．リアルタイム検出は時系列情報があれば精度がよくなるが，それを入れることで速度の低下が起きてしまうのでこの2点のトレードオフになっている？</p><ul><li><a href="https://arxiv.org/abs/1711.06368">arXiv</a></li></ul></div></div><div class="slide_index">[#87]</div><div class="timestamp">2018.6.6 12:06:05</div></div></section><section id="Dense_Decoder_Shortcut_Connections_for_Single-Pass_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation</div><div class="info"><div class="authors">Piotr Bilinski, Victor Prisacariu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ResNeXtを用いたEncoder-Decoder（エンコーダ-デコーダ）構造、かつシングルパスのセマンティックセグメンテーション手法を提案する。エンコーダとデコーダは折り返したような構造になっており、エンコーダの特徴は図のように対称となる/同じサイズのデコーダ位置に統合される（enc1-dec1が対応）。今回は特にデコーダ側に改善があり、(1)コンテキスト情報を抽出、(2)セマンティック情報を生成、(3)異なる解像度の出力を適宜統合という新規性がある。これを実現するため、DenseNetを参考にしたDense Decoder Shortcut Connectionsを提案し、デコーダにおいてコンテキスト特徴を全て後段に渡すようにした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180606DenseDecoderShortcut.png" alt="180606DenseDecoderShortcut"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>デコーダにおいてDenseNetを参考にしたDense Decoder Shortcut Connectionsを提案、コンテキスト情報を後段に渡して精度を向上させた。ResNeXtの構造適用と合わせて各データセットにてState-of-the-artな精度を達成。NYUD datasetにて48.1（mean IoU）、CamVid datasetにて70.9（mean IoU）となった。PascalVOC2012においても81.2であった（SoTAはPSPNetの82.6）。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>セマンティックセグメンテーションの覇権争いが激化。ここら辺まで精度が向上すると確率的にSoTAになったりならなかったりする（回す回数が多いと一回くらい精度が高いモデルが学習される）？逆に、学習しやすい（誰が、どんなパラメータで回しても同じくらいの精度が出る）アーキテクチャというのが提案されてもよいかも。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://github.com/facebookresearch/ResNeXt">ResNeXt(facebookresearch)</a></li><li><a href="https://github.com/liuzhuang13/DenseNet">DenseNet</a></li><li><a href="https://github.com/hszhao/PSPNet">PSPNet</a></li></ul></div></div><div class="slide_index">[#88]</div><div class="timestamp">2018.6.6 09:38:43</div></div></section><section id="Recognize_Actions_by_Disentangling_Components_of_Dynamics"><div class="paper-abstract"><div class="title">Recognize Actions by Disentangling Components of Dynamics</div><div class="info"><div class="authors">Yue Zhao, Yuanjun Xiong, Dahua Lin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人物行動認識のための表現に対して、モーションとアピアランスの共起表現（Disentangling Components of Dynamics）を提案する。従来の人物行動認識に限らず動画認識ではRGBを入力とするアピアランス、オプティカルフローを画像に投影したフロー画像が用いられていたが、本論文ではそれらの共起表現を新たに提案した。フロー画像とは異なり、特に「アピアランスの変化」をカラー付きで表現できる。さらに、3Dプーリングを提案し、上記３つのチャンネルからの特徴を蓄積する手法についても考案した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180606DisentanglingAction.png" alt="180606DisentanglingAction"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>人物行動認識の文脈において、新規の特徴表現方法であるDisentangling Components of Dynamicsを提案した。同手法はフローとは異なり、RGB値の変化を効果的に捉える方法である。さらに、3Dプーリングも提案し、RGB/Flowも合わせた3チャンネルの特徴を適切にプーリングすることができる。フルモデルを用い、さらにKineticsにて事前学習を行った実験では、95.9%@UCF101を達成、従来の行動認識の大部分よりも高い精度を実現。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Kinetics Datasetの事前学習特徴が（やはり）強い。ImageNetでは91.8%だったものがImageNet+Kineticsで95.9%。転じて、やはりアルゴリズムなどよりもデータを用意するのがもっとも効果的。</p><ul><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Recognize_Actions_by_CVPR_2018_paper.pdf">論文</a></li><li><a href="https://zhaoyue-zephyrus.github.io/">著者</a></li></ul></div></div><div class="slide_index">[#89]</div><div class="timestamp">2018.6.6 09:16:56</div></div></section><section id="Single-Shot_Refinement_Neural_Network_for_Object_Detection"><div class="paper-abstract"><div class="title">Single-Shot Refinement Neural Network for Object Detection</div><div class="info"><div class="authors">Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and Stan Z. Li</div><div class="conference">CVPR2018</div><div class="paper_id">545</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>SSDをベースにした2つのモジュールから構成されるSingle-shotベースの物体検出アルゴリズム「RefineDet」を提案．Anchor Refine Module (ARM) とObject Detection Module (ODM) と呼ばれるモジュールと，2つを繋いで特徴マップを転送するTransfer Connection Block (TCB) からなる．ARMは物体が存在しない領域を示すNegative Anchor(※)の削減や，Anchorの粗い調整を行う．ODMはTCBを通じて特徴マップを受け取って座標の回帰およびクラス推定を行う．</p><p>※物体候補領域を示すBounding-boxをAnchorと呼ぶ．SSDでDefault boxと呼ばれているものと同じ．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180606_RefineDet1.jpg" alt="20180606_RefineDet1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>SSDで細かい物体をより精度よく検出するために，一度畳み込んだ特徴マップをDeconvしたりUp samplignしたりする手法がいくつかあるが，この手法はTCBで特徴マップを転送するときに1つ前 (=出力側) の特徴マップをDeconvして足している．Single-shotでありながら2つの役割分割されたモジュールがうまく連携している．推論速度は入力320x320で24.8ms (40.3FPS)，512x512で41.5ms (24.1FPS) @TITAN Xと非常に高速である．精度もDSSDより高性能 (VOC2007: 83.8mAP, MSCOCO: 41.8AP)である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Single-Shotベースの物体検出は前層の特徴マップを持ってくる系が流行り？精度も良い．</p><ul><li><a href="https://arxiv.org/abs/1711.06897">arXiv</a></li><li><a href="https://github.com/sfzhang15/RefineDet">GitHub</a></li></ul></div></div><div class="slide_index">[#90]</div><div class="timestamp">2018.6.6 01:36:45</div></div></section><section id="Neural_Kinematic_Networks_for_Unsupervised_Motion_Retargetting"><div class="paper-abstract"><div class="title">Neural Kinematic Networks for Unsupervised Motion Retargetting</div><div class="info"><div class="authors">Ruben Villegas, Jimei Yang, Duygu Ceylan, Honglak Lee</div><div class="conference">CVPR 2018 Oral</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>異なるキャラクタに対するモーションのリターゲティングをRNN、Cycle consisteny lossを用いることで教師なしで学習する手法を提案。RNNのencoder-decoderを用いて入力された関節位置、局所座標の原点の4次元モーションから、
各関節のクォータニオンと局所座標の4次元モーションを出力しそれをForwad Kinematicsによってターゲットキャラクターに転写する。
これを教師なしで行うためにCycle consistency loss、GAN lossを導入する。
これによって同じモーションを持った異なるキャラクタのデータが無い場合にも、モーションのリターゲティングを行うことが可能となる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Neural_Kinematic_Networks_for_Unsupervised_Motion_Retargetting.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>RNNのencoder-decoder、Cycle consistency lossを用いることで同じモーションを持った異なるキャラクタのデータが無い場合にも、モーションのリターゲティングが可能な手法を提案。</li><li>モーションのリターゲティングはオンラインで実行可能。</li><li> <a href="https://www.mixamo.com/#/">Mixamo animation data</a>を用いて、トレーニングは同じモーションを持たない７体のキャラクタの計1646のモーションを使用し、テストには６体のキャラクタを使用した。</li><li>RNN、RNNからrecurrent connectionを削除したMLP、入力モーションを単純にコピーした結果、ablation testを行い推定された関節位置のMSEを比較した結果、提案手法が最も高い精度を達成した。</li><li>特に入力モーションを単純にコピーした場合にはターゲットキャラクタの足が空中に浮いてしまったが、提案手法ではこれを防ぐことに成功している。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>クォータニオンの出力で止めているのは、クォータニオンがスケルトンに不変であることと、ボーンの回転角を制限するロス関数twist lossを取るためだと考えられる。</li><li>異なるキャラクタで同じモーションのGTがあるようなので、教師あり学習との比較を見てみたかった。一方でことモーションに関しては数値的には悪くても見た目では良し悪しがつかないということもあるので、これを考慮したのかもしれない。</li><li>Most of this work was done during Ruben’ internship at Adobe.</li><li><a href="https://arxiv.org/abs/1804.05653">論文</a></li></ul></div></div><div class="slide_index">[#91]</div></div></section><section id="Cross-Domain_Weakly-Supervised_Object_Detection_through_Progressive_Domain_Adaptation"><div class="paper-abstract"><div class="title">Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation</div><div class="info"><div class="authors">Naoto Inoue, Ryosuke  Furuta, Toshihiko Yamasaki, Kiyoharu Aizawa</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>インスタンスレベルのアノテーションを持つソースドメイン(S)とイメージレベルのアノテーションを持つターゲットドメイン(T)を用いてdomain adaptationを行い、Tに対する物体検出を行う手法を提案。Sを用いて物体検出器のプリトレーニングを行い、
Cycle GANによってSをTに変換した画像を用いて物体検出器のfine-tuningを行う。
続いてSとそのイメージレベルのアノテーションを用いて半教師学習を行いSに対する物体検出を行う。
半教師学習を行う際にインスタンスレベルのアノテーションが施されたデータセットが必要なため、
クリップアート、水彩画、漫画のデータセットの構築も行っている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Cross-Domain_Weakly-Supervised_Object_Detection_through_Progressive_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Cycle GANによる検出器のfine-tuning、半教師学習による物体検出というステップをへてイメージレベルのアノテーションを持つ実画像ではないドメイン(クリップアートなど)に対する物体検出手法を提案。</li><li>Clipart1k, Watercolor2k, Comic2kという、それぞれクリップアート1000枚、水彩画2000枚、漫画2000枚の画像に対してインスタンスレベルのアノテーションを施したデータセットを構築。</li><li>自ら構築した三種のデータセットにおいて教師なし学習、半教師学習、SSD300、YOLOv2と比較した結果、最も高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>検証しているラベル数が最大でも20と少ないことが気になった。これはターゲットドメインの構築が難しかったからであり、データさえあればラベルを増やすことができるのだろうか？</li><li><a href="https://arxiv.org/abs/1803.11365">論文</a></li><li><a href="https://naoto0804.github.io/cross_domain_detection/">Project page</a></li><li><a href="https://github.com/naoto0804/cross-domain-detection">GitHub       </a></li></ul></div></div><div class="slide_index">[#92]</div></div></section><section id="Real-Time_Monocular_Depth_Estimation_Using_Synthetic_Data_With_Domain_Adaptation_via_Image_Style_Transfer"><div class="paper-abstract"><div class="title">Real-Time Monocular Depth Estimation Using Synthetic Data With Domain Adaptation via Image Style Transfer</div><div class="info"><div class="authors">Amir Atapour-Abarghouei, Toby P. Breckon</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>合成画像とそのデプス画像、そして実世界画像を用いてunsupervised domain adaptaionを行うことで、実世界画像に対するデプス画像を生成する手法を提案。
実世界画像に対するデプスのアノテーションは困難であり、かつ枚数も多くない。
一方合成画像に対するデプスのアノテーションは完璧だが、
実世界画像に対する推定を行うときにドメインシフトが起きてしまう。
提案手法ではUnetによって合成画像からデプスを推定し、Cycle GANによって実世界画像を合成画像に変換することでデプスを推定する手法を提案。
GPUを用いることで44FPSで実行することが可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Real-Time_Monocular_Depth_Estimation_Using_Synthetic_Data_With_Domain_Adaptation_via_Image_Style_Transfer.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ラベルなし実世界画像とラベルあり合成画像に対してCycle GANによるスタイルトランスファーによりdomain adaptaionを行うことで、実世界画像のデプスを推定する手法を提案。</li><li>合成画像、KITTIデータセットでトレーニングを行い、KITTIデータセットの推定精度をstate-of-the-artと比較した結果、最も高い精度を達成。</li><li>Cycle GANによるスタイルトランスファーでは急激な照明変化や影を物体として認識してしまうといったリミテーションが存在する。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>Cycle GANによってdomain adaptationを行う割合ベーシックな手法だが、その推定精度がstate-of-the-artに優っている。</li><li><a href="http://breckon.eu/toby/publications/papers/abarghouei18monocular.pdf">論文</a></li><li><a href="http://dro.dur.ac.uk/24333/">Project page</a></li><li><a href="https://vimeo.com/260393753">Vimeo</a></li></ul></div></div><div class="slide_index">[#93]</div></div></section><section id="Unsupervised_Domain_Adaptation_with_Similarity_Learning"><div class="paper-abstract"><div class="title">Unsupervised Domain Adaptation with Similarity Learning</div><div class="info"><div class="authors">Pedro Pinheiro</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ソースドメイン(S)の各カテゴリの重心ベクトルと、S・ターゲットドメイン(T)から得られたadversarial featuresの行列積を用いることでdomain adaptation(DA)を行う手法を提案。
従来のDAではSとTのそれぞれから得られる特徴量をGANによってdomai-confusionを行い、
Sで学習したラベル識別器をTに適用するという手法だった。提案手法ではadversarial-confusionに加えて、
Sの各カテゴリにおける重心ベクトルとgeneratorから得られる特徴量の類似度を高くするように学習しDAを行う手法を提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Unsupervised_Domain_Adaptation_with_Similarity_Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>domain-confusionに加えてラベルごとの重心ベクトルとgeneratorから得られる特徴量の類似度を高くするように学習しDAを行う手法を提案。</li><li>MNIST・USPS・MISNT-M、Officde-31, VisDAデータセットで検証。11のdomain adaptationにおいて、9つの設定においてstate-of-the-artよりも高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>この論文に限らずDAを提案する論文ではdomain-confusionを可視化しており、数値評価だけではなく、ドメインの分布の可視化画像を載せることも重要だと思われる。</li><li><a href="https://arxiv.org/abs/1711.08995">論文</a></li></ul></div></div><div class="slide_index">[#94]</div></div></section><section id="Image-Image_Domain_Adaptation_with_Preserved_Self-Similarity_and_Domain_Dissimilarity_for_Person_Re-identification"><div class="paper-abstract"><div class="title">Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</div><div class="info"><div class="authors">Weijian Deng, Univ. of Chinese Academy; Liang Zheng, Qixiang Ye, Guoliang Kang, Yi Yang, Jianbin Jiao</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>人物認証(person re-ID)の精度が落ちないようにソースドメインの人物画像をターゲットドメインの画像に変換するSimilarity Preserving GAN(SPGAN)を提案。ドメイン間の変換をCycleGANで行う。
またそれぞれのperson re-IDのデータセットには基本的に同じ人物は写っていないということを利用して、
ソースドメインとターゲットドメインで異なるデータセットを使用し、
ターゲットドメインへと変換された画像はIDが保たれ、かつターゲットドメインのどの人物のIDとも一致しないように学習を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Image-Image_Domain_Adaptation_with_Preserved_Self-Similarity_and_Domain_Dissimilarity_for_Person_Re-identification.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>person re-IDデータセットの特徴を生かしドメイン変換された画像はターゲットドメインの人物画像とは一致せず、かつ元々のIDを生かすように学習を行い、ドメイン間で人物画像の変換を行うSPGANを提案。</li><li>Market-1501、Duke-MTMC-reIDデータセットで検証を行い、一方のデータセットの人物画像をもう一方のドメイン画像に変換した際に正しくre-IDができるのかを検証した。</li><li>ベースラインであるCycleGANや教師なし学習のstate-of-the-artと比較して最も高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>person re-IDのタスクの中でもソースドメインの人物がターゲットドメインに存在する場合にも発見する、というタスクを解いている。</li><li>ソースドメインとターゲットドメインに含まれるIDが全く違う、ということを逆手にとった手法。</li><li><a href="https://arxiv.org/abs/1711.07027">論文</a></li></ul></div></div><div class="slide_index">[#95]</div></div></section><section id="Boosting_Domain_Adaptation_by_Discovering_Latent_Domains"><div class="paper-abstract"><div class="title">Boosting Domain Adaptation by Discovering Latent Domains</div><div class="info"><div class="authors">Massimiliano Mancini , Lorenzo Porzi, Samuel Rota Bulò, Barbara Caputo, Elisa Ricci</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>domain adaptaion(DA)に対して、ソースデータは潜在的に複数のドメインで構成されていると仮定し、ソースサンプルがどのドメインに所属しているかを精度よく識別するためにMulti-domain DA layer(mDA-layer)を導入することで、
ターゲットのラベルの識別精度を向上させる手法を提案。
実験ではmulti-soure domain adaptationを行うことでその有効性を検証している。
ソースデータないのドメインを識別するCNNの特徴量を用いることで、ターゲットドメインのラベル識別の精度が向上している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Boosting_Domain_Adaptation_by_Discovering_Latent_Domains.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>mDA layerによってマルチソースドメイン内のドメインを識別する学習を行うことで、ターゲットドメインのラベル識別に有効な特徴量を獲得。</li><li>MNIST・MISNT-m・USPS、Office-31、Office-Caltech、PACSデータセットで提案手法の有効性を検証。state-of-the-artのmulti-source domain adaptation(DA)よりも高い精度を達成。</li><li>ソースサンプルにドメインのラベルが全くない場合とラベルがない場合でも、精度は1%ほどしか変わらない。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1805.01386">論文</a></li></ul></div></div><div class="slide_index">[#96]</div></div></section><section id="Large_Scale_Fine-Grained_Categorization_and_the_Effectiveness_of_Domain-Specific_Transfer_Learning"><div class="paper-abstract"><div class="title">Large Scale Fine-Grained Categorization and the Effectiveness of Domain-Specific Transfer Learning</div><div class="info"><div class="authors">Yin Cui Yang Song, Chen Sun, Andrew Howard, Serge Belongie</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>鳥の種族などより細かいラベルを推定するdomain-specific fine-grained visual categorization(FGVC) taskにおいて、効果的なトレーニングデータセットの構築方法を提案。
事前実験からターゲットドメインの画像の見た目に近い画像を含むソースドメインでトレーニングするほど、
識別精度が高くなるということを発見している。
ターゲットドメインに含まれる画像の見た目に近い画像を多く持つソースドメインのクラスをいくつか選択することで
トレーニングデータセットを構築する。画像の見た目はEarth Mover’s Distanceで測定され、
7つのfine-grainedデータセットにおいて提案手法が効果的であることを示した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Large_Scale_Fine-Grained_Categorization_and_the_Effectiveness_of_Domain-Specific_Transfer_Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>FGVCを行う際のトレーニングスキームとして、ImageNetのような大規模データセットやクラスごとのデータ数が偏っているiNatを学習するのではなく、
より効果的なトレーニングデータセットを構築する手法を提案。</li><li>fine-grainedデータセットCUB200、Stanford Dogs、Flower-102、Stanford Cars、Aircraft、Food101、NABirdsで検証した結果、5つのデータセットにおいて提案手法によって構築されたトレーニングデータセットで
学習した場合に最も高い精度を達成。</li><li>classificationで使用したネットワークはResNet、Inception、Squeeze-and-Excitationであり識別ネットワーク自体には依存しないことも検証している。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>手法自体は単純ながら、事前実験に基づく論文展開や既存手法に対して投げかけた疑問を回収できたところが評価されたと思われる。</li><li><a href="https://vision.cornell.edu/se3/wp-content/uploads/2018/03/FGVC_CVPR_2018.pdf">論文</a></li></ul></div></div><div class="slide_index">[#97]</div></div></section><section id="Residual_Parameter_Transfer_for_Deep_Domain_Adaptation"><div class="paper-abstract"><div class="title">Residual Parameter Transfer for Deep Domain Adaptation</div><div class="info"><div class="authors">Artem Rozantsev, Mathieu Salzmann, Pascal Fua</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ソースドメインを学習したネットワークのパラメタを残差ブロックで変換することでターゲットドメインへdomain adaptaionを行う手法を提案。
既存手法ではドメインに普遍な特徴量を学習していたためにネットワークのパラメタが多すぎてしまう。
提案手法は学習時には残差ブロックとソースドメインを学習するネットワークのファインチューニングを行い、
ソースドメインに対するラベルの識別と2つのドメインに対してadversarial domain adaptationを行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Residual_Parameter_Transfer_for_Deep_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ドメインに普遍な特徴量を学習するのではなく、ソースドメインを学習したネットワークの重みをソースドメイン用に変換することでパラメタ数を抑えかつ精度の高い
domain adaptationを実現。</li><li>state-of-the-artと比べて、SVHN・MNIST、UAV-200データセット、Officeデータセットにおいてもっとも高い精度を達成。</li><li>ソースドメインを学習するネットワークがResNetのような深いネットワークの場合にも有効であることを主張。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.07714">論文</a></li></ul></div></div><div class="slide_index">[#98]</div></div></section><section id="Importance_Weighted_Adversarial_Nets_for_Partial_Domain_Adaptation"><div class="paper-abstract"><div class="title">Importance Weighted Adversarial Nets for Partial Domain Adaptation</div><div class="info"><div class="authors">Jing Zhang, Zewei Ding, Wang Ding, Wanqing Li, Philip Ogunbona</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ターゲットドメインがソースドメインが所持するクラスの一部しか持たずかつラベルがない場合であるpartial domain adaptationをadversarial netベースで行う手法を提案。
adversarila netの手前いにドメインを識別するclassifierを用意し、
このclassifierが精度良く判別可能なソースサンプルはターゲットドメインには含まれていないクラスに所属している可能性が高いので重みを小さくし、
逆にconfidenceが低いソースサンプルはターゲットにも存在するクラスに所属している可能性が高いので重みを大きくする。
この重みとソースサンプルを掛け合わせたものとターゲットサンプルをadversarial netで学習させる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Importance_Weighted_Adversarial_Nets_for_Partial_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>4つのドメインを持つOffice+Caltech-10において、ソースは各ドメインで10のラベル、ターゲットは各ドメインで5つのラベルを使用。同様の設定でOffice-31データセット、Caltech256→Office10データセットで実験を行った。</li><li>partial domain adaptationのstate-of-the-artである<a href="https://arxiv.org/abs/1707.07901">SAN</a>と比較して8つの実験のうち4つの設定でより高い精度を達成。</li><li> <a href="https://arxiv.org/abs/1707.07901">SAN</a>ではソースのクラスの数だけclassifierを必要とするが、提案手法で必要なclassifierは2つのみ。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.09210">論文</a></li></ul></div></div><div class="slide_index">[#99]</div></div></section><section id="Domain_Generalization_with_Adversarial_Feature_Learning"><div class="paper-abstract"><div class="title">Domain Generalization with Adversarial Feature Learning</div><div class="info"><div class="authors">Haoliang Li, Sinno Jilain Pan, Shiqi Wang, Alex Kot</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>Adversarial Autoencoder(AAE)に対してMaximum Mean Discrepancy(MMD)を導入することでトレーニングデータを過学習することなくdomain generalizationを行う手法を提案。
domain generalizationとは、複数ドメインのラベル付きデータセットを学習し、
テスト時にはデータセットに含まれていないドメインのデータセットにおける識別や生成タスクを行うことを指す。
複数のソースドメインで不変な特徴量を取得する<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Ghifary_Domain_Generalization_for_ICCV_2015_paper.pdf">multi-task learning</a>に対して、提案手法ではMMDベースでドメイン間の差分をとることと、
AAEによって特徴量空間に対して事前分布が押し込むことでソースドメインに対する過学習が防ぐ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Domain_Generalization_with_Adversarial_Feature_Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>AAEに対してMMDを組み込むことで、ソースドメインを過学習することなくdomain generalizationを行う。</li><li>domain generalizationのstate-of-the-artと識別タスクにおいて比較。</li><li>MNISTを15度刻みで回転させた場合の認識精度、VLCSデータセットにおける物体認識、IXMASにおける行動認識においてstate-of-the-artよりも高い精度を達成。</li><li>AAEにおける事前分布の違いによる精度も議論しており、ラプラシアン分布が最も精度が良かったと主張。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://www.researchgate.net/publication/324691022_Domain_Generalization_with_Adversarial_Feature_Learning">論文</a></li></ul></div></div><div class="slide_index">[#100]</div></div></section><section id="Adversarial_Feature_Augmentation_for_Unsupervised_Domain_Adaptation"><div class="paper-abstract"><div class="title">Adversarial Feature Augmentation for Unsupervised Domain Adaptation</div><div class="info"><div class="authors">Riccardo Volpi, Pietro Morerio, Silvio Savarese, Vittorio Murino</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>特徴量空間におけるデータオーギュメンテーションとソースドメインとターゲットドメインに不変な特徴量を取得することでunsupervised data adaptationを行う手法を提案。
右図にあるようにstep1で、ソースドメインとノイズをデコードして生成されたベクトルをGANにかけ、
特徴量空間においてソースドメインに対するオーギュメンテーションを行う。
続いてstep2において、ソースドメインとターゲットドメインを同一のエンコーダーに入力することでドメインに不変な特徴量を取得する。
ベースラインである<a href="https://arxiv.org/abs/1702.05464">Adversarial discriminative domain adaptation</a>ではドメインごとにエンコーダーを使用していたが、提案手法ではエンコーダーは一つ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Adversarial_Feature_Augmentation_for_Unsupervised_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>GANを用いてソースドメインの特徴量空間でデータオーギュメンテーションを行い、かつソースドメインとターゲットドメインに不変な特徴量を推定することで、unsupervised data adaptationを行った。</li><li>ベースラインである<a href="https://arxiv.org/abs/1702.05464">Adversarial discriminative domain adaptation</a>に対して上記の2つの拡張の有効性を議論している。</li><li>state-of-the-artと比較して、数字の識別、物体の識別において既存手法と同等かそれ以上の精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>Limitationにも書かれているようにsourceとtargetのラベが同じになる保証はなく、最終的な精度はsourceのエンコーダーがどれほどうまく学習できているかに強く依存する。</li><li><a href="https://arxiv.org/abs/1711.08561">論文</a></li><li> <a href="https://github.com/ricvolpi/adversarial-feature-augmentation">GitHub</a></li></ul></div></div><div class="slide_index">[#101]</div></div></section><section id="Dynamic_Video_Segmentation_Network"><div class="paper-abstract"><div class="title">Dynamic Video Segmentation Network</div><div class="info"><div class="authors">Yu-Syuan Xu, Tsu-Jui Fu, Hsuan-Kung Yang, Chun-Yi Lee</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>動画像セグメンテーションの問題に対してネットワーク選択（Decision Network）を行い適応的にCNNモデルを処理するDynamic Video Segmentation Network (DVSNet)を提案する。同手法では性質の異なるふたつのネットワーク（深くて精度が高いが低速/浅くて精度は低いが高速）を組み合わせて交通シーンにおけるシーン解析にて高速な処理を実現する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180605DynamicVideoSegmentationNetwork.png" alt="180605DynamicVideoSegmentationNetwork"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>DVSNetは低速なもので70.1%/20fps、高速なものだと65.2%/34.4fps（いずれもCityScapes datasetにて処理した結果）を達成する。両者を、トレードオフを考慮してあらゆる場面に適応することができるという意味で新規性がある。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>こういう通し方があったのか、と勉強になる。実利用を想定し、トレードオフを考慮、それを解決することも重要な問題である。</p><ul><li><a href="https://arxiv.org/abs/1804.00931">論文</a></li><li><a href="https://challengerocket.com/files/photo_projects/he/hell/hellochick/-sadasd_4f5749/31bc7a20.pdf">論文（査読ver.）</a></li></ul></div></div><div class="slide_index">[#102]</div><div class="timestamp">2018.6.5 21:14:03</div></div></section><section id="Deep_Cross-media_Knowledge_Transfer"><div class="paper-abstract"><div class="title">Deep Cross-media Knowledge Transfer</div><div class="info"><div class="authors">Xin Huang, et al.</div><div class="paper_id">1803.03777</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>画像とテキストなどの異なるメディアタイプ間で検索する、クロスメディア検索手法のcross-media knowledge transfer(DCKT)の提案。大規模なクロスメディアデータセットの知識を、小規模なデータセットのモデルに転移学習する。メディアレベルと相関性レベルでのドメインの違いを最小化するために、2レベルでドメイン変換することで精度向上。また、ドメインの違いを徐々に減らすようにトレーニングサンプルを選択することで、モデルがより頑健になる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180605_DCKT.jpg" alt="20180605_DCKT.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>マルチメディア分野における検索。既存の手法では、ラベル付きデータを学習する方法が多いが、大規模なデータの収集とラベル付けは手間取るため問題とされる。そこで、既存のデータを転移して解決する。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.03777.pdf">論文</a></li></ul></div></div><div class="slide_index">[#103]</div></div></section><section id="Dynamic_Graph_Generation_Network_Generating_Relational_Knowledge_from_Diagrams"><div class="paper-abstract"><div class="title">Dynamic Graph Generation Network: Generating Relational Knowledge from Diagrams</div><div class="info"><div class="authors">Daesik Kim, et al.</div><div class="paper_id">1711.09528</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>視覚情報とテキストの情報が抽象的に統合された図であるダイアグラムを解析するためのunified diagram parsing network(UDPnet)の提案。入力は様々なイラストやテキスト、レイアウトを持つ図のみ。物体検出器によって、図内のグラフ構造を推論し、新手法であるdynamic graph generation network(DGGN)によってグラフを生成。生成されたグラフからテキストで関係性を出力する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180605_DGGN.jpg" alt="20180605_DGGN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>ダイアグラムのような図には、豊富な知識が含まれているが、固有の特性やレイアウトの問題から、コンピュータに自動的に理解させる方法はあまり提案されていない。本手法では、物体検出器やRNNを統合し、ダイアグラムから知識をテキストとして生成する。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>自然画像でなく，人間による作為的なグラフ理解において優れている。人間の意図や、人間にとって自然な解釈を学習できているのではないか。</p><ul><li><a href="https://arxiv.org/pdf/1711.09528.pdf">論文</a></li></ul></div></div><div class="slide_index">[#104]</div></div></section><section id="Instance_Embedding_Transfer_to_Unsupervised_Video_Object_Segmentation"><div class="paper-abstract"><div class="title">Instance Embedding Transfer to Unsupervised Video Object Segmentation</div><div class="info"><div class="authors">Siyang Li, Bryan Seybold, Alexey Vorobyov, Alireza Fathi, Qin Huang, C.-C. Jay Kuo</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体インスタンス特有の特徴（同じ物体領域に属しているか？）を捉えることでビデオに対する教師なしの物体セグメンテーションを実施する。ここでは静止画で捉えた特徴を、ビデオに表れる物体候補/オプティカルフローと組み合わせて物体のインスタンスセグメンテーションを実施。本論文ではさらに、ビデオに対するfine-tuningなしに高精度なセグメンテーション手法を構築したと主張している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180605InstanceEmbeddingTransfer.png" alt="180605InstanceEmbeddingTransfer"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>静止画の学習パラメータを動画に適用していく、その際に物体候補/オプティカルフローと統合していくことで動画的な表現を教師なしで獲得していく。DAVIS datasetを用いた評価で78.5%、FBMS datasetにて71.9%（いずれもmean Intersection-over-Union (mIoU)の評価にて）を達成し、それぞれのデータセットでState-of-the-art。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>"Without finetuning"というのもアピールになるということを勉強した（ただしそれでstate-of-the-artである必要がある？）。</p><ul><li><a href="https://arxiv.org/abs/1801.00908">論文</a></li><li><a href="https://github.com/philferriere/tfvos">GitHub(Semi-Supervised Object Segmentation)</a></li></ul></div></div><div class="slide_index">[#105]</div><div class="timestamp">2018.6.5 08:58:32</div></div></section><section id="Depth-Aware_Stereo_Video_Retargeting"><div class="paper-abstract"><div class="title">Depth-Aware Stereo Video Retargeting</div><div class="info"><div class="authors">Bing Li, Chia-Wen Lin, Boxin Shi, Tiejun Huang, Wen Gao, C.-C. Jay Kuo</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ステレオビデオ（Stereo Video）に対するリターゲティング（Retargeting）を扱う。ステレオ（かつビデオ）に対するリターゲティングは従来のリターゲティングと比較すると、動画中の顕著性が高い物体の把握やダイナミクスを含むためまだ新しくチャレンジングな課題である。ここに対して、Depth-aware Fidelity Constraint（距離画像から推定される信頼性のようなもの）を適用することで物体の顕著性を把握しつつ3次元空間を再構成することができる（リターゲティングと3次元再構成の同時推定問題）。最適化にはTotalCost関数を適用して物体の顕著性を把握しつつ形状、時間情報、距離画像のディストーションを推定。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180605DepthRetargeting.png" alt="180605DepthRetargeting"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ステレオビデオの入力から、顕著性の把握、形状推定、時間情報、距離画像のディストーションを同時推定し、従来法であるCVWよりも綺麗なリターゲティング画像を生成することに成功した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>VR/AR、3D映画などに使える！より自然に見せることで映像酔いを軽減することができる？</p><ul><li><a href="http://alumni.media.mit.edu/~shiboxin/files/Li_CVPR18.pdf">論文</a></li><li><a href="http://alumni.media.mit.edu/~shiboxin/">著者</a></li><li><a href="https://news.mynavi.jp/article/computer_vision-43/">リターゲティング（マイナビ記事）</a></li><li><a href="https://ieeexplore.ieee.org/document/7055924/">CVW（従来手法）</a></li></ul></div></div><div class="slide_index">[#106]</div><div class="timestamp">2018.6.5 08:23:28</div></div></section><section id="Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data"><div class="paper-abstract"><div class="title">Frustum PointNets for 3D Object Detection from RGB-D Data</div><div class="info"><div class="authors">Charles R. Qi, et al.</div><div class="paper_id">1711.08488</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>屋内および屋外シーンにおける3D物体検出手法のfrustum PointNetsの提案。まず、RGBデータからCNNで2Dの物体候補領域を推定する。次に、点群の深度情報を用いて、各物体領域の視錐台(viewing frustum)を推定する。最後に、frustum PointNetsによって3Dバウンディングボックスを推定。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180604_PointNets.jpg" alt="20180604_PointNets.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来の手法では、画像や3Dボクセルに処理を加えて、3Dデータの自然なパターンや不変性を曖昧にしている。本手法では、RGB-Dスキャンによって生の点群データを直接操作する。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>2Dと3Dで別々のネットワークを使うことで、小さな物体やオクルージョン、まばらな点群についても正確に推定することができる。リアルタイムも実現。</p><ul><li><a href="https://arxiv.org/pdf/1711.08488.pdf">論文</a></li></ul></div></div><div class="slide_index">[#107]</div></div></section><section id="Dynamic_Zoom-in_Network_for_Fast_Object_Detection_in_Large_Images"><div class="paper-abstract"><div class="title">PhaseNet for Video Frame Interpolation</div><div class="info"><div class="authors">Mingfei Gao, et al.</div><div class="paper_id">1711.05187</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>高解像度画像に出現する様々なサイズの物体を、精度の維持と処理コストの低減を実現しながら検出するフレームワークの提案。最初はダウンサンプリングされた粗い画像から、次に高解像度の細かい画像から検出する。強化学習を用いた2つのネットワークで構成。R-net：低解像度の画像を入力し、その検出結果を用いて高解像度領域を解析する。これにより、どの順番にズームインすべき判断できる。Q-net：ズームの履歴を使用し、拡大領域を順次選択。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604Dynamic_Zoom-in.jpg" alt="180604Dynamic_Zoom-in.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>しっかり検出する範囲を絞ることで処理量を低減、効率化を図ることができる。基本的な検出の構造はいじっていない。処理する画素数を約70％、処理時間を50％以上短縮し、なおかつ高い検出性能を維持できる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>YOLOやSSDなどの物体検出手法の精度向上にも使える。</p><ul><li><a href="https://arxiv.org/pdf/1711.05187.pdf">論文</a></li></ul></div></div><div class="slide_index">[#108]</div></div></section><section id="Efficient_Video_Object_Segmentation_via_Network_Modulation"><div class="paper-abstract"><div class="title">Efficient Video Object Segmentation via Network Modulation</div><div class="info"><div class="authors">Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, Aggelos K. Katsaggelos</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>セグメンテーションを実行する際に任意のアノテーション済み物体を事前情報（Spatial Prior）として高精度化を図るための技術を提供する。本論文では、最初の一フレームに対してセグメンテーションを行うだけで、動画中の物体に対してセグメンテーションを行うモデルを提案する。アノテーションから抽出した事前情報はニューラルネットの中間層にて情報を挿入して抽象化を行う。図は提案のフレームワークを示しており、VisualModulator（初期フレームのアノテーションから視覚的なガイドを行う）、SegmentationNet（VisualModulator/SpatialModulatorの補助を受けつつ、RGB画像の入力からセグメンテーションを実行）、SpatialModulator（空間的にどこらへんに対象物体があるかをサポート）の３つのコンポーネントから構成される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604NetworkModulation.png" alt="180604NetworkModulation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>最初のフレームのアノテーションのみから動画セグメンテーションを実行するという問題を提供した、さらに視覚的な特徴量/位置的な事前知識をセグメンテーションのネットワークに導入し、動画セグメンテーションを高精度化した点が評価された。動画セグメンテーションタスクであるDAVIS2016にて74.0、YoutubeOjbsにて69.0（処理速度は0.14second/image）であった。State-of-the-artには劣る（それぞれ79.8, 74.1）が、処理速度では優っている（提案 0.14 vs. 従来 10.0）。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>メタ学習の枠組みを使用している。</p><ul><li><a href="https://arxiv.org/abs/1802.01218">論文</a></li><li><a href="https://github.com/linjieyangsc/video_seg">GitHub</a></li></ul></div></div><div class="slide_index">[#109]</div><div class="timestamp">2018.6.4 20:56:17</div></div></section><section id="Real-world_Anomaly_Detection_in_Surveillance_Videos"><div class="paper-abstract"><div class="title">Real-world Anomaly Detection in Surveillance Videos</div><div class="info"><div class="authors">Waqas Sultani, Chen Chen, Mubarak Shah</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>監視カメラの文脈において異常検出を実行する研究である。ここで、異常検出においてビデオに対して時間のアノテーションを付与するのは非常にコストのかかる作業であるが、ここに対して弱教師付き学習の一種であるMultiple Instance Learning (MIL)を適用して正常/異常ラベルが付いたビデオから異常検出を行うモデルDeep Anomaly Ranking Modelを提案する。さらに、13種類の異常シーン（e.g. road accident, robbery）を収集したデータセットを提供することで同問題の解決を実践した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604AnomalyDetection.png" alt="180604AnomalyDetection"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>弱教師付き学習であるMILをベースとして異常検出を行なった、おそらく初めての例であり、その精度は従来法による精度を上回りState-of-the-artとなった（AUCにて75.41を達成）。また、1900の動画に対して13種類の異常を収集したデータセットを構築し、公開した。同データセットは合計で128時間にも及ぶ。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>異常の動画データセットを公開したことが評価できるポイント。現在ではYouTube検索とダウンロードである程度のデータセットは構築できそう？（ここらへんを効率化する研究自体があってもよい）</p><ul><li><a href="https://arxiv.org/abs/1801.04264">論文</a></li><li><a href="http://crcv.ucf.edu/cchen/">Project</a></li><li><a href="http://crcv.ucf.edu/projects/real-world/">DB</a></li></ul></div></div><div class="slide_index">[#110]</div><div class="timestamp">2018.6.4 20:22:54</div></div></section><section id="Normalized_Cut_Loss_for_Weakly-supervised_CNN_Segmentation"><div class="paper-abstract"><div class="title">Normalized Cut Loss for Weakly-supervised CNN Segmentation</div><div class="info"><div class="authors">M. Tang, A. Djelouah, F. Perazzi, Y. Boykov, C. Schroers</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>Weakly-supervisedなセマンティックセグメンテーション手法があって，その方針はインタラクティブに部分的に正解（シードとか）を与えるというものである．そこで，よく用いられるロス関数（クロスエントロピー等）で評価しようとすると，教示の塗りミスが致命的になったりする．そもそも設計的にエラーが考慮されていないからである．</p><p>本論文では，非Deepな手法で行われていた評価指標に基づく新たなロス関数Normalized Cut Lossを提案．</p><p>従来法と違うところは，提案するロス関数におけるクロスエントロピーの部分は，ラベルが既知のシードの部分での評価だけやっているという点．<a href="http://slideplayer.com/slide/6125890/18/images/29/Example+Normalized+Cut.jpg">Normalized Cut</a>はゆるく全ピクセルに対する一貫性の評価を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Normalized_Cut_Loss_for_Weakly-supervised_CNN_Segmentation.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Fully-supervisedな手法と同レベルの性能を実現できた．</p><p>従来法の知見を活かした橋渡し的手法．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Disney Researchのインターンでやった模様．</p><ul><li><a href="https://arxiv.org/abs/1804.01346">arXiv</a></li></ul></div></div><div class="slide_index">[#111]</div><div class="timestamp">2018.6.4 12:33:29</div></div></section><section id="Burst_Denoising_with_Kernel_Prediction_Networks"><div class="paper-abstract"><div class="title">Burst Denoising with Kernel Prediction Networks</div><div class="info"><div class="authors">B. Mildenhall, J.T. Barron, J. Chen, D. Sharlet, R. Ng, R. Carroll</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>携帯含む最近のカメラは連写機能が付いているので，手ブレのあるようなハンドヘルドカメラの連写で撮ったノイズ入り画像をデノイズしようという話．<strong>連続撮影における手ブレに頑健なデノイズCNN</strong>を提案する．</p><p>写実的ノイズ定式化に基づく，インターネットから拾ってきた加工済み画像からカメラで撮ったような写実的画像を生成する合成データ生成手法で学習データを作成．学習中に空間的に変化するカーネルを使い，位置調整とデノイズを実現．
不慮の局所解落ち回避のための，焼きなましロス関数をガイドとした最適化．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Burst_Denoising_with_Kernel_Prediction_Networks.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>流行に乗った手法（合成データによる学習，適応的パラメータ調整）を使って実現．問題設定も地に足がついている感じがする．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Google Researchのインターンでやった模様．</p><ul><li><a href="https://arxiv.org/abs/1712.02327">arXiv</a></li><li><a href="http://people.eecs.berkeley.edu/~bmild/kpn/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#112]</div><div class="timestamp">2018.6.4 10:40:17</div></div></section><section id="MaskLab_Instance_Segmentation_by_Refining_Object_Detection_with_Semantic_and_Direction_Features"><div class="paper-abstract"><div class="title">MaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction Features</div><div class="info"><div class="authors">Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng Wang and Hartwig Adam</div><div class="conference">CVPR2018</div><div class="paper_id">525</div></div><div class="slide_editor"><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/">Ryosuke Araki</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体のBounding-box detection, Semantic segmentationとDirection predictionを同時に行うモデル「MaskLab」を提案する．Faster R-CNN・ResNet-101をベースに，Bounding-box内の前景と背景をわけることでSegmentationを行う．Mask R-CNNと違い，Segmentationを行うときは単純に前景背景分割をするだけでなくクラス分類も行い，また，各ピクセルのDirectionを予測して同じクラスの重なっている物体のInstance segmentationも可能である．また，検出されたBox内でさらに切り出しを行い，小さな物体の検出をしやすくする仕組みも入れている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180604_MaskLab1.jpg" alt="20180604_MaskLab1.jpg"><img src="slides/figs/20180604_MaskLab2.jpg" alt="20180604_MaskLab2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Object detectionとSemantic segmentationを同時にEnd-to-endで解くモデルの提案．それだけでなく，Semantic segmentationではDirectionを考慮して高精度な認識が可能である．MSCOCOで性能評価を行い，FCIS+++（mAP，Seg：33.6），Mask R-CNN（Seg：35.7，Det：38.2）よりも高い性能（学習時にScale augmentationを行いSeg：38.1，Det：43.0）を達成した．Res-NeXtを用いたMask R-CNN（Seg：37.1，Det：39.8）よりも高性能である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>最近，Detection + Segmentationがいくつか出てきているので今後に注目．検出速度に関する記述は見当たらなかったが，Faster R-CNNベースなのでそれ相応の速度だと思われる．ワンショット系の検出器に適応してこの精度を保ちつつ高速な検出ができればウケそう？</p><ul><li><a href="https://arxiv.org/abs/1712.04837">arXiv</a></li></ul></div></div><div class="slide_index">[#113]</div><div class="timestamp">2018.6.4 10:00:39</div></div></section><section id="Making_Convolutional_Networks_Recurrent_for_Visual_Sequence_Learning"><div class="paper-abstract"><div class="title">Making Convolutional Networks Recurrent for Visual Sequence Learning</div><div class="info"><div class="authors">Xiaodong Yang, Pavlo Molchanov, Jan Kautz</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>RNNの改良であり、畳み込み層や全結合層の役割を前処理として構造に入れ込むPreRNNを提案した。従来のRNNとPreRNNの違いは図に示すとおりである（従来型TraditionalなRNNは構造内にfc/conv+avepoolを要するが、PreRNNではそれらを内包している）。このPreRNNを用いて、より有効だと思われるタスクーSequential Face Alighnment, Dynamic Hand Gesture Recognition, Action Recognitionにて適用した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604PreRNN.png" alt="180604PreRNN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来型のRNNを改善して、fc-layer/conv+avepool-layerをその構造の中に取り込んだPreRNNを提案し、複数タスク（顔アライメント推定、ジェスチャ認識、人物行動認識）にて従来法よりも高い精度を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>画像キャプションなどにも効果あり？どのように説明文が改善されるのか試してみたい。</p><ul><li><a href="http://xiaodongyang.org/publications/papers/prernn-cvpr18.pdf">論文</a></li><li><a href="http://xiaodongyang.org/publications/papers/prernn-supp-cvpr18.pdf">SupplementaryMaterial</a></li></ul></div></div><div class="slide_index">[#114]</div><div class="timestamp">2018.6.4 09:29:31</div></div></section><section id="Inferring_Shared_Attention_in_Social_Scene_Videos"><div class="paper-abstract"><div class="title">Inferring Shared Attention in Social Scene Videos</div><div class="info"><div class="authors">Lifeng Fan, Yixin Chen, Ping Wei, Wenguan Wang, Song-Chun Zhu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>複数人いる人物が同時に同領域に注意を向けることをCo-attention/Shared-attentionといい、本論文では三人称視点の入力からこの推定に取り組む。ここに対してConvLSTM（Convolutional Long-Short Term Memory）を用いたモデルを適用、さらにはVideoCoAttと呼ばれるTV番組をメインとしたビデオからデータ収集を行なった。モデルは視線推定（YOLOv2による顔検出も含む）、領域推定（Region Proposal Map）、空間推定（Convolution）と時系列最適化（LSTM）から構成される。データは380ビデオ/492,000フレームから構成される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604SharedAttention.png" alt="180604SharedAttention"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>新しい問題である、三人称視点からの共注視を設定し、データとモデルを公開したことが採択された理由である。また、実験により従来法を抑えて、提案法が71.4%の精度かつ誤差がもっとも小さい手法であることを明らかにした。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>共注視、面白い！（が、ビデオを見てみると曖昧な部分もありもうすこしアノテーションなどに改善の余地がある？）</p><ul><li><a href="http://www.stat.ucla.edu/~lifengfan/SharedAttention_CVPR18/shared_attention_camera_ready.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=4uA5buFgi38">YouTube</a></li><li><a href="http://www.stat.ucla.edu/~lifengfan/shared_attention">Project</a></li></ul></div></div><div class="slide_index">[#115]</div><div class="timestamp">2018.6.4 09:07:48</div></div></section><section id="Aperture_Supervision_for_Monocular_Depth_Estimation"><div class="paper-abstract"><div class="title">Aperture Supervision for Monocular Depth Estimation</div><div class="info"><div class="authors">Pratul P. Srinivasan, Rahul Garg, Neal Wadhwa, Ren Ng, Jonathan T. Barron</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Aperture Supervision（カメラのフォーカスによる教示）により単眼画像からデプスマップを推定する研究である。これを推定するために、Focus/Defocusを処理して、領域ごとの反応を確認することでデプスの教示に相当する。CNNベースの距離画像推定では、確率的距離マップ、Shallow Depth-of-field（各距離における重み付けされたマップ）を適用する。図は本論文における単眼カメラによる距離画像推定のパイプラインである。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604MonoDepth.png" alt="180604MonoDepth"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>RGB-Depthを変換する、いわゆるダイレクトな距離画像推定では計算コストも高く、かつ解像度も低かったが、本論文ではフォーカスに関係する教示によりこの問題を解決し、単眼による距離画像推定を実現した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>距離画像を直接的には使わなくても、LightFieldなどの情報から距離画像を推定することができるので、他の関連手法とは異なるアプローチを与えている。</p><ul><li><a href="https://arxiv.org/abs/1711.07933">論文</a></li><li><a href="https://github.com/google/aperture_supervision">GitHub</a></li></ul></div></div><div class="slide_index">[#116]</div><div class="timestamp">2018.6.4 08:48:55</div></div></section><section id="Deep_End-to-End_Time-of-Flight_Imaging"><div class="paper-abstract"><div class="title">Deep End-to-End Time-of-Flight Imaging</div><div class="info"><div class="authors">Shuochen Su, Felix Heide, Gordon Wetzstein, Wolfgang Heidrich</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>End-to-EndでセンサデータからToFセンサの出力を行うToFNet (Time-of-Flight Network)を提案する。従来のシステムであh、センサーデータの入力からデノイジング、Phase Unwrapping (PU)やMultipath Correction (MP)を行っていたが、ToFNetでは一括処理が可能となるだけでなく、ノイズがない鮮明な画像を出力可能、リアルタイムで動作可能である。ToFNetはPatchGANという枠組みにより最適化が行われる。PatchGANはEncoder-Decoderの構造をした生成器と非常にシンプルな構造の識別器により構成される。誤差はL1+DepthGradient+Adversarialと、その重み付き和により計算される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180604E2EToF.png" alt="180604E2EToF"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のカスケード型処理（デノイジング、PU、MP）ではノイズが蓄積してしまいがちだが、提案のToFNetは一括での処理を行い、(1)ノイズを鮮明に除去できるのみならず(2)リアルタイムでの処理が可能である。主にこの２点が採択された理由であると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Depth推定、すでに数値や見た目による判断が曖昧になりつつある？屋内だけでなく、多様なドメインでの適応が待たれる。</p><ul><li><a href="http://vccimaging.org/Publications/Su2018EndToEndTOF/Su2018EndToEndTOF.pdf">論文</a></li><li><a href="http://vccimaging.org/Publications/Su2018EndToEndTOF/">Project</a></li></ul></div></div><div class="slide_index">[#117]</div><div class="timestamp">2018.6.4 08:26:21</div></div></section><section id="Dont_Just_Assume_Look_and_Answer_Overcoming_Priors_for_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering</div><div class="info"><div class="authors">Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrel and Dawn Song</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>VQAの学習は学習データの答えの分布に依存してしまう。そこで、答えの分布が異なる学習データを用いて学習した場合でもGrounded Visual Question Answering(GVQA)を提案した。
GVQAでは質問に答える上で、(1)必要な情報を認識する（例：物体の色を聞かれている場合対象となる物体を認識する)(2)必要な答えを推測する(例：物体の色を聞かれている場合色を答える)の2つが重要であると仮定する。
そこで、画像から質問に答えるために必要な情報を抽出する部分と答えを推定する部分の2つに分けたモデルを構築した。
その際、質問から質問のタイプ(yes/noで答えられるか)を推定することで、質問の答えを異なるネットワークによって出力させる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Dont_Just_Assume_Look_and_Answer_Overcoming_Priors_for_Visual_Question_Answering.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>質問の答えの分布を学習データとテストデータで異なる分布にしたVQA-CPデータセットを提案した。同データセットを用いて従来手法及びGVQAの精度を調べたところ、従来のデータセットと比べた際の従来手法の精度低下及びGVQAの方が高い精度を記録したことを示した。
また、GVQAによって答えの根拠を説明することが可能となった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.00377">論文</a></li></ul></div></div><div class="slide_index">[#118]</div><div class="timestamp">2018.6.4 02:20:01</div></div></section><section id="Fooling_Vision_and_Language_Models_Despite_Localization_and_Attention_Mechanism"><div class="paper-abstract"><div class="title">Fooling Vision and Language Models Despite Localization and Attention Mechanism</div><div class="info"><div class="authors">Xiaojun Xu, Xinyun Chen, Chang Liu, Anna Rohrbach, Trevor Darrel and Dawn Song</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Adversarial attackが、VisionとLanguageの融合問題のようにより複雑な問題に対しても有効であるかを調査した。対象とするタスクは、画像キャプショニング及びVQAとして画像のAdversarial exampleによる出力の変化を調べた。
また、これらの手法におけるlocalizationがAdversarial Attackに影響されるかを確認した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Fooling_Vision_and_Language_Models_Despite_Localization_and_Attention_Mechanism.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Dense Captionについては、97％の確率で騙すことに成功した。同じ画像の同じ領域に対しても目標とするキャプションが異なると異なるキャプションを出力させることが可能なことを確認した。
VQAについてもごく一部を除いて騙すことができることを確認した。
Attention Mapを確認すると、Adversarial exampleを入力した場合異なる領域に注目していることが明らかになった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1709.08693">論文</a></li></ul></div></div><div class="slide_index">[#119]</div><div class="timestamp">2018.6.4 00:26:41</div></div></section><section id="Visual_Question_Reasoning_on_General_Dependency_Tree"><div class="paper-abstract"><div class="title">Visual Question Reasoning on General Dependency Tree</div><div class="info"><div class="authors">Qingxing Cao, Xiaodan Liang, Bailing Li, Guanbin Li, Liang Lin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>VQAの答えだけでなく判断根拠も出力する手法を提案。質問をtree構造に分解し、各nodeに関する情報(例：plane)が画像中のどこに存在するかを示すattention mapを求める。
既に得られているattentionマップ及びhidden stateを更新していくことで、質問の答えとたどり着いていく。
最終的な質問の答えはhidden stateを用いて求める。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Visual_Question_Reasoning_on_General_Dependency_Tree.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>質問への回答の精度は従来手法と比べて大きく向上されているわけではない。従来の判断根拠を求める研究はルールを人間が設計するもしくはground truthが必要であるのに対してこれらを必要とせずに回答根拠を得ることに成功。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00105">論文</a></li></ul></div></div><div class="slide_index">[#120]</div><div class="timestamp">2018.5.29 19:59:57</div></div></section><section id="Blind_Predicting_Similar_Quality_Map_for_Image_Quality_Assessment"><div class="paper-abstract"><div class="title">Blind Predicting Similar Quality Map for Image Quality Assessment</div><div class="info"><div class="authors">Da Pan, Ping Shi, Ming Hou, Zefeng Ying, Sizhe Fu and Yuan Zhang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像の品質を評価するためのBlind Predicting Similar Quality Map for IQA(BPSQM)を提案した。CNNを用いた画像の品質評価手法は数多く提案されているが、その大半はブラックボックスとなっている。
本研究は、ピクセル単位の画像の損失度合いを示すquality mapを始めに推定することで、画像圧縮などに伴いどのように画像の品質が低下してるかの可視化を可能とした。
また、qualityマップから画像の損失度合いを表すスコアの算出を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Blind_Predicting_Similar_Quality_Map_for_Image_Quality_Assessment.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のquality mapを求める手法は、損失前の画像(reference)が必要なものが大半であり、reference不要なCNNベースの手法はパッチ単位で推定するのみであった。それに対して本研究は、referenceなしでピクセル単位のquality mapを推定することを可能とした。
損失度合いの推定に関しても、referenceなしの手法と比べて精度の向上を実現した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1805.08493">論文</a></li></ul></div></div><div class="slide_index">[#121]</div><div class="timestamp">2018.6.3 21:52:18</div></div></section><section id="AMNet_Memorability_Estimation_with_Attention"><div class="paper-abstract"><div class="title">AMNet: Memorability Estimation with Attention</div><div class="info"><div class="authors">Jiri Fajtl, Vasileios Argyriou, Dorothy Monekosso, Paolo Remagnino</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像中の記憶に残りやすい領域（Memorability）を可視化するネットワークであるAMNet（Attention and Memorability Network?）の提案。ResNet50による特徴表現、LSTMにより実装されたAttention構造の仕組みによりMemorabilityスコアを算出する。アノテーションは従来研究であるLaMem（下記リンク参照）に使用したデータセットであるSUN Memorability（同じく下記参照）を用いて学習を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180603AMNet.png" alt="180603AMNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来法よりも精度が良かった（より人間の記憶の構造に近かった？）ことを示した。これはアテンション構造を用いていることが、より人間の記憶の仕組みにおいて再現性が良かったことを示しているといえる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>記憶の仕組みも人間の直感が必要な高次機能の再現である。このように高次なラベリングが今後は増えてくると思うし、人間のタスクをカバーする意味でも重要になるか？</p><ul><li><a href="https://arxiv.org/abs/1804.03115">論文</a></li><li><a href="https://github.com/ok1zjf/AMNet">GitHub</a></li><li><a href="http://memorability.csail.mit.edu/download.html">LaMem</a></li><li><a href="http://web.mit.edu/phillipi/Public/WhatMakesAnImageMemorable/">SUN Memorability</a></li></ul></div></div><div class="slide_index">[#122]</div><div class="timestamp">2018.6.3 23:48:57</div></div></section><section id="Lose_The_Views_Limited_Angle_CT_Reconstruction_via_Implicit_Sinogram_Completion"><div class="paper-abstract"><div class="title">Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion</div><div class="info"><div class="authors">Rushil Anirudh, Hyojin Kim, Jayaraman J. Thiagarajan, K. Aditya Mohan, Kyle Champley, Timo Bremer</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>手荷物検査や医療用として用いられるComputed Tomography (CT)画像の復元を、限られた角度のSinogramの入力から行う技術（CTNet）を提案する。CTNetは1D/2D畳み込みで構成され、SinogramからFull-viewのCT画像を復元することができる。図はCTNetの学習とテストを示したものである。学習時にはGAN-likeな手法により構成され、入力から1DCNNにより特徴量を生成、GeneratorがCT画像を復元、DiscriminatorがReal/Fakeを判断することでGeneratorを鍛える。テスト時にはさらにFBP (Filtered Back Projection)/WLS (Weighted Least Squares)なども用いて最終的な結果を得る。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180527CTNet.png" alt="180527CTNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>角度が限定されたx線画像から、360度のCT画像を生成するというチャレンジングな試みを行ったことが評価された。同課題に対してGAN-likeな手法を提案し、手法的な新規性も打ち出せたことが採択された基準であると考える。PSNRやセグメンテーションベースの方法で評価を行い、従来法よりも優れた手法であることを示した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>CT画像を復元できてしまうのがすごい！</p><ul><li><a href="https://arxiv.org/abs/1711.10388">論文</a></li></ul></div></div><div class="slide_index">[#123]</div><div class="timestamp">2018.5.28 00:07:00</div></div></section><section id="Learning_to_Extract_a_Video_Sequence_from_a_Single_Motion-Blurred_Image"><div class="paper-abstract"><div class="title">Learning to Extract a Video Sequence from a Single Motion-Blurred Image</div><div class="info"><div class="authors">Meiguang Jin, Givi Meishvili, Paolo Favaro</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>１枚のブラー画像から時系列フレームを推定して動画像を生成するアプローチを提案。モーションブラーは通常、カメラなどセンサによる露光により発生するが、その分解は非常に困難な問題として扱われていた。本論文では平均化を除去してフレームを時系列方向に並べ、次にDeconvolutionを復元して同問題に取り組む（この問題は通常、Blind Deconvolutionと言われる）。提案法では、深層学習の手法としてこの両者を実現する構造を構築。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180527SingleMotionBlurredImage.png" alt="180527SingleMotionBlurredImage"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Blind Deconvolutionの課題を取り扱っているが、さらにここでは単一のブラー画像から動画像を生成するアルゴリズムや深層学習アーキテクチャを提案した。特に、ブラー画像から時系列画像を順次復元するための誤差関数を提案したことが最も大きな新規性である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>もともとあった問題に少し味付けして、新しい問題を作り出すセンスが欲しい。。</p><ul><li><a href="https://arxiv.org/pdf/1804.04065.pdf">論文</a></li></ul></div></div><div class="slide_index">[#124]</div><div class="timestamp">2018.5.27 23:44:14</div></div></section><section id="Learning_to_Detect_Features_in_Texture_Images"><div class="paper-abstract"><div class="title">Learning to Detect Features in Texture Images</div><div class="info"><div class="authors">Linguang Zhang, Szymon Rusinkiewicz</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>テクスチャに対して有効かつスケーラブル、さらに学習可能な局所特徴量を提案する。さらに提案手法は既存のランキングロスやFully-Convolutional Networks (FCN; 全層畳み込みネットワーク)と統合可能である。著者らは、新規の学習誤差関数であるPeakednessという指標を畳み込みマップに対して導入した。画像はテスト画像に対して提案手法を施した結果であり、Repeatableな特徴量（画像の中に再帰的に登場するテクスチャ特徴）が検出されている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180527LearningToDetectFeatures.png" alt="180527LearningToDetectFeatures"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>（i）FCN構造によりフルサイズの再帰的なテクスチャパターンを評価することに成功した、（ii）Peakednessという指標を導入し、これを最大化することでテクスチャを評価するための畳み込みマップを洗練化することに成功、という点がもっとも重要な新規性である。実験ではcarpet/asphalt/wood/tile/granite/concrete/coarseといったテクスチャパターンに対して有効であることを示した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>複雑かつ特徴が比較的取りづらいテクスチャの解析は今後さらに重要性を増すと考えられる（道路面のひび割れ調査など）。ここに教師なし学習（Self-Supervision含む）が導入されていくことになると思う。</p><ul><li><a href="http://gfx.cs.princeton.edu/pubs/Zhang_2018_LTD/keypoint-cvpr.pdf">論文</a></li><li><a href="http://gfx.cs.princeton.edu/pubs/Zhang_2018_LTD/index.php">Project</a></li></ul></div></div><div class="slide_index">[#125]</div><div class="timestamp">2018.5.27 23:19:17</div></div></section><section id="Smart_Sparse_Contours_to_Represent_and_Edit_Images"><div class="paper-abstract"><div class="title">Smart, Sparse Contours to Represent and Edit Images</div><div class="info"><div class="authors">T.Dekel, C.Gan, D.Krishnan, C.Liu and W.T.Freeman</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1712.08232</div></div><div class="slide_editor">Kota Yoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>元画像の輪郭情報から画像を再構成する手法を提案.GANをベースとして，入力情報が与えられない領域のテクスチャと細部を合成する.実験では，顔認証システムや人間を対象にして元画像と再構成された画像と区別されないという結果となった．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Smart_Sparse_Contours_to_Represent_and_Edit_Images.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Pix2pixなどの既存の手法よりも大幅に向上している．</li><li>2つのネットワークで構成されており，1つ目のネットワークでは，画像全体の構造，色を再構成，2つ目のネットワークでは画像のテクスチャと細部の表現をしている．</li><li>直感的な操作が可能で，顔のパーツを移動させたり，追加させることもできる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>入力情報がない輪郭と輪郭の間の画像部分の再構成にも力を入れてる</p></li><li><p><a href="https://arxiv.org/abs/1712.08232">Paper</a></p></li></ul></div></div><div class="slide_index">[#126]</div><div class="timestamp">2018.6.3 19:36:19</div></div></section><section id="R-FCN-3000_at_30fps_Decoupling_Detection_and_Classification"><div class="paper-abstract"><div class="title">R-FCN-3000 at 30fps: Decoupling Detection and Classification</div><div class="info"><div class="authors">Bharat Singh, Hengduo Li, Abhishek Sharma and Larry S. Davis</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Kodai Nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>オブジェクト性検出と分類を分離した物体検出器であるR-FCN-3000を提案した．RoIのための検出スコアを得るために，オブジェクト性検出と分類スコアをかける．
R-FCNで提案されたposition-sensitive filterはfine-grained classificationには必要ないというのが基本アイディア．
また本論文では，R-FCN-3000はオブジェクト数が増えると性能が向上することが示されている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/R-FCN-3000_1.PNG" alt="R-FCN-3000_1.PNG"><img src="slides/figs/R-FCN-3000_2.PNG" alt="R-FCN-3000_2.PNG"><img src="slides/figs/R-FCN-3000_3.PNG" alt="R-FCN-3000_3.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ImageNet detection datasetで一秒あたり30枚の画像を処理したところ，mAPが34.9%であった（YOLO9000は18%）．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.01802.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#127]</div><div class="timestamp">2018.6.2 21:48:26</div></div></section><section id="Learning_to_See_in_the_Dark"><div class="paper-abstract"><div class="title">Learning to See in the Dark</div><div class="info"><div class="authors">Chen Chen, Qifeng Chen, Jia Xu and Vladlen Koltun</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kodai Nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>暗い環境において，同じシーンを短時間露光で撮影した暗い画像と長時間露光で撮影した明るい画像のrawデータを集めたデータセットを提案した．このデータセットは，5094個の暗い画像のrawデータと424個の明るい画像のrawデータが1対多で対応付けられている．
インドアとアウトドアの両方で撮影を行った．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning-to-see-in-the-dark-1.PNG" alt="Learning-to-see-in-the-dark-1.PNG"><img src="slides/figs/Learning-to-see-in-the-dark-2.PNG" alt="Learning-to-see-in-the-dark-2.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>このデータセットを用いてFCNをトレーニングし，テストしたところ図に示すような結果が得られた．このネットワークはrawデータを直接扱うため，図に示すように，従来の画像処理パイプラインの多くの代わりになる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1805.01934.pdf">論文URL</a></li><li><a href="https://github.com/cchen156/Learning-to-See-in-the-Dark">github</a></li></ul></div></div><div class="slide_index">[#128]</div><div class="timestamp">2018.6.2 18:44:28</div></div></section><section id="AVA_A_Video_Dataset_of_Spatio-temporally_Localized_Atomic_Visual_Actions"><div class="paper-abstract"><div class="title">AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions</div><div class="info"><div class="authors">C. Gu et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">大規模な新規動画データセットを構築．
従来の動画データセットが複合的な行動ラベルを扱うのに対して，
このデータセットではStand, Sit, WatchのようなAtomicな行動ラベル (80 classes) を扱う．
このようなラベルが1秒間隔で動画中のすべての人にアノテーションされており，
しかもBounding Boxまで付いているというのがこのデータセットの強み．
80種類ものAtomicな行動ラベルが大規模にしかも密に付いているデータセットは初．
加えて，Two-stream I3D & Faster R-CNNというような手法を提案．
従来のSpatio-temporal Action Localization用のデータセットではSOTAを達成したものの，
このデータセットは15.6% mAPと問題の難しさも主張している．
</div></div><div class="item2"><img src="slides/figs/AVA_A_Video_Dataset_of_Spatio-temporally_Localized_Atomic_Visual_Actions.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>Bounding Boxまでアノテーションされている初の大規模動画データセットを構築</li><li>動画中の一部ではなく密にAtomicな行動のラベルがアノテーションされている</li><li>Spatio-temporal Localizationをするためのベンチマークとなる新規手法も提案</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1705.08421">論文 (arXiv)</a></li><li><a href="https://research.google.com/ava/">データセット</a></li><li><a href="http://activity-net.org/challenges/2018/tasks/guest_ava.html">ActivityNet Challenge 2018 Task B</a></li></ul></div></div><div class="slide_index">[#129]</div><div class="timestamp">2018.6.1 14:53:37</div></div></section><section id="SGAN_An_Alternative_Training_of_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">SGAN: An Alternative Training of Generative Adversarial Networks</div><div class="info"><div class="authors">Tatjana Chavdarova, Idiap and EPFL; Francois Fleuret, Idiap Research Institute</div><div class="conference">CVPR2018</div><div class="paper_id">1712.02330</div></div><div class="slide_editor">KenichiroWani</div><div class="item1"><div class="text"><h1>概要</h1><p>General Advesarial Networks(GAN)は現在，コンピュータビジョン分野で広く使われている手法である．しかしながら，複雑な学習をするには時間がかかり，人の手が必要となる．そこでSGANというトレーニングプロセスを検討する．SGANではいくつかの敵対的でローカルなネットワークの組み合わせを独立させて学習させることでグローバルな一対のネットワークの組み合わせを学習することができる．SGANの学習はローカルディスクリミネータとジェネレータによってグローバルディスクリミネータとジェネレータが学習される．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SGAN_An_Alternative_Training_of_Generative_Adversarial_Networks.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>adversarial pairs (G1,D1),...,(GN,DN)を学習し， G0はD1,...,DNによって学習， D0はG1,...,GNによって学習させることでグローバルな一対のネットワークを学習する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.02330">arxiv</a></li></ul></div></div><div class="slide_index">[#130]</div><div class="timestamp">2018.5.30 21:15:54</div></div></section><section id="Conditional_Generative_Adversarial_Network_for_Structured_Domain_Adaptation"><div class="paper-abstract"><div class="title">Conditional Generative Adversarial Network for Structured Domain Adaptation</div><div class="info"><div class="authors">W.Hong, Z.Wang, M.Yang and J.Yuan</div><div class="conference">CVPR2018</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>コンピュータによって学習用のアノテーションを生成し，実画像のような合成画像として用いることが流行．しかし，ドメインの不一致という問題が起きる．それを解決するために，GANをFCNフレームワークに統合することでSemanticSegmentationのためのドメイン適用のための手法を提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Conditional_Generative_Adversarial_Network_for_Structured_Domain_Adaptation.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>合成画像の特徴を実画像のように変換する条件付きジェネ−レータとディスクリメーターを学習</li><li>ジェネレータは合成画像を実画像のようにディスクリメーターを騙すように学習させることでFCNのパラメータを更新．</li><li>本手法である実際のラベルを用いずに実験を行い，CityscapesデータセットのIoU平均が12〜20上回りSoTA．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>FCN＋GANでSemanticSegmentation</li><li><a href="https://weixianghong.github.io/publications/papers/CVPR_18.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#131]</div><div class="timestamp">2018.5.28 15:36:39</div></div></section><section id="Learning_to_Sketch_with_Shortcut_Cycle_Consistency"><div class="paper-abstract"><div class="title">Learning to Sketch with Shortcut Cycle Consistency</div><div class="info"><div class="authors">Jifei Song, Kaiyue Pang, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像からスケッチのストロークを取得する手法の提案。人間が画像からスケッチをすると、同じ画像に対しても様々なバリエーションが生じてしまう。
そこで、教師有学習と教師無学習を組み合わせることによって画像からスケッチの取得を実現する。
教師有学習は、画像からスケッチもしくはスケッチから画像という変換を学習する。
教師無学習は、オートエンコーダのように画像もしくはスケッチを符号化し、元に戻すという処理を学習する。
その際、CycleGANのようにドメイン変換を繰り返すのではなく、符号化したものをそのまま復号化する(Shortcut Cycle)。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Sketch_with_Shortcut_Cycle_Consistency.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Pix2pixやCycleGANなどの手法と比較を行い、いずれの手法と比較してもスケッチとして抽象化されつつもセマンティックな特徴を捉えていることを確認した。また、数値評価としてスケッチの認識及び検索タスクを行って評価した。
どちらのタスクにおいても、従来手法と比較して高い精度でスケッチへの変換ができていることを示した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1805.00247">論文</a></li></ul></div></div><div class="slide_index">[#132]</div><div class="timestamp">2018.5.29 14:10:18</div></div></section><section id="Show_Me_a_Story_Towards_Coherent_Neural_Story_Illustration"><div class="paper-abstract"><div class="title">Show Me a Story: Towards Coherent Neural Story Illustration</div><div class="info"><div class="authors">Hareesh Ravi, Lezi Wang, Carlos M. Muniz, Leonid Sigal, Dimitris N. Metaxas, Mubbasir Kapadia</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>複数の文で構成されたテキストの内容を表す画像シークエンスを検索する手法を提案。文章から抽出される特徴と画像から抽出された特徴を対応付けることにより、各文に対して1枚の画像を選択する。
その際、文章特徴はGRUによって前後の文章との関係を含めて抽出する。
また、heやitなどの代名詞が何を指しているかを明らかにするために、テキスト全体としての一貫性を測るcoherence vectorを導入した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Show_Me_a_Story_Towards_Coherent_Neural_Story_Illustration.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ベースラインとなる手法では、文単位で画像の検索を行っているために画像シークエンスとしての一貫性が損なわれてしまう。そこで、GRU及びcoherence vectorによって前後の文で登場した単語などを考慮することが可能となり、テキスト全体を表す画像シークエンスの検索が可能となった。
ユーザースタディにより、ベースライン、coherence vector無し、coherence vector有りの比較を行い、coherence vector有りが最も好まれる結果を得た。
また、画像シークエンスがテキストに合っているかは主観的な評価であるため、saliencyベースの新たな評価指標を提案した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://www.cs.ubc.ca/~lsigal/Publications/cvpr2018ravi.pdf">論文URL</a></li><li><a href="https://arxiv.org/abs/1511.06361">ベースライン</a></li></ul></div></div><div class="slide_index">[#133]</div><div class="timestamp">2018.5.29 12:03:27</div></div></section><section id="SO-Net_Self_Organizing_Network_for_Point_Cloud_Analysis"><div class="paper-abstract"><div class="title">SO-Net: Self-Organizing Network for Point Cloud Analysis</div><div class="info"><div class="authors">Jiaxin Li et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>順序構造に対して不変な３次元 Point Cloud のための deep learning アーキテクチャー SO-Net を提案. Self-Organizing Map (SOM) を作ることで点群の空間分布をモデル化し, SOMのノードを用いて階層的な特徴量の抽出を行う. Point Cloud のクラス分類やセグメンテーションなどのタスクを用いた評価実験では, 先行研究と同等以上の結果をより短い学習時間で達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-SO-Net-Self-Organizing-Network-for-Point-Cloud-Analysis.png" alt="fukuhara-fukuhara-SO-Net-Self-Organizing-Network-for-Point-Cloud-Analysis.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>SOM を用いることで Point Cloud を複数の Point Cloud の部分集合に分割し, 各部分集合ごとの特徴量を抽出した後, 全体の特徴量を階層的に抽出する.</li><li>初期ノードの位置を固定し, 学習を batch 単位で行うことで, SOM の学習が順序構造に対して不変となるようにしている.</li><li>様々なタスクの事前学習として用いるための Point Cloud の autoencoder を提案. </li><li>ネットワークの構造が単純かつ並列計算可能なため, 先行研究よりも短時間で学習をすることが可能.</li><li>point cloud reconstruction, classification, object part segmentation, shape retrieval などの複数のタスクを用いて評価実験を行った.</li><li>評価実験の結果では Point-Net++ や Kd-Net などとの先行研究と同等以上の結果を半分以下の学習時間で達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.04249" target="blank">[論文] SO-Net: Self-Organizing Network for Point Cloud Analysis</a></li><li><a href="https://github.com/lijx10/SO-Net" target="blank">[Code] GitHub</a></li></ul></div></div><div class="slide_index">[#134]</div><div class="timestamp">2018.5.17 12:19:55</div></div></section><section id="Large-scale_Point_Cloud_Semantic_Segmentation_with_Superpoint_Graphs"><div class="paper-abstract"><div class="title">Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs</div><div class="info"><div class="authors">Yoshihiro Fukuhara et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>大規模(数百万規模)な point clouds データに対して効率的に Semantic Segmentation を行う研究. まず, point clouds 全体を形状が単純で, 意味的に同じ点が属する部分集合(superpoint）に分類し, superpoint が作るグラフ（SPG）に graph convorution を適用することで segmentation を行う. Semantic3D と S3DIS dataset を用いた評価実験では先行研究よりも良い結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Large-scale-Point-Cloud-Semantic-Segmentation-with-Superpoint-Graphs.png" alt="fukuhara-Large-scale-Point-Cloud-Semantic-Segmentation-with-Superpoint-Graphs.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>superpoint の構成は先行研究(Guinard+17)で提案された, Global Energy を用いて行う.</li><li>各 superpoint の特徴量を PointNet を用いて抽出する. (大規模なデータを扱うため, 各 superpoint 内でダウンサンプリングを行っている.) </li><li>抽出された各 superpoint の特徴量に対して Gated Recurrent Unit (GRU) を用いた graph convorution を適用することで, 各 superpoint のクラス分類を行う.</li><li>Semantic3D と S3DIS dataset を用いた評価実験では, ShapeNet などの先行研究と比較して複数の評価尺度で最も優位な結果を達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09869" target="blank">[論文] Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs</a></li><li><a href="https://github.com/loicland/superpoint_graph" target="blank">[Code] GitHub</a></li></ul></div></div><div class="slide_index">[#135]</div><div class="timestamp">2018.5.28 00:47:55</div></div></section><section id="FoldingNet_Point_Cloud_Auto_encoder_via_Deep_Grid_Deformation"><div class="paper-abstract"><div class="title">FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation</div><div class="info"><div class="authors">Yaoqing Yang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>3次元点群処理のための autoencoder を提案. Folding という新しい decoding 演算を導入することで, 2次元グリッド上の点から3次元点群の表面上への射影を教師なしで学習した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-FoldingNet-Point-Cloud-Auto-encoder-via-Deep-Grid-Deformation.png" alt="fukuhara-FoldingNet-Point-Cloud-Auto-encoder-via-Deep-Grid-Deformation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新しい end-to-end な3次元点群処理のための deep autoencoder を提案した.</li><li>提案手法のdecoderのパラメータ数は既存手法の7%であるが, これで2次元グリッドと任意の3次元点群表面への写像が構成できることを理論的に証明した.</li><li>MN40 や MN10 dataset を用いた classification タスクの評価実験では, 最先端の教師あり手法（Achlioptas+17）などと同等の精度を達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.07262" target="blank">[論文] FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation</a></li><li><a href="https://www.youtube.com/watch?v=csC6SodV6vk" target="blank">[動画] YouTube</a></li></ul></div></div><div class="slide_index">[#136]</div><div class="timestamp">2018.5.22 12:19:55</div></div></section><section id="FFNet_Video_Fast-Forwarding_via_Reinforcement_Learning"><div class="paper-abstract"><div class="title">FFNet: Video Fast-Forwarding via Reinforcement Learning</div><div class="info"><div class="authors">Shuyue Lan et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Video Fast-forwarding のタスクを MDP(Markov Decision Process) として定式化し, 強化学習を用いて解く方法を提案. 評価実験では精度と効率の両方に置いて先行研究よりも優れた結果を示した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-FFNet-Video-Fast-Forwarding-via-Reinforcement-Learning.png" alt="fukuhara-FFNet-Video-Fast-Forwarding-via-Reinforcement-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Video Fast-forwarding を MDP (Markov Decision Process) として定式化した.</li><li>現在の Frame の特徴量を状態, スキップする Frame 数を行動として, Q-learningで強化学習を行う. </li><li>報酬はスキップした Frame の中に重要なものがどの程度含まれていたかに基づいて計算される.</li><li>Tour20 や TVSum dataset を用いた先行研究との比較実験では, 主観評価と定量的評価の両方に置いて最も良い結果となった.(6-20%程度、重要なframeを含んでいる割合が増加)</li><li>先行研究と比較して80%近く処理するフレーム数を削減し, 効率化することに成功した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://vcg.engr.ucr.edu/publications/Shuyue_CVPR.pdf" target="blank">[論文] FFNet: Video Fast-Forwarding via Reinforcement Learning</a></li></ul></div></div><div class="slide_index">[#137]</div><div class="timestamp">2018.5.17 17:25:55</div></div></section><section id="Egocentric_Activity_Recognition_on_a_Budget"><div class="paper-abstract"><div class="title">Egocentric Activity Recognition on a Budget</div><div class="info"><div class="authors">Rafael Possas et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>ウェアラブルデバイスのような使用可能な電力が限られる状況において, 電力消費と精度を強化学習を用いてバランスするフレームワークを提案. 複数のセンサー情報を用いた行動認識のタスクにおいて, 高精度・高電力消費な predictor と低精度・低電力消費な predictor を強化学習の結果に基づいて適宜切り替えることで少ない消費電力で先行研究と同等の精度を達成した. また, 一人称視点動画行動認識のための新しいデータセットを作成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Egocentric-Activity-Recognition-on-a-Budget.png" alt="fukuhara-Egocentric-Activity-Recognition-on-a-Budget.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ウェアラブルカメラの情報を用いた高精度・高コストな predictor とモーションセンサーの情報を用いた低精度・低コストな predictor のどちらを使用して推定を行うべきかを A3C の agent が判断する.</li><li>どちらのセンサーの情報を用いても正しい推定結果となるような状況では低精度・低コストな predictor を使用した場合に大きな報酬が得られるように agent の学習を行う.</li><li>提案手法では報酬についてのパラメータ１つを調整する事で精度と消費電力の簡単なトレードオフが可能.</li><li>一人称視点動画行動認識のための新しいデータセット（DataEgo）を作成.</li><li>Multimodal egocentric dataset を用いた評価実験では従来手法(Song+16)とほぼ同等の精度を少ない消費電力で達成.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://web.it.usyd.edu.au/~framos/Publications_files/egocentric-activity-recognition%20%282%29.pdf" target="blank">[論文] Egocentric Activity Recognition on a Budget</a></li></ul></div></div><div class="slide_index">[#138]</div><div class="timestamp">2018.5.19 13:40:55</div></div></section><section id="A2-RL_Aesthetics_Aware_Reinforcement_Learning_for_Image_Cropping"><div class="paper-abstract"><div class="title">A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping</div><div class="info"><div class="authors">Debang Li et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>強化学習 (A3C) を用いて Image cropping を行う手法を提案. 従来の sliding winodow に基づく手法のように膨大な数の cropping 候補を評価する必要がないため, 先行研究よりも短時間で結果の計算が可能. また, 評価実験では精度についても先行研究よりも優位な結果を達成した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-A2-RL-Aesthetics-Aware-Reinforcement-Learning-for-Image-Cropping.png" alt="fukuhara-A2-RL-Aesthetics-Aware-Reinforcement-Learning-for-Image-Cropping.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Image cropping を sequential decision-making process として定式化した. (14種類の cropping を action として, Markov 過程としてモデル化.)</li><li>上記の問題を A3C を用いた強化学習を用いて解いた. </li><li>報酬については学習済みの View Finding Network (Chen＋2017）を使用.</li><li>各ステップで候補となる cropping の種類の数が少ないため, 先行研究と比較して非常に短い計算時間で結果を出力することが可能となった.</li><li>Flickr Cropping Dataset, CUHK Image Cropping Dataset, Human Cropping Dataset を用いて行った評価実験ではいずれも先行研究よりも優位な結果を達成した.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1709.04595.pdf" target="blank">[論文] A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping</a></li><li><a href="https://github.com/wuhuikai/TF-A2RL" target="blank">[Code] GitHub</a></li></ul></div></div><div class="slide_index">[#139]</div><div class="timestamp">2018.5.22 18:27:55</div></div></section><section id="Good_View_Hunting_Learning_Photo_Composition_from_Dense_View_Pairs"><div class="paper-abstract"><div class="title">Good View Hunting: Learning Photo Composition from Dense View Pairs</div><div class="info"><div class="authors">Zijun Wei, Jianming Zhang, Xiaohui Shen, Zhe Lin, Radomir Mech, Minh Hoai, Dimitris Samaras</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像の構図の良し悪しを評価するComparative Photo Compositionデータセットを構築。10800枚の画像から24の構図の画像を作成し、クラウドソーシングによって2つの構図のどちらがいいかをアノテーションした。
また、入力画像をどのようにクロッピングすると良い構図になるかを提示するシステムを構築した。
その際、IOUを評価尺度にすると構図的に評価が低いものも高いスコアになるため、画像を評価するネットワークから得られるスコアを指標とした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Good_View_Hunting_Learning_Photo_Composition_from_Dense_View_Pairs.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のデータセットでは画像に対してスコアがついていたのに対して、構図の異なる2枚の画像どちらがいいかを100万ペアアノテーションを行った。構図推薦システムは、ユーザースタディの結果従来手法よりも良いと感じる人が多いことを確認した。
また、計算速度も従来手法と比べはるかに向上した(75FPS+)．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://www.zijunwei.org/VPN_CVPR2018.html">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#140]</div><div class="timestamp">2018.5.28 00:50:47</div></div></section><section id="DVQA_Understanding_Data_Visualization_via_Question_Answering"><div class="paper-abstract"><div class="title">DVQA: Understanding Data Visualization via Question Answering</div><div class="info"><div class="authors">Kushal Kafle, Brian Price, Scott Cohen, Christopher Kanan</div><div class="conference">CVPR 2018</div><div class="paper_id">694</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>新規なバーグラフに対して質問回答タスクDVQA及びデータセットの提案．</li><li>バーグラフが情報の一つとしてより豊かな統計的な情報を表現できる．提案手法がバーグラフを対象としたDVQAを提案し，バーグラフの自動的情報抽出と理解を可能にした．</li><li>大規模なバーグラフQAデータセットDVQAを提案した．DVQAが3Mのグラフ‐質問ペアから構成され，バーグラフに対し３種類の質問(構造理解，データ検索，reasoning)を設定した．また，全部の質問がopen-endedである．</li><li>DVQAタスクにおいて，2種類のネットワーク構造を提案した．①MOM:グラフの局所領域を抽出し文章を生成ことにより回答できる問題を対応するネットワークboundingbox OCR及びグラフの局所領域を抽出せずに回答する一般的な問題を対応するClassifierの二つのサブネットから構成される．どのネットにより回答するかを2クラス分類問題として取り扱っている②SANDY:従来手法SANにダイナミックエンコーディングモデルを用いて，質問文中のchart-specific単語をエンコーディングし，それをベースに直接chart-specificな回答文を生成できる．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DVQA.png" alt="DVQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>実用性が高い新規なバーグラフに対し質問回答タスクを提案．</li><li>提案データセットDVQAに対し5種類の従来のVQA手法と提案のMOM,SANDYの比較実験を行った．一般的問題・chart-specific問題の両方に対し提案のSANDYモデルが最も良い精度を達成した．</li><li>提案のデータセットDVQAがバーグラフの理解と質問文・回答文によりバーグラフ自動生成に用いられる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>VQAタスクのVを画像からバーグラフに変更し実用性が高い提案である．</li><li>類似した考えで従来の”V”か“Q”か“A”を同じ処理で別の似た概念に変更する研究をするも面白そう</li><li><a href="https://arxiv.org/abs/1801.08163">論文</a></li></ul></div></div><div class="slide_index">[#141]</div><div class="timestamp">2018.5.25 17:28:12</div></div></section><section id="RotationNet_Joint_Object_Categorization_and_Pose_Estimation_Using_Multiviews_from_Unsupervised_Viewpoints"><div class="paper-abstract"><div class="title">RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints</div><div class="info"><div class="authors">Asako Kanezaki, Yasuyuki Matsushita, Yoshifumi Nishida</div><div class="conference">CVPR 2018</div><div class="paper_id">628</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>物体のマルチ視点の画像からジョイントで3D姿勢推定及び物体認識を行う手法RotationNetの提案．</li><li>3D MFPにより作成されたマルチ視点画像データセットMIROを提案した．(12classes, 10 instances/class,160viewpoints)</li><li>物体を観測する視点及び物体のカテゴリをジョイントで推定した方がより良い精度を達成できると指摘し，更にトレーニングする際に物体を観測する視点をlatent variablesとして取り扱い，視点unalignedな学習データセットからunsupervisedで物体の姿勢推定を学習する．</li><li>また，視点-specificな特徴をクラス内だけではなく，異なるクラス間の姿勢アライメントを行う．</li><li>RotationNetのネットワーク構造はマルチ視点の画像から画像ごとにそ全部の視点の確率(その画像がその視点であるか)及び物体カテゴリを予測し，全部の画像から予測した結果から正解ラベルのクラスの確率＊視点の確率の統合を最大化するように学習する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/RotationNet.png" alt="RotationNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>物体認識においてはSHREC’17のnormalデータに対し優勝した．また，ModelNet-10,ModelNet-40に対し従来のマルチ視点・ポイントクラウド・ボクセルベースな様々な手法より良い精度を達成．</li><li>物体姿勢推定において，無監督な方法で従来の監督方法レベルな結果が得られた．</li><li>実環境で，良い姿勢な画像をと撮影できるとは限らない．RotationNetで物体の姿勢及び認識を行う際，画像枚数（＞＝１）で動作でき，観測が更新したら予測結果を更新する．そのため，RotationNetはAR応用などの実環境の応用に適応する．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>クラス間のViewpoint-specificな特徴を学習することが面白い．可視化手法を加えて学習済みモデルに対しどういうようにアライメントしているのかを知りたい．また，問題定義を詳細的に考える必要がありそう</li><li>疑問点としては予測したそれぞれの視点の結果の統合は平均をとる？</li><li><a href="https://arxiv.org/abs/1603.06208">論文</a></li><li><a href="https://github.com/kanezaki/rotationnet">コード</a></li></ul></div></div><div class="slide_index">[#142]</div><div class="timestamp">2018.5.25 17:21:58</div></div></section><section id="Visual_to_Sound_Generating_Natural_Sound_for_Videos_in_the_Wild"><div class="paper-abstract"><div class="title">Visual to Sound: Generating Natural Sound for Videos in the Wild</div><div class="info"><div class="authors">Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, Tamara Berg</div><div class="conference">CVPR 2018</div><div class="paper_id">435</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>ビデオからリアルな音声を生成する(waveformな)手法及びビデオ―音声データセットを提案した．</li><li>人がビジョンとサウンド間の関連性をある程度把握できる．そこで，in-the-wildビデオから音声(waveform型)を自動生成するタスクを提案し，また，このタスクのためのデータセットVEGASを提案した．VEGASはAudioSetデータセットをAMTよりクリーンし，10カテゴリのビデオ及び対応した音声28109ペアから構成される．データセットのビデオの総時間が55時間となる．</li><li>提案タスクに対応したフレームワークはビデオエンコーダー及び音声ジェネレータから構成される．音声ジェネレータは階層的RNNを用いた．ビデオエンコーダーに対し:①frame-to-frame②sequence-to-sequence③flow-basedの３種類の設計を用いた．3種類モデルの生成結果に対し定量評価及びヒューマンテストを用いて評価し，flow-based構造が最も良い性能とヒューマン評価を達成した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/VisualToSound_InTheWild.png" alt="VisualToSound_InTheWild"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来のビデオから音声を生成する手法はビデオに対し拘束条件を加えている．提案手法は初めてのin-the-wildビデオから音声を生成する手法．</li><li>ビデオから音声を自動生成する手法の応用場面が広い．(VRシステムでの没入感の増強，音声編集作業の自動化，視覚障害の人に視覚体験を聴覚体験として提供)</li><li>ヒューマンテスト (ビデオがリアルかフェクか)に対し，ビデオエンコーダーをflow-basedな構造を用いた場合，平均73.36%の生成音声がリアル音声と評価された．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・視覚情報の抽出機に更にコンテンツと物体relationなどを重視したネットワークを用いたら更なる良い結果が得られそう・逆設定として，音声情報からビデオの予測も面白そう</p><ul><li><a href="https://arxiv.org/abs/1712.01393">論文</a></li><li><a href="https://github.com/arXivTimes/arXivTimes/issues/562">コード</a></li></ul></div></div><div class="slide_index">[#143]</div><div class="timestamp">2018.5.25 17:15:56</div></div></section><section id="Functional_Map_of_the_World"><div class="paper-abstract"><div class="title">Functional Map of the World</div><div class="info"><div class="authors">Gordon Christie, Neil Fendley, James Wilson, Ryan Mukherjee</div><div class="conference">CVPR 2018</div><div class="paper_id">795</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>建物や土地などの機能的目的を予測するタスクに用いられる大規模な衛星画像データセットfMoWの提案(bounding box, 時系列，カテゴリ，メタ情報などのアノテーションがあり)</li><li>データセットの具体的な統計情報は①200以上の国の１,047,691 枚画像②63カテゴリ③一枚の画像1つ以上のバウンディングボクス定義④時系列画像が大量に含む．</li><li>このデータセットに対応した新たなタスクを設定した：連続な時系列画像によりバウンディングボクス内の物体を認識する．提案データセットfMoWを用いて5つのネットワーク構造:LSTM-M,CNN-I,CNN-IM,LSTM-I,LSTM-IM(I:画像M:メタ特徴)に対し比較実験を行た．平均F1スコアにおいてLSTM-IMが最も高い精度を示したので，時系列情報及びメタ情報をジョイントでreasoningするアプローチの有効性を証明した</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/FunctionalMapOfTheWorld.png" alt="FunctionalMapOfTheWorld"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>公開されている最も大規模な衛星画像データセット．</li><li>異なる国・撮影時間・撮影年代などで撮影された画像から構成され，提案データセットを統計比較などにも用いられる．</li><li>従来の衛星画像データセットは主にbrief momentsの情報だけをキャプチャーし，メタ情報(ロケーション，時間，太陽角度など)がアノテーションされていない．提案データセットはメタ情報をアノテーションし，様々な応用を可能にした．(例：パーキングエリアの時系列駐車量の統計・影と時間情報によりオブジェクトの高さ推定など)</li><li>検出と識別タスクの間に位置付ける新たな問題設定“時系列画像のバウンディングボックス内の物体識別”をして，更に実験を通してメタ情報と時系列情報をジョイントで処理することの重要性を示した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>地理情報に関する分析の研究に用いられるデータセット</p></li><li><p>国のバリエーションが豊かなデータセットなので，国ごと上空シーン特徴の比較などにも用いられる</p></li><li><p><a href="https://arxiv.org/abs/1711.07846">論文</a></p></li><li><p><a href="https://github.com/fMoW">コード</a></p></li><li><p><a href="https://github.com/fMoW/dataset">データセット</a></p></li><li><p><a href="https://www.iarpa.gov/index.php/working-with-iarpa/prize-challenges/1015-functional-map-of-the-world-fmow">fMoW Challenge</a></p></li></ul></div></div><div class="slide_index">[#144]</div><div class="timestamp">2018.5.25 17:05:45</div></div></section><section id="Deep_Cocktail_Networks_Multi-source_Unsupervised_Domain_Adaptation_with_Category_Shift"><div class="paper-abstract"><div class="title">Deep Cocktail Networks: Multi-source Unsupervised Domain Adaptation with Category Shift</div><div class="info"><div class="authors">Ruijia Xu, Ziliang Chen, Wangmeng Zuo, Junjie Yan, Liang Lin</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ソースドメインのラベル付きデータセットが複数ある場合のunsupervised domain adaptation(UDA)であるmultiple domain adaptation(MDA)によってターゲットドメインのクラシフィケーションを行う
Deep Cocktail Network(DCTN)を提案。MDAではUDAで問題視されるドメインシフトに加えて、
ソースドメインのデータセット間で全てのカテゴリが共有されていないカテゴリシフトが存在する。
DCTNでは、k番目のソースドメインのデータセットとターゲットドメインのデータセットを入力として
discriminatorによってperplexity scoreを算出することでどのソースドメインのデータセットの分布に近いかを算出し、
これを全てのソースドメインのデータセットに対して行い、perplexity scoreを重み付けるすることで最終的な識別結果を出力する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Cocktail_Networks_Multi-source_Unsupervised_Domain_Adaptation_with_Category_Shift.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>discriminatorによってターゲットドメインがソースドメインのデータセットのうちどのデータの分布に近いかを計算することで、MDAに取り組むDCTNを提案。</li><li>3つのベンチマークにおいてUDAのstate-of-the-artと比較し他結果、提案手法が最も高い精度を達成。</li><li>カテゴリシフトを解決できているかどうかを確認するために、ターゲットドメイン内でカテゴリの重複あり/なしにおける識別結果を比較したところ、
state-of-the-artと同等以上の精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>discriminatorが算出したperplexity scoreによって重み付けをするというシンプルな手法だが、UDAに取り組むstate-of-the-artよりも高い精度を達成している。</li><li><a href="https://arxiv.org/abs/1803.00830">論文</a></li></ul></div></div><div class="slide_index">[#145]</div></div></section><section id="Unsupervised_Correlation_Analysis"><div class="paper-abstract"><div class="title">Unsupervised Correlation Analysis</div><div class="info"><div class="authors">Yedid Hoshen, Lior Wolf</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>2つのドメインを結合する手法であるCanonical Correlation Analysis(CCA、正準相関分析)を教師なし学習に対して行うUnsupervised Correlation Analysis(UCA)を提案。
既存のCCAは教師あり学習かつ2つのドメインが何らかの対応関係を持っていることを前提としていたが、
UCAは教師なし学習かつ2つのドメインに対応関係がない場合を想定している。
教師あり学習とは異なり、トレーニング時に2つのドメインにおける相関係数を計算することができないため、入力する2つのドメインと、
ネットワークによって射影された潜在変数空間の3つのドメイン間の射影、逆射影がうまくいくように様々なロスをとることで学習を行う。
ロスに対するablationも行なっている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Unsupervised_Correlation_Analysis.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>教師なしかつ2つのドメインに対応関係がない状況におけるCCAの拡張であるUCAを提案。</li><li>評価尺度として潜在変数空間における相関係数、AUCを用いて以下の5つの状況で実験を行なった。1.MNISTの画像とそのミラー画像、2.MNISTの上半分の画像と下半分の画像、3.鳥の画像とそのキャプション、4.花の画像とそのキャプション、5.Flickerの画像とそれに付随する5つの文章。
関節位置のエラーを測定したところ上記のstate-of-the-artの手法と同等、あるいは上回る精度を達成。</li><li>教師なし学習の結果をGANと比較しており、全ての実験においてGANよりも高い精度を達成。</li><li>教師あり学習をUCAで行なった結果も乗せられており、実験3、４、5において通常のCCAよりも高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>現状のネットワークを見ると、それぞれのドメインにおける直交性と、それぞれのドメインの射影先が同じ空間になるように様々なロスをとっているだけなので、
もう少しアップデートすることができるかもしれない。</li><li>CCAの特徴であるL_Orthだけを除いた場合に、どれほどの影響が出るのかが気になった。</li><li><a href="https://arxiv.org/abs/1804.00347">論文</a></li></ul></div></div><div class="slide_index">[#146]</div></div></section><section id="Transferable_Joint_Attribute-Identity_Deep_Learning_for_Unsupervised_Person_Re-Identification"><div class="paper-abstract"><div class="title">Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification</div><div class="info"><div class="authors">Jingya Wang, Xiatian  Zhu, Shaogang Gong, Wei Li </div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルなしデータセットにおいてperson re-identification(re-id)を教師なしで行うために、ラベルありデータセットからdomain adaptationを行うTransferable Joint Attribute-Identity 
Deep Learning(TJ-AIDL)を提案。person re-idとは、街中の監視カメラのような異なる視点、
重複のない領域を撮影された映像内の同一人物を探すことである。
TJ-AIDLにはアイデンティティーを推定するIdentity branch、アトリビュートを推定するAttribute branch、
アトリビュートからアイデンティティーを推定するモジュールであるIdentity Inferred Attirbute(IIA)からなる。
domain adaptationの際には、Attribute branch、IIAの更新のみを行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Transferable_Joint_Attribute-Identity_Deep_Learning_for_Unsupervised_Person_Re-Identification.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>domain adaptationを用いて教師なしでperson re-idを行うために、画像のアトリビュートからアイデンティティーを推定するTJ-AIDLを提案。</li><li>personn re-idのベンチマークである4つのデータセットを使用しており、Rank-1mAPにおいてre-idを教師なしで行うstate-of-the-artよりも高い精度を達成。</li><li>TJ-AIDLにおいてアトリビュート/アイデンティティーのみ学習した際の結果、adaptation有り/無しの結果についても議論しており、提案したTJ-AIDLが最も高い精度となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.09786">論文</a></li></ul></div></div><div class="slide_index">[#147]</div></div></section><section id="Duplex_Generative_Adversarial_Network_for_Unsupervised_Domain_Adaptation"><div class="paper-abstract"><div class="title">Duplex Generative Adversarial Network for Unsupervised Domain Adaptation</div><div class="info"><div class="authors">Lanqing Hu, Meina Kan, Shiguang Shan, Xilin Chen</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>同一カテゴリのdomain間におけるadaptation, transferをラベル識別と2つのdiscriminatorを用いるネットワークDupGANを提案。target domainにはラベルがない状況である教師なし学習を対象としている。
DupGANはencoderでそれぞれのドメインの潜在変数をエンコードし、generatorでデコードを行い、
2つのdiscriminatorでそれぞれのドメインに対してfake/realとラベルの認識を行う。
結果はdomain transferされた数字画像のラベル認識・生成結果、物体認識の精度において比較を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Duplex_Generative_Adversarial_Network_for_Unsupervised_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ラベル認識と2つのdiscriminatorによってdomain adaptaion/transferをおこなうDupGANを提案。</li><li>既存手法である<a href="http://jmlr.org/papers/volume17/15-239/15-239.pdf">DANN</a>、<a href="https://arxiv.org/abs/1702.05464">ADDA</a>はadversarial lossを使用してtarget→source のマッピングを行うが、
これらの手法ではマッピングされたtarget domainの分布が歪んでいないことは保証できない。
一方DupGANではラベルの認識を行わせることでカテゴリ構造を保つことができる。また提案手法では画像の生成も可能である。</li><li>state-of-the-artと比較して、数字画像データセットであるMNIST、USPS、SVHN、SVHN-extraそれぞれのデータセット間におけるdomain transferに対するラベル認識の結果、
最も高い精度を達成。またdomain transferによる画像も生成することが可能。</li><li>31種類のラベル、3つのドメインを持つOffice-31データセットにおける物体認識結果がstate-of-the-artよりも高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>クラシフィケーション生成された画像ではなくはエンコードされた潜在変数に対して行われている。</li><li>画像の生成力はそこまで高くなく、実際Office31に対する画像生成は難しかったと主張している。</li><li><a href="http://vipl.ict.ac.cn/uploadfile/upload/2018041610083083.pdf">論文</a></li></ul></div></div><div class="slide_index">[#148]</div></div></section><section id="Pixels_voxels_and_views_A_study_of_shape_representations_for_single_view_3D_object_shape_prediction"><div class="paper-abstract"><div class="title">Pixels, voxels, and views: A study of shape representations for single view 3D object shape prediction</div><div class="info"><div class="authors">Daeyun Shin, Charless Fowlkes, Derek Hoiem</div><div class="conference">CVPR 2018</div><div class="paper_id">384</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>1枚の画像から3次元形状を推定するタスクにおいて，異なる形状representation及びcoordinate framesを用いた場合，精度がどのように変化するのかの徹底的比較実験に関する研究．</li><li>従来形状推定タスクにおいて異なる設計の比較分析の研究がないので，著者達が異なる設計を比較できるフレームワーク及び具体的な実験を行った．</li><li>比較実験は具体的に，a.RGB画像b.デプス画像からの形状推定タスクにおいて，“①マルチサーフェス画像VS volumetricデータ表示②viewer-centered VS object-centeredな座標”などの設定に対し，定量的及び定性的な比較実験を行った．</li><li>提案の比較用フレームワークはencoder-decoderベースなネットワークを用いて，decoderに変更を加えることで， マルチサーフェス画像及び volumetricデータの2種類を生成できるようにした．また，coordinate frameをスイッチすることにより，viewer/object centeredを変更できる．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Study_Of_Shape_Representations.png" alt="A_Study_Of_Shape_Representations"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>3次元形状推定タスクにおいて，異なる設定の比較実験を行った．</li><li>形状representationの設定において，Multi-surfaceの方がvoxel と比べunseenクラスにおいてより良い性能を達成した． Multi-surfaceの方が高い解像度をエンコーディングできるのが理由な可能性があると指摘した．</li><li>coordinate framesの設定において，意外なことに従来広く採用されているobject-centeredはviewer-centeredと比べunseenクラスにおいて精度が劣っていて， object-centeredの方がカテゴリ認識に対応が強いのが原因となることを指摘した．</li><li>以上の結論を元に，object centeredなsurface-basedな1枚の画像から3次元形状推定の手法3D-R2N2を提案し，PASCAL 3D+データセットにおいてmean IoU0.414を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>比較をしていない設計(Oct-tree based representationなど)もあるので，そういった構造に対して比較実験を行うのも面白い．</p></li><li><p>3次元あたりの徹底的比較を行って，何らかの結論を出すような研究がまだ少ないので，研究テーマを沢山作れるかも？</p></li><li><p><a href="https://arxiv.org/abs/1804.06032">論文</a></p></li></ul></div></div><div class="slide_index">[#149]</div><div class="timestamp">2018.5.24 18:20:50</div></div></section><section id="PlaneNet_Piece-wise_Planar_Reconstruction_from_a_Single_RGB_Image"><div class="paper-abstract"><div class="title">PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image</div><div class="info"><div class="authors">Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, Yasutaka Furukawa</div><div class="conference">CVPR 2018</div><div class="paper_id">336</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>1枚のRGB画像から“piece-wise planar depthmap”を推定するend-to-endなネットワークを提案した．提案手法を用いてRGB画像から平面パラメータ及び平面セグメンテーションマスク及びデプスマップを同時に推定できる．</li><li>画像からpiece-wiseな平面を検出するタスクはARの応用に一つ重要なタスクとなっている．しかし従来，デプス推定とpiece-wiseな平面検出を同時に行う研究がない．著者達が新たにこのタスク及びタスクに対応できるネットワークを定義した．</li><li>提案フレームワークは:①DRNs(Dilated Residual Networks)を用いて入力画像から特徴抽出を行う②平面パラメータ推定・non-planarデプスマップ推定・セグメンテーションマスク推定の3つの推定ネットワークを用いる③推定した3つの結果から“piece-wise planar depthmap”を生成する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PlaneNet_PieceWise_PlaneEstimation.png" alt="PlaneNet_PieceWise_PlaneEstimation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新規な問題定義．実験で提案手法が部屋のレイアウト推定・ARアプリ(テクスチャー編集・バーチャルルーラーなど)に応用できることを指摘した．</li><li>51,000枚ほどの学習データを作成した．(これが大変そう)</li><li>plane segmentationタスクにおいてNYUデータセットでの精度が従来の三つの手法より優れている(比較している手法は2009年，2009年，2012年の手法だけど。。)</li><li>デプスマップ推定タスクにおいてNYUv2データセットにおいて前述した３つの手法より精度良い</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>ARアプリに応用できるところから考えると単純なデプス推定より実用性が高い</p></li><li><p>平面検出も同時に行うので，部屋レイアウト推定に良い精度を達成したのが理解できる．しかし，疑問としては提案手法が平面検出＋デプス推定だけで部屋の幾何構造実際は学習していないので，デプス推定＋平面パーツ検出の従来研究と比べると新規性と技術的の難しさがどこなのかちょっとわからない</p></li><li><p><a href="https://www.cse.wustl.edu/~chenliu/planenet/paper.pdf">論文</a></p></li><li><p><a href="https://github.com/art-programmer/PlaneNet">コード</a></p></li><li><p><a href="http://www.cse.wustl.edu/~chenliu/planenet.html">プロジェクト</a></p></li></ul></div></div><div class="slide_index">[#150]</div><div class="timestamp">2018.5.24 18:13:54</div></div></section><section id="PointNetVLAD_Deep_Point_Cloud_Based_Retrieval_for_Large-Scale_Place_Recognition"><div class="paper-abstract"><div class="title">PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition</div><div class="info"><div class="authors">Mikaela Angelina Uy, Gim Hee Lee</div><div class="conference">CVPR 2018</div><div class="paper_id">573</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>PointNetとNetVLADを用いたポイントクラウドベースな“場所検索”ネットワークPointNetVLAD及びデータセットの提案．</li><li>従来の自動運転などに用いられる場所検索技術では2次元画像ベースで行われている．しかし，照明条件などに対しロバスト性が低い．ポイントクラウドベースな場所検索が従来良いグローバル特徴抽出機がないため，まだ研究されていない．近年PointNetなどの良いポイントクラウド特徴抽出機が提案され，そこで著者達がPointNetとNetVLADを用いたLiDARで撮ったポイントクラウドをベースとした場所検索手法を提案した．</li><li>提案データセットの収集過程は:①Oxford RobotCar などのdatasetからフルールートを選択する②フルールートから局所を選択する③選択した局所ポイントクラウドをダウンサンプルと正規処理を行う．また，Oxford RobotCar 以外，3種類の他のデータセットからデータを集めた．</li><li>fixedサイズなポイントクラウドからグローバル特徴を抽出できるPointNet，NetVLADと全結合層をコンバインたend-to-endなグローバル特徴抽出機を構築した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PointNetVLAD.png" alt="PointNetVLAD"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新規なポイントクラウドベースな場所検索及び場所検索3次元ポイントクラウドデータセットの提案．</li><li>従来の2次元画像ベースな場所検索と比べ，提案したポイントクラウドベースな場所検索が照明条件にロバストである．</li><li>PointNetとNetVLADを用いているので，ポイントクラウドの無順序性及びpermulationを対応できる．</li><li>新規なロス関数Lazy quadrupletを定義した．</li><li>提案データセットにおいて，PointNetとModelNetなどの従来手法と比べ良い検索精度達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>PointNet，PointNet++, Kd-networkなどのポイントクラウドデータを扱えるネットワークでポイントクラウドから情報抽出を利用した研究がこれからまだ増えるのかな？</p></li><li><p>ポイントクラウドデータを直接処理できるネットワークがいくつかあるが，主にPointNet，PointNet++が引用されていそう．ほかの手法があまり使われていない理由が知りたい</p></li><li><p><a href="https://arxiv.org/abs/1804.03492">論文</a></p></li><li><p><a href="https://github.com/mikacuy/pointnetvlad">コード</a></p></li></ul></div></div><div class="slide_index">[#151]</div><div class="timestamp">2018.5.24 18:03:54</div></div></section><section id="Pix3D_Dataset_and_Methods_for_3D_Object_Modeling_from_a_Single_Image"><div class="paper-abstract"><div class="title">Pix3D: Dataset and Methods for 3D Object Modeling from a Single Image</div><div class="info"><div class="authors">Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Tianfan Xue, Joshua Tenenbaum, William Freeman</div><div class="conference">CVPR 2018</div><div class="paper_id">375</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>大規模なピクセルレベルに対応付けられたimage-shape pairsデータセットPix3Dの提案及び画像から同時に三次元形状及び姿勢を推定するネットワークの提案．</li><li>従来のimage-shape pairsデータセットは①合成データセットを用いる②image-shapeの対応が精密ではない③データセット規模が小さいなどの問題点がある．そこで，著者達が大規模なピクセルレベルに対応付けられたデータセットを提案した．Pix3Dは395個の3次元物体モデル(9カテゴリ)，10069ペアの画像―形状ペアから構成される．画像と形状のペアはピクセルレベルの精密的に対応付けられている．</li><li>データセットの収集段階では:①IKEA及び自撮りで大量な画像―形状ペアを集める②AMTにより画像からキーポイントをアノテーションする③Efficient PnP及びLevenberg-Marquardtを用いて粗い・精密なposeを求める．</li><li>更に，提案手法は画像から同時に姿勢及び3次元形状を予測できるネットワークを提案した．提案ネットワークはまず画像から2.5Dスケッチを推定し，推定したスケッチをエンコーディングする．また，デコーディングにより3次元形状を推定し，同時にview estimatorネットワークにより姿勢を推定する．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Pix3D.png" alt="Pix3D"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来のデータセットではCGモデルで合成されている方が多く，提案のデータセットが実物体を用い，更にピクセルレベルな精密度の画像―形状対応付けアノテーションがある．</li><li>画像から同時に形状姿勢を推定するフレームワークの定量化結果は提案したデータセットでは3D-VAE-GAN,MarrNetなどの従来手法より良い精度を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>現在の学習データアノテーション段階でAmazon Mechanical Turkを用いている．Semantic Keypointの自動的検出を用いたら自動化できることはデータセットの更なる拡大化につなぎられそう</p></li><li><p><a href="https://arxiv.org/abs/1804.04610">論文</a></p></li><li><p><a href="https://github.com/xingyuansun/pix3d">コード</a></p></li></ul></div></div><div class="slide_index">[#152]</div><div class="timestamp">2018.5.24 17:57:53</div></div></section><section id="Learning_to_Look_Around_Intelligently_Exploring_Unseen_Environments_for_Unknown_Tasks"><div class="paper-abstract"><div class="title">Learning to Look Around: Intelligently Exploring Unseen Environments for Unknown Tasks</div><div class="info"><div class="authors">Dinesh Jayaraman, Kristen Grauman</div><div class="conference">CVPR 2018</div><div class="paper_id">152</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>新規な問題設定“シーンや物体を有効的に観測できる視点を学習する”及びこの問題を対応できる “アクティブ観測補完”ネットワークの提案．</li><li>従来のCVタスクは主に与えられた観測(画像・ビデオ・ポイントクラウドなど)から視覚性質(クラス分類・検出など)の分析を行う．しかし，リアルな知能はまず環境から目的を達成するための観測を取得することから始まる．また，異なる観測から得られる情報量も異なる．そこで，著者達が“active observation completion”タスクを提案し，未知なシーンかオブジェクトからシーン及び物体のより多く3次元情報が含めた数が限られた観測視点の推定を目標とする．</li><li>提案手法は強化学習を用いる．RNNベースなネットワークを用いて選択された視点からシーンか物体のパーツ情報を統合する．また，統合されたモデルから推定できるunobserved視点とgt間の誤差をベースにロス関数を設定した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/LearningToLookAround.png" alt="LearningToLookAround"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>学習データを手動でラベリングする必要がないので，大量な学習が行える．</li><li>提案フレームワークを“シーン”の補完及び“物体モデル”の補完の2種類だいぶ異なったタスクに実験を行い，良い精度を達成したので，”提案した“無監督探索的な”フレームワークを遷移学習でほかのタスクに用いられる．</li><li>SUN360(Scene dataset)及び”ModelNet” (Object dataset)を用いて，従来のいくつかベースとなる手法より良い精度を達成した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>Interactive 環境でのVQAタスク(Embodied Question Answeringなど)は環境から“情報量が豊かな画像”を集めるのが重要の一環なので，提案フレームワークを用いられそう．</p></li><li><p><a href="https://arxiv.org/abs/1709.00507">論文</a></p></li></ul></div></div><div class="slide_index">[#153]</div><div class="timestamp">2018.5.24 17:50:58</div></div></section><section id="PU-Net_Point_Cloud_Upsampling_Network"><div class="paper-abstract"><div class="title">PU-Net: Point Cloud Upsampling Network</div><div class="info"><div class="authors">Lequan Yu, XIANZHI LI, Chi-Wing Fu, Daniel  Cohen-Or, Pheng-Ann Heng</div><div class="conference">CVPR 2018</div><div class="paper_id">355</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>data-drivenなポイントクラウドアップサンプリング手法の提案．スパースなポイントクラウドから，もっとデンスでユニフォームなポイントクラウドを取得できる．</li><li>従来の2D画像super-resolutionタスクと比べ，3D Upsamplingでは処理対象が空間オーダーとレギュラー構造がないポイントクラウドで，物体の本当のサーフェス(ポイントクラウドのリアル物体)に近づき，点の密度も均等であることがタスクの目標となる．こういったことから，提案手法はポイントクラウドからマルチレベルの特徴を抽出し，更にマルチブランチで特徴を拡張することにより，ポイントクラウドの局所及びグローバルな情報を取得できる．</li><li>提案ネットワークPU-Netは入力のポイントクラウド(N points)に対し①ポイントクラウドに対し異なるスケールのパッチを抽出し，②パッチからPointNet++を用いたマルチレベルの特徴抽出を行う．③feature expansion構造により特徴を拡張し，④全結合層を用いて出力のポイントクラウド(N＊ｒ points)を生成する．また，物体のサーフェスまでの距離及びポイントクラウドの過密程度を基準に，ジョイントロスを設計した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PU_Net.png" alt="PU_Net"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新たな評価指標：“物体のサーフェスまでの距離偏差”及び“ポイントクラウド分布のユニフォーム性”を評価できる指標を提案し，この2つの指標においてSHREC2015データセットに対し従来研究より優れた精度と指摘した．</li><li>Pointnet++を用いてローカル及びグローバル情報抽出を行うので，ポイントクラウドの幾何的無オーダーを対応できる</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><p>提案手法を更に発展し物体モデルの補完およびアップサンプリング同時にできることを期待される</p></li><li><p>Pointnet++を基本構造として使っていることがすごそう</p></li><li><p><a href="https://arxiv.org/abs/1801.06761">論文</a></p></li></ul></div></div><div class="slide_index">[#154]</div><div class="timestamp">2018.5.24 17:36:47</div></div></section><section id="Deep_Unsupervised_Saliency_Detection_A_Multiple_Noisy_Labeling_Perspective"><div class="paper-abstract"><div class="title">Deep Unsupervised Saliency Detection: A Multiple Noisy Labeling Perspective</div><div class="info"><div class="authors">J.Zhang, T.Zhang, Y.Daiy, M.Harandi, and R.Hartley</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1803.10910</div></div><div class="slide_editor">KotaYoshdia</div><div class="item1"><div class="text"><h1>概要</h1><p>深層学習を用いた教師あり学習による顕著性の検出方法は教師データに依存する．そこで，“汎化能力を改善しつつ教師データなしで顕著性マップを学習することは可能か？”という問いに対して，弱いものやのノイズのある教師なし顕著性検出手法によって生成される多数のノイズラベルを学習することによって教師なしで顕著性の検出を行った．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Unsupervised_Saliency_Detection_A_Multiple_Noisy_Labeling_Perspective.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来の教師なし顕著性検出に新たな顕著性を推定し，複数のノイズの多い顕著性検出方法から顕著性マップを学習する．</li><li>我々の深層学を用いた顕著性検出モデルは，人間のアノテーションなしでEnd to Endで学習できとても簡潔である．</li></ul></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><ul><li>評価実験をしたところ従来の教師なしの顕著性検出方法を大きく上回り，深層学習を用いた顕著性の精度と同等のものとなった．</li><li><a href="https://arxiv.org/pdf/1803.10910">Paper</a></li></ul></div></div><div class="slide_index">[#155]</div><div class="timestamp">2018.5.23 20:28:11</div></div></section><section id="Cross-View_Image_Synthesis_using_Conditional_GANs"><div class="paper-abstract"><div class="title">Cross-View Image Synthesis using Conditional GANs</div><div class="info"><div class="authors">Krishna Regmi and Ali Borji</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>対応する航空写真とストリートビュー写真間の変換を行うcGANを提案．pix2pixによる変換に比べて，オブジェクトの正しいセマンティックスを捉え維持する変換が可能となっている．提案したcGANモデルは２つあり，X-Fork とX-Seq と呼んでいる．出力が変換画像とセグメンテーションマップであることが特徴．Inception Scoreの比較実験をすると，航空写真からストリートビュー方向の変換ではがX-Forkが優れ，逆方向の変換ではX-Seqの生成結果が優れていることがわかった．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Cross-View_Image_Synthesis_using_Conditional_GANs_fig.png" alt="Image"><br>256x256の解像度で生成可能．gがストリートビューで，aが航空写真に当たる．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>X-Forkは１つのGeneratorと１つのDiscriminatorから成るシンプルな構成のcGAN．出力は変換後の画像とセグメンテーションマップの２つであることが特徴．</li><li>X-Seqは２つのGeneratorと２つのDiscriminatorから成るcGAN．１つ目のGeneratorで変換後の画像を生成．それを元に２つ目のGeneratorでセグメンテーションマップを生成する．<br>セグメンテーションマップのGround-Truthには，学習済みのRefineNetを用いた生成結果を使用している．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>航空写真とストリートビューという劇的に見た目が変わる場合の変換において，どのようなことが問題点となるのか５つ挙げられていたので気になる場合は元論文を参照してください．</li><li>コードやデータは公開予定</li><li><a href="https://arxiv.org/abs/1803.03396">arXiv</a></li></ul></div></div><div class="slide_index">[#156]</div><div class="timestamp">2018.5.23 20:24:52</div></div></section><section id="Multimodal_Explanations_Justifying_Decisions_and_Pointing_to_the_Evidence"><div class="paper-abstract"><div class="title">Multimodal Explanations: Justifying Decisions and Pointing to the Evidence</div><div class="info"><div class="authors">D. H. Park et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">性能がよく，かつ説明可能なモデルの実現のための新規手法の提案．
これまでの説明可能なモデルは視覚的なAttentionのみやテキストの説明のみという単一のmodalだけだったのに対して，
この論文では両者を合わせたmulti-modalな説明を出力可能にした．
それを行う手法の提案と，学習と評価に使うデータセットを構築したのがこの論文のContribution．
データセットはVQAと静止画からのActivity Recognitionのタスクで，
従来あったデータセットに，理由のテキスト説明と視覚的な根拠となった領域のアノテーションを追加して作成．
手法は，まず答えを出力して，それを元に根拠となった理由を出力するという形式のネットワーク構造を採用．
</div></div><div class="item2"><img src="slides/figs/Multimodal_Explanations_Justifying_Decisions_and_Pointing_to_the_Evidence.png" alt="Multimodal_Explanations_Justifying_Decisions_and_Pointing_to_the_Evidence.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>モデルの出力に加えて視覚的，テキストのmulti-modalな根拠説明をする手法を提案</li><li>VQAとActivity Recognitionでそれを評価可能なデータセット（追加アノテーション）を構築</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1802.08129">論文 (arXiv)</a></li><li>データセットはまだ公開されていない模様</li></ul></div></div><div class="slide_index">[#157]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="A_Variational_U-Net_for_Conditional_Appearance_and_Shape_Generation"><div class="paper-abstract"><div class="title">A Variational U-Net for Conditional Appearance and Shape Generation</div><div class="info"><div class="authors">Patrick Esser, Ekaterina Sutter, Björn Ommer</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>画像を構成する成分はshape(ジオメトリ、ポーズなど)とappearanceであるという考えのもと、VAEによってappearanceを推定し、
U-Netにshapeを学習させることで入力画像のappearanceとshapeの
片方を保ったままもう一方を変更することが可能なVariational U-Netを提案。
通常のVAEではshape、appearanceの分布を分離することが不可能なため、
VAEに画像とshapeを入力することでappearanceの特徴量を抽出し、U-Netによってshape情報を保つように学習を行う。
shapeとして体のポーズや線画が入力される。トレーニングデータには同一物体に対する様々なバリエーションの画像は必要としない。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Variational_U-Net_for_Conditional_Appearance_and_Shape_Generation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>VAEでappearanceを、U-Netでshapeを学習させることで画像に内在する2つの事前分布を別々に学習することができるVarational U-Netを提案。</li><li>コンディションによって画像を編集するpix2pixとポーズをコンディションとして人物画像を編集するPG2と比較を行った。COCO、DeepFashion、Market-1501データセットにおいてSSIMやIS、
関節位置のエラーを測定したところ上記のstate-of-the-artの手法と同等、あるいは上回る精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>VAEとU-Netのいいとこ取りをすることで、2つの変数を扱うことが可能になった。</li><li><a href="https://arxiv.org/abs/1804.04694">論文</a></li><li><a href="https://compvis.github.io/vunet/">Project page</a></li><li><a href="https://github.com/CompVis/vunet">GitHub</a></li></ul></div></div><div class="slide_index">[#158]</div></div></section><section id="Total_Capture_A_3D_Deformation_Model_for_Tracking_Faces_Hands_and_Bodies"><div class="paper-abstract"><div class="title">Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies</div><div class="info"><div class="authors">Hanbyul Joo, Tomas Simon, Yaser Sheikh</div><div class="conference">CVPR 2018 Oral</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>表情、体全体の動き、手のジェスチャといった様々なスケールの動きをマーカー無しでキャプチャするdeformation modelである”Frankenstein”と”Adam"を提案。
3Dキャプチャシステムに置いて、画像の解像度と3Dキャプチャシステムの視野はトレードオフであるため、
体の局所的な動きと全体的な動きを同時に捉えことは難しかった。提案手法では顔、両手、両足、
手の指における3Dキーポイントと3D Point Cloudを用いて表情などの
局所的モーションと体全体のモーションをキャプチャすることができるFrankensteinを構築。
また70人のトラッキングデータを用いてFrankensteinモデルを最適化することで、
髪と服を表現することが可能なAdamモデルを提案。結果は既存手法とのトラッキングの精度によって比較している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Total_Capture_A_3D_Deformation_Model_for_Tracking_Faces_Hands_and_Bodies.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>表情や手のジェスチャといった局所的なモーションと、体全体の動きを同時にトラッキングすることが可能なdefromation modelを提案。620台のVGAカメラと31台のHDカメラが必要とする。</li><li>state-of-the-artである<a href="http://files.is.tue.mpg.de/black/papers/SMPL2015.pdf">SMPL</a>では顔の表情を表現することは不可能だが、提案手法では可能になっている。</li><li>SMPLとトラッキングにおけるGTとのオーバーラップを計算した結果、SMPLが84.79%であるのに対し提案手法は87.74%となり、提案手法の方が高い精度を達成</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1801.01615">論文</a></li><li><a href="http://www.cs.cmu.edu/~hanbyulj/totalcapture/">Project Page</a></li><li><a href="https://www.youtube.com/watch?v=5QzdXQSf-oY">Video</a></li></ul></div></div><div class="slide_index">[#159]</div></div></section><section id="SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild"><div class="paper-abstract"><div class="title">SfSNet: Learning Shape, Reflectance and Illuminance of Faces in the Wild</div><div class="info"><div class="authors">Soumyadip Sengupta, Angjoo Kanazawa, Carlos D. Castillo, ; David Jacobs</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベル付き合成顔画像とin-the-wildなラベルなし実顔画像のどちらもトレーニングデータとして使用することで、実顔画像からシェイプ、リフレクタンス、イルミネーションを推定してリコンストラクションをend-to-endに行うSfSNetを提案。
実顔画像に十分なラベルがついているデータセットが存在しない、という問題を解決。Shape from Shading(SfS)のアイディアに基づき、
低周波成分を合成顔画像から、高周波成分を実顔画像から推定する。リコンストラクションされた画像のL1ロスを取ることで、
トレーニングにおける合成顔画像と実画像の橋渡しが行われる。リコンストラクションにはランバーシアンレンダリングモデルを使用する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ラベル付きの合成顔画像とラベルなしの実世界顔画像でトレーニングすることで、実世界顔画像の法線、アルベド、シェーディングを推定しインバースレンダリングを行うSfSNetを提案。</li><li>インバースレンダリングによってリコンストラクションされた画像のロスを取ることで、合成顔画像と実世界顔画像の橋渡しを実現。</li><li>インバースレンダリングの見た目がstate-of-the-artよりも良い結果となった。</li><li>法線・シェーディングの推定精度が、法線・シェーディング単体をそれぞれ推定するstate-of-the-artよりも良い結果となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>画像をリコンストラクションする際によく使われるU-NetではなくResNetを使った理由についても議論されている。</li><li><a href="https://arxiv.org/abs/1712.01261">論文</a></li><li><a href="https://senguptaumd.github.io/SfSNet/">Project Page</a></li><li><a href="https://github.com/senguptaumd/SfSNet">GitHub</a></li></ul></div></div><div class="slide_index">[#160]</div></div></section><section id="Whos_Better_Whos_Best_Pairwise_Deep_Ranking_for_Skill_Determination"><div class="paper-abstract"><div class="title">Who's Better? Who's Best? Pairwise Deep Ranking for Skill Determination</div><div class="info"><div class="authors">Hazel Doughty, Dima Damen and Walterio Mayol-Cuevas</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>2つの動画から、手術や絵を描くなどの技能がどちらが上かを予測する手法の提案。入力動画をTemporal Segment Networks(リンク参照)によりいくつかのセグメントに分割し，技能評価に用いるフレームを3枚選択する。
技能評価の学習は、2つの動画のどちらが技能が上か、2つの動画の技能が同じであるとき同じであると判定できるかの2つの尺度をロスとして行う。
技能を表すスコアは、Two Stream CNN(リンク参照)によって空間と時間それぞれについてスコアを取得する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Whos_Better_Whos_Best_Pairwise_Deep_Ranking_for_Skill_Determination.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>手術、ピザ生地をこねる、絵を描く、箸を使うの4つの技能を撮影したデータセットにより実験を行った。そのうち絵を描く、箸を使うは新たにデータセットを構築した。
全てのタスクで70%以上の精度を達成し、箸を使う以外のタスクではベースラインと比べ精度が向上した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1703.09913">論文</a></li><li><a href="https://arxiv.org/abs/1406.2199">Two Stream CNN</a></li><li><a href="https://arxiv.org/abs/1608.00859">Temporal Segment Networks</a></li></ul></div></div><div class="slide_index">[#161]</div><div class="timestamp">2018.5.22 17:48:35</div></div></section><section id="LiteFlowNet_A_Lightweight_Convolutional_Neural_Network_for_Optical_Flow_Estimation"><div class="paper-abstract"><div class="title">LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation</div><div class="info"><div class="authors">T. Hui et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">FlowNet2よりも，性能が良く，モデルサイズが小さく，高速に動作するOptical Flow推定手法を提案．
FlowNet2（Feature Warping, Correlation）は性能が良いけどモデルサイズが大きい，
SPyNet（ピラミッド構造を採用）はモデルが小さいけど性能はあまり良くない，
ということで，提案手法は両者の良いところを合わせることをしている．
2フレームを入力として，各フレームをCNNに入れてピラミッド構造の特徴表現を得る．
一番解像度の低いところから順にFlow推定を繰り返していって洗練化していく．
各Flow推定では軽量な2つのモデルをカスケードさせたりして2フレーム間の大きな移動にも対応しながら，
軽量かつ高速な推定を実現．
</div></div><div class="item2"><img src="slides/figs/LiteFlowNet_A_Lightweight_Convolutional_Neural_Network_for_Optical_Flow_Estimation.png" alt="LiteFlowNet_A_Lightweight_Convolutional_Neural_Network_for_Optical_Flow_Estimation.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>軽量な2つのネットワークをカスケードさせて使うCascaded flow inferenceの提案</li><li>CNNベースのFlow推定にFlow Regularizationを導入</li><li>高性能，省メモリ，高速な推定を実現</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1805.07036">論文 (arXiv)</a></li><li><a href="http://mmlab.ie.cuhk.edu.hk/projects/LiteFlowNet/">プロジェクトページ</a></li><li><a href="https://github.com/twhui/LiteFlowNet">コード (GitHub)</a></li><li>カスケード構造が複雑でなぜこれが良いのか少し納得しにくい</li><li>実験は各コンポーネントのON/OFFで性能比較がわかりやすい</li></ul></div></div><div class="slide_index">[#162]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="Person_Transfer_GAN_to_Bridge_Domain_Gap_for_Person_Re-Identification"><div class="paper-abstract"><div class="title">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</div><div class="info"><div class="authors">Longhui Wei, Shiliang Zhang, Wen Gao and Qi Tian</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>Person Re-identification (ReID)のパフォーマンスは大きく向上したが，複雑なシーンや照明の変化、視点や姿勢の変化といった問題の調査は未だなされていない．本稿ではこれらの問題に関する調査を行った．このためにMulti-Scene MultiTime person ReID dataset (MSMT17)を構築した．またドメインギャップがデータ間に存在するため，このドメインギャップを埋めるためのPerson Transfer Generative Adversarial Network (PTGAN)を提案した．実験ではPTGANによってドメインギャップを実質的に狭められることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180522_PTGAN1.png" alt="1"><img src="slides/figs/20180522_PTGAN2.png" alt="2"><img src="slides/figs/20180522_PTGAN3.png" alt="3"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ReIDを行う際の現実的な問題について網羅的に調査</li><li>新たなReIDデータセットMSMT17を構築</li><li>データ間のドメインギャップを埋めるPTGANを提案</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1711.08565.pdf">論文</a></li></ul></div></div><div class="slide_index">[#163]</div><div class="timestamp">2018.5.22 17:09:22</div></div></section><section id="Zero-Shot_Sketch-Image_Hashing"><div class="paper-abstract"><div class="title">Zero-Shot Sketch-Image Hashing</div><div class="info"><div class="authors">Yuming Shen, Li Liu, Fumin Shen and Ling Shao</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>大規模スケッチベース画像検索において，既存の手法では学習中にカテゴリの存在しないスケッチクエリがある場合失敗するという問題がある．本稿ではそのような問題を解決するZero-shot Sketch-image Hashing(ZSIH)モデルを提案した．2つのバイナリエンコーダとデータ間の関係を強化する計3つのネットワークで構成される．重要な点として，Zero-shot検索での意味的な表現を再構成する際に生成的ハッシングスキームを定式化する点である．Zero-shotハッシュ処理を行う初のモデルであり，関連する研究と比較しても著しく精度が向上した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180522_ZSIH1.png" alt="1"><img src="slides/figs/20180522_ZSIH2.png" alt="2"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>スケッチイメージハッシングの研究において初のZero-shot</li><li>意味的な表現を再構成する際に生成的ハッシングスキームを定式化</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.02284.pdf">論文</a></li></ul></div></div><div class="slide_index">[#164]</div><div class="timestamp">2018.5.22 16:03:53</div></div></section><section id="Lions_and_Tigers_and_Bears_Capturing_Non-Rigid_3D_Articulated_Shape_from_Images"><div class="paper-abstract"><div class="title">Lions and Tigers and Bears: Capturing Non-Rigid, 3D, Articulated Shape from Images</div><div class="info"><div class="authors">Silvia Zuffi, Angjoo Kanazawa and Michael J. Black</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>3Dスキャンは人間をキャプチャするために設計されており，自然環境での使用や野生動物のスキャンおよびモデリングには不向きという問題がある．この問題を解決する方法として，画像から3Dの形状を取得する方法を提案した．SMALモデルを画像内の動物にフィット，形状が一致するようにモデルの形状を変形(SMALR)，さらに複数の画像においても整合性がとれるよう姿勢を変形させ、詳細な形状を復元する．本手法は，従来の手法に比べ大幅に3D形状を詳細に抽出することを可能にするだけでなく，正確なテクスチャマップを抽出し，絶滅した動物といった新しい種についてもモデル化できることを可能にした．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180501_lion.png" alt="Item3Image"><img src="slides/figs/20180501_lion2.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>3Dスキャンが困難な動物のモデルを構築する方法を提案</li><li>SMALモデルを基として形状を変形させることで，より詳細な3D復元が可能</li><li>上記手法により，一貫したテクスチャマップの抽出が可能</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://files.is.tue.mpg.de/black/papers/zuffiCVPR2018.pdf">論文</a></li></ul></div></div><div class="slide_index">[#165]</div><div class="timestamp">2018.5.22 15:06:58</div></div></section><section id="DOTA_A_Large-scale_Dataset_for_Object_Detection_in_Aerial_Images"><div class="paper-abstract"><div class="title">DOTA: A Large-scale Dataset for Object Detection in Aerial Images</div><div class="info"><div class="authors">Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, Liangpei Zhang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Tetsuya Narita</div><div class="item1"><div class="text"><h1>概要</h1><p>俯瞰画像から物体検出するためのデータセットを提案．従来のデータセットのものよりも小さい物体が多いデータセットである．各画像は4000×4000ピクセルであり，さまざまな大きさ，向き，形状を示す物体を含む．データセットは15カテゴリに分類されており，188282のインスタンスを含み，それぞれは任意の四角形でラベリングされている．人工衛星での物体検出の基礎構築のために，DOTA上の最先端の物体検出アルゴリズムを評価した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DOTA.png" alt="DOTA.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>俯瞰画像データセット内のインスタンスは小さいものの割合が高く，細かいものも検出可能人工衛星による物体検出に応用が利く可能性を示唆．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.10398">論文</a></li></ul></div></div><div class="slide_index">[#166]</div><div class="timestamp">2018.5.21 18:34:11</div></div></section><section id="Illuminant_Spectra-based_Source_Separation_Using_Flash_Photography"><div class="paper-abstract"><div class="title">Illuminant Spectra-based Source Separation Using Flash Photography</div><div class="info"><div class="authors">Zhuo Hui, Kalyan Sunkavalli, Sunil Hadap, and Aswin C. Sankaranarayanan</div><div class="conference">CVPR2018</div><div class="paper_id">752</div></div><div class="slide_editor">Kouyou OTSU</div><div class="item1"><div class="text"><h1>概要</h1><p>フラッシュを当てた状態の写真とそうでない写真の2種類を利用して，画像を光源の違いに基づく構成画像へと自動的に分離するアルゴリズムの提案．2つの写真の色情報の違いに基づき，光源に対応するスペクトルや陰影との関係を見出す．従来手法と比較して，光の色合いや陰影を忠実に反映した低ノイズでの分離が可能であることを示した(従来手法(Hsu et.al.)でのSNR:10.13dB 提案手法でのSNR 20.43dB)．また，提案手法が画像のライティングの編集，カラー測光ステレオに有用であることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Illuminant_Spectra-based_Source_Separation_Using_Flash_Photography.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>光源分離にカメラのフラッシュを利用（手軽）</li><li>従来手法を上回る性能．</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1704.05564">論文</a></li><li><a href="https://www.youtube.com/watch?v=7iizopNYJT0">動画</a></li></ul></div></div><div class="slide_index">[#167]</div><div class="timestamp">2018.5.21 20:53:52</div></div></section><section id="Multi-Label_Zero-Shot_Learning_with_Structured_Knowledge_Graphs"><div class="paper-abstract"><div class="title">Multi-Label Zero-Shot Learning with Structured Knowledge Graphs</div><div class="info"><div class="authors">Chung-Wei Lee, Wei Fang, Chih-Kuan Yeh, Yu-Chiang Frank Wang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Shusuke Shigenaka</div><div class="item1"><div class="text"><h1>概要</h1><p>この論文は,各々の入力インスタンスに対して,複数の見えないクラスラベルを予測できるmulti-label learning及びmulti-label zero-shot learning(ML-ZSL)の新しい深層学習の提案した研究．
提案手法は複数のラベル間で人間が関心を持つsemantic knowledgeをグラフの中に組み込むことにより,
情報伝播メカニズムを学習し見えているクラスと見えないクラスの間の相互依存関係をモデル化することに適用できる．
本手法はstate-of-the-artと比較して,同等または改善されたパフォーマンスとして達成をすることができる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/171106526.png" alt="171106526"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・見た目だけでなく,経験を通して学んだ知識を使って物体を認識・WordNetから観察された知識グラフをend-to-endの学習フレームワークに組み込み,意味空間に電番されるラベル表現と情報を学習
・NUS-81およびMS-COCOの結果をWSABIE,WARP,Fast0Tag,Logisticsと比べたところ精度について一番高い結果を残した．
・ML-ZSLについてもFast0Tagと比べて高い精度を残している．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.06526">論文</a></li></ul></div></div><div class="slide_index">[#168]</div><div class="timestamp">2018.5.22 14:28:22</div></div></section><section id="Wasserstein_Introspective_Neural_Networks"><div class="paper-abstract"><div class="title">Nonlinear 3D Face Morphable Model</div><div class="info"><div class="authors">Luan Tran, Xiaoming Liu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>generatorとdiscriminatorを一つのモデルで表現するIntrospective Neural Network(INN)に対してwasserstein distanceを導入することで、INNと同等の生成能力・識別能力を保ちつつclassifierにおけるCNNの数を20分の1にしたWasserstein INN(WINN)を提案。
生成された画像の比較はDCGAN、INN for generative(INNg)、INNgのclassifierにおけるCNNを一つにしたINNg-singleと行った。
またadversarial exampleに対して頑健な識別精度を達成した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Wasserstein_Introspective_Neural_Networks.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>INNにwasserstein distanceを導入することで、生成・識別においてINNと同等以上の性能を持ちながら識別器におけるCNNの数が20分の1であるIWNNを提案。</li><li>テクスチャの生成やCelebA・SVHNを学習することで生成された画像はDCGANと比べてはっきりとしており質が高い。</li><li>CIFAR-10の学習によって生成された画像におけるInception scoreはDCGANの方が良い結果となった。</li><li>CNN、ReosNet、ICNと比較して、adversarial exampleに対する誤識別率が低く、 adversarial examples に惑わされずに識別を行うことが可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.08875">論文</a></li><li><a href="https://github.com/kjunelee/WINN">GitHub</a></li><li><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lazarow_Introspective_Neural_Networks_ICCV_2017_paper.pdf">Introspective Neural Networks for Generative Modeling</a></li></ul></div></div><div class="slide_index">[#169]</div></div></section><section id="Nonlinear_3D_Face_Morphable_Model"><div class="paper-abstract"><div class="title">Nonlinear 3D Face Morphable Model</div><div class="info"><div class="authors">Luan Tran, Xiaoming Liu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>3Dスキャンデータを使用せずにin-the-wildな顔画像のみを用いてencoder-decoderによって3D Morphable Model(3DMM)を生成する手法を提案。生成された3DMMを nolinear 3DMMと呼んでいる。
従来のlinear 3DMMは学習のために3Dスキャンデータが必要であり、かつPCAによって次元削減を行うため表現力に乏しいという問題点があった。
提案手法ではencoderによってプロジェクション、シェイプ、テクスチャのパラメタを取得し、decoderによってシェイプ、テクスチャを推定する。
また初期の学習では既存手法によって得られる3DMMのプロジェクションパラメタ、
シェイプパラメタとUV空間から得られるテクスチャを擬似的なGTとすることで弱教師学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Nonlinear_3D_Face_Morphable_Model.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>3Dスキャンデータを使用せずに、in-the-wildな顔画像のみを学習させることで、入力画像から3D Morphalbe Modelを生成する。</li><li>linear 3DMMと比較して、3次元形状、テクスチャの精度が高い。また見た目もGTにより近い。</li><li>顔のアラインメントにおいてstate-of-the-artよりも高い精度を達成。</li><li>3次元形状における精度はstate-of-the-artと同等であった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><li>弱教師学習がどれほど影響を持つかが気になった。</li><ul><li><a href="https://arxiv.org/abs/1804.03786">論文</a></li><li><a href="http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html">Project page</a></li></ul></div></div><div class="slide_index">[#170]</div></div></section><section id="UV-GAN_Adversarial_Facial_UV_Map_Completion_for_Pose-invariant_Face_Recognition"><div class="paper-abstract"><div class="title">UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition</div><div class="info"><div class="authors">Jiankang Deng, Shiyang Cheng, Niannan Xue, Yuxiang Zhou, Stefanos Zafeiriou</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>in-the-wildな入力顔画像から得られるUVマップの補完をU-Netで行う手法を提案。入力画像に対して3D Morphalbe Modelを適用し不完全なUVマップを取得し、U-Netで補完を行うように学習を行う。
discriminatorにはUVマップ全体と顔領域の判定をさせる。
またUVマップの個人性が失われないように、アイデンティティーに関するロスを取る。
1892人のUVマップをもつWildUVデータセットの構築も行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/UV-GAN_Adversarial_Facial_UV_Map_Completion_for_Pose-invariant_Face_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>in-the-wildな顔画像に対してもリアルかつ精度の高いUVマップの補完を達成。入力されるUVマップが50%欠けていても補完可能。</li><li>入力画像からUVマップと3D shapeを取得するため、入力画像を任意の顔向きに編集可能。</li><li>横向き顔画像から生成されたUVマップはPSNR, SSIMにおいて既存手法を上回る精度を達成。</li><li>frontal-profile face verificationにおいてstate-of-the-artを上回る94.05%を達成。</li><li>1892のアイデンティティーのUVマップをもつ大規模UVマップデータセットであるWildUVデータセットを公開（予定）。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.04695">論文</a></li></ul></div></div><div class="slide_index">[#171]</div></div></section><section id="LIME_Live_Intrinsic_Material_Estimation"><div class="paper-abstract"><div class="title">LIME: Live Intrinsic Material Estimation</div><div class="info"><div class="authors">A. Meka, M. Maximov, M. Zollhöfer, A. Chatterjee, H.P. Seidel, C. Richardt and Ch. Theobalt</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>単RGB画像で，リアルタイムに材質反射特性を推定する手法を提案し，デモシステムを作った．</p><p>構造は，主に複数のU-Netからなり，それぞれ前景セグメンテーション，スペキュラー推定，鏡面反射推定を行う．ロス関数も定義．</p><p>さらに，形状情報も使えるのなら，低・高周波光源情報の推定も可能．連続撮影時の光源情報の連続性を考慮した時系列統合の枠組みも提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/LIME_Live_Intrinsic_Material_Estimation.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>実用的なシチュエーション（リアルタイム，複雑な光源下，連続撮影）で利用可能であることを示している．</li><li>定性，定量評価を行い，性能の良さを示している．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>デモビデオを作り慣れているように見えるあたり，CG勢と思われる．デモも結構評価されているだろうか．
アプリケーション枠で評価されるように書いているかもしれない．</p><ul><li><a href="https://arxiv.org/abs/1803.05619">arXiv</a></li><li><a href="https://www.youtube.com/watch?v=5ntLiAYsMm4">Youtube</a></li><li><a href="http://gvv.mpi-inf.mpg.de/projects/LIME/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#172]</div><div class="timestamp">2018.5.21 21:08:44</div></div></section><section id="Fast_End-to-End_Trainable_Guided_Filter"><div class="paper-abstract"><div class="title">Fast End-to-End Trainable Guided Filter</div><div class="info"><div class="authors">H. Wu, S. Zheng, J. Zhang, K. Huang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>低解像度＋高解像ガイダンスマップを与えると，高解像度画像を効率的（省計算時間，省メモリ）に出力できるGuided Filtering Layerなるものを提案．</p><p><a href="http://kaiminghe.com/eccv10/">GuidedFilter</a>は，
空間的に変化する線形変換行列のグループとして表現でき，
CNNに統合可能．つまり，end-to-endで最適化可能な
深層ガイデッドフィルタネットワークを構成できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Fast_End-to-End_Trainable_Guided_Filter_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Fast_Image_Processing_ICCV_2017_paper.pdf">Context Aggregation Network</a>にGuided Filtering Layerを載せたものを、5つの先進的な画像処理タスクで試したところ，<strong>10～100倍高速</strong>であり，<strong>SoTA性能</strong>も出た．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>かなり省コストになっている．DNN導入可能にするように（エレガントに）定式化し，コストダウンしつつ深層学習できるようにする手法がいくつか見られている．</p><ul><li><a href="https://arxiv.org/abs/1803.05619">arXiv</a></li><li><a href="https://github.com/wuhuikai/DeepGuidedFilter">GitHub</a></li></ul></div></div><div class="slide_index">[#173]</div><div class="timestamp">2018.5.21 20:01:20</div></div></section><section id="Guide_Me_Interacting_with_Deep_Networks"><div class="paper-abstract"><div class="title">Guide Me: Interacting with Deep Networks</div><div class="info"><div class="authors">Christian Rupprecht, Iro Laina, Nassir Navab, Gregory D. Hager and Federico Tombari</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/">Shintaro Yamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>CNNにより学習したタスクの出力結果に対して、人間がヒント(例：画像中に空は見えない)を与えていくことで精度向上を図る研究。CNNモデルをheadとtailの2つのパートに分割し、headから得られた特徴マップをヒントによって修正していくことで精度の向上を実現する。
その際、ネットワークの重みを更新するのではなく修正に用いるパラメータを言語情報から推測することで行う。
ネットワークの予測結果とground truthの差分を取り、正しく予測できていない物体の種類や位置を推定することで学習に用いる文章は自動で生成する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Guide_Me_Interacting_with_Deep_Networks.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>セマンティックセグメンテーションにより実験を実施したところ、クラス間違い、物体の一部が欠けている、物体の一部のみが見えるといったケースにおいて精度が向上することを確認した。ヒントを繰り返し与えていくことはノイズとなってしまうためあまり精度が向上しなかった。
従来のディープラーニングは一度学習をしてしまうと得られる出力が固定されてしまうのに対して、人間が介入することで結果を変えるという新しい応用方法を提案している。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.11544">論文</a></li></ul></div></div><div class="slide_index">[#174]</div><div class="timestamp">2018.5.21 16:15:43</div></div></section><section id="Face_Detector_Adaptation_without_Negative_Transfer_or_Catastrophic_Forgetting"><div class="paper-abstract"><div class="title">Face Detector Adaptation without Negative Transfer or Catastrophic Forgetting</div><div class="info"><div class="authors">Muhammad Abdullah Jamal, Haoxiang Li, Boqing Gong</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔検出におけるターゲットドメインからソースドメインへのadaptationを、negative transferとcatastrophic forgettingの両方を引き起こさずに行う手法を提案。
negative transferとはadaptation後のソースドメインにおける検出精度がadaptation前のソースドメインにおける検出精度に劣ることを指し、
catastorophic forgettingとはadaption後におけるソースドメインの検出精度が著しく下がることを指す。
提案手法では、ソースドメインとターゲットドメインの違いを、ロス関数とDNNの重みの差分で表現し、
この差分がなくなるように学習を行う手法を提案。
またターゲットドメインにface or notのラベルがないという状況も考えて教師あり学習だけでなく教師なし学習、
半教師あり学習の結果についても議論を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Face_Detector_Adaptation_without_Negative_Transfer_or_Catastrophic_Forgetting.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ソースドメインとターゲットドメインの違いを、DNNのロス関数・重みの差分で表現することでadaptationを行った。</li><li>実験は、CascadeCNN+AFLW(25000 faces), Faster-R CNN+WIDER FACE dataset(393,703 faces, highly labeled)の2つのモデルでソースドメインの学習を行い、ターゲットドメインははFDDB(5171 labeled faces)、COFWで行った。</li><li>検出結果はターゲットドメインのみを学習した検出器、ソースドメインからターゲットドメインへfine tuningされた検出器、domain adaptaionを行うstate-of-the-artと比較を行った。提案手法はターゲットドメインにおける検出においてもっとも高い精度を達成。
またソースドメインにおける検出においてもターゲットドメインのみを学習した識別器と同等の精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>adaptationというより、もはやトレーニングデータセットの事後拡張となっており、後でトレーニングデータを追加したくなった時に有用なのではないだろうか。</li><li><a href="http://boqinggong.info/papers/cvpr18-deep-face.pdf">論文</a></li><li><a href="http://boqinggong.info/papers/cvpr18-deep-face-supp.pdf">Supplementary</a></li></ul></div></div><div class="slide_index">[#175]</div></div></section><section id="Extreme_3D_Face_Reconstruction_Looking_Past_Occlusions"><div class="paper-abstract"><div class="title">Extreme 3D Face Reconstruction: Looking Past Occlusions</div><div class="info"><div class="authors">Anh Tuấn Trần, Tal Hassner, Iacopo Masi, Eran Paz, Yuval Nirkin, Gérard Medioni</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要s</h1><p>入力顔画像からバンプマップや視点を推定することで、入力画像からは見えていない側面や、強いオクルージョンがある顔画像からも精度の高い三次元形状を取得する手法を提案。
入力画像から帯域的な情報として三次元の大まかな形と、
局所的な情報としてしわなどのディティールを表現するバンプマップを別々のDNNモデルを使って取得する。
続いてオクルージョンがある場合には、バンプマップが不自然な起伏を持つため深層学習による修正を行う。
最後に顔の対称性を利用して、入力画像からは見えていない側面などをルールベースで復元する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Extreme_3D_Face_Reconstruction_Looking_Past_Occlusions.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力画像から3Dモデル全体を一気に復元するのではなく、帯域的な特徴と局所的な情報を分けて取り扱うことで精度の高い三次元復元を可能にした。</li><li>結果の評価は復元された三次元形状による個人認証の精度で行っている。画像にオクルージョンがない場合にはstate-of-the-artよりも高い精度を達成。オクルージョンがある場合でも、オクルージョンがない場合よりと比べて2%ほどしか劣らなかった。(state-of-the-artはそもそもオクルージョンを考慮できない。)</li><li>復元された三次元形状は、既存手法がオクルージョンを考慮することができなかったりシワなどの復元ができていないのに対して、提案手法ではオクルージョンがある場合でもシワなどの詳細な情報を復元できている。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>帯域的な顔形状の復元やバンプマップの修正などを既存手法に頼っているものの、復元された三次元形状は既存手法に比べて圧倒的なクオリティを持つ。
しかし形状自体のGTとの比較がなかったのが残念。</li><li><a href="https://arxiv.org/abs/1712.05083">論文</a></li><li> <a href="https://github.com/anhttran/extreme_3d_faces">GitHub</a></li></ul></div></div><div class="slide_index">[#176]</div></div></section><section id="InverseFaceNet_Deep_Monocular_Inverse_Face_Rendering"><div class="paper-abstract"><div class="title">InverseFaceNet: Deep Monocular Inverse Face Rendering</div><div class="info"><div class="authors">Hyeongwoo Kim, Michael Zollhöfer, Ayush Tewari, Justus Thies, Christian Richardt, Christian Theobalt</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>実世界の3D顔モデルを使用せず合成された3DモデルのみでCNNをトレーニングすることで、実世界の顔画像から顔向き、形、表情、リフレクタンス、イルミネーションの3D復元を行う手法を提案。
CNNをトレーニング際の問題点として、実世界の3D顔モデルに対するアノテーションが足りないという問題があった。
これに対して、実世界の顔画像から推定されるパラメタと合成顔から推定されるパラメタに対してself-supervised bootstrappingを行うことで、
トレーニングに使用する合成顔3Dモデルのパラメタの分布を実世界のパラメタの分布に近づくようにトレーニングデータを逐次的に更新を行うことで、
CNNの学習を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/InverseFaceNet_Deep_Monocular_Inverse_Face_Rendering.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>self-supervised bootstrappingを使用することで、実世界のパラメータを再現するように合成顔のデータセットを再構築することで、データセットがないという問題に取り組んだ。</li><li>既存の学習ベースの手法に比べて、ジオメトリーにおいて最も高い精度を達成。</li><li>最適化ベースの手法に比べると、パーツのディティールやシワの再現の精度が悪い。</li><li>リミテーションとして、データセットにない顔向きや髪によるオクルージョンを考量することができない。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>異なるドメインを使ったトレーニングの方法として、GANを使ってcross domainの分布を近づける方法が提案されているなど、トレーニングデータ不足を解決する方法が提案されてきている。</li><li><a href="https://arxiv.org/abs/1703.10956">論文</a></li><li><a href="https://web.stanford.edu/~zollhoef/papers/arXiv17_Inverse/supple.pdf">Supplementary</a></li></ul></div></div><div class="slide_index">[#177]</div></div></section><section id="Towards_Pose_Invariant_Face_Recognition_in_the_Wild"><div class="paper-abstract"><div class="title">Towards Pose Invariant Face Recognition in the Wild</div><div class="info"><div class="authors">Jian Zhao, Yu Cheng, Yan Xu, Lin Xiong, Jianshu Li, Fang Zhao</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>様々な照明環境、表情をした横向き顔画像を入力として、正面顔画像を生成することで高い個人認証率を達成するGANベースのPose Invariant Model(PIM)というネットワークを提案。
学習で使用できるトレーニングデータが少ないため、効率的かつ過学習を防ぐために以下のようにPIMを構築。<li>顔全体を生成するgeneratorと両目・鼻・口の4つのパーツを生成するgeneratorを用意。</li><li>4つのパーツが検出された画像と取得できない画像(横顔画像など)を異なるドメインの画像とみなして、cross-domain adversarial trainingを行うことで、両目・鼻・口を復元。</li><li>上記のGANを２セット用意し、discriminator同士でlearning to learnを行うことで効率的な学習を行った。</li></p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Towards_Pose_Invariant_Face_Recognition_in_the_Wild.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>2つのGANをもつTP-GANやDR-GANは最適化が困難で合ったが、これに対してlearning-to-learnを導入することでこの問題を解決。</li><li>MultiPIE、CFPデータセットにおいて様々な角度の顔画像に対する個人識別においてほぼ全てのケースにおいてstate-of-the-artよりも優れた精度を達成。(唯一Multi-PIEで顔向きが±30°の場合にTP-GANに劣った。)</li><li>横向き顔画像から生成される正面顔画像において、既存手法ではテクスチャが崩れていたり完全に正面を向いていない場合があったが、提案手法では見た目が良い正面顔画像を生成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>データセットが少ないという根本的な問題に対して、cross-domain adversarial training、learing to learnを行うことで解決しているが、これがデータベースが欠乏している他の問題設定でも解決できるのかを試してみたい。</li><li><a href="https://arxiv.org/abs/1803.00130">論文</a></li></ul></div></div><div class="slide_index">[#178]</div></div></section><section id="Ring_loss_Convex_Feature_Normalization_for_Face_Recognition"><div class="paper-abstract"><div class="title">Ring loss: Convex Feature Normalization for Face Recognition</div><div class="info"><div class="authors">Yutong Zheng, Dipan K. Pal and Marios Savvides</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>DNNによって得られた特徴量を超球面上に配置するように正規化を行うロス関数であるRing lossを提案。特に教師あり識別問題においてはDNNによる特徴量を正規化することでより精度の高いモデルを構築することができる、
というアイディアもとにRing lossを提案。
SoftMaxといった基本的なロス関数と組み合わせることでより高い精度を達成。
実験には様々な識別タスクを行うことができる顔データセットを用いることで、精度の向上を確認した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Ring_loss_Convex_Feature_Normalization_for_Face_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>SoftMaxとSphereFaceにRing lossを組み合わせることでLFW, IJB-A Janus, Janus CS3, CFP, MegaFaceデータセットにおけるface verification, identificationにおいて他のロス関数と同等あるいはそれ以上の精度を達成。</li><li>極端に低解像度の画像におけるface matchingにおいてベースラインの手法を凌駕した。</li><li>実験ではResNet64を使用。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.00130">論文</a></li></ul></div></div><div class="slide_index">[#179]</div></div></section><section id="Label_Denoising_Adversarial_Network_LDAN_for_Inverse_Lighting_of_Face_Images"><div class="paper-abstract"><div class="title">Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images</div><div class="info"><div class="authors">Hao Zhou, Jin Sun, Yaser Yacoob, David W. Jacobs</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>3Dモデルから実画像へのドメイン変換をGANによって行うことで、単一顔画像から照明パラメタを推定するLabel Denoising Adversarial Network(LDAN)を提案。
人の顔画像に対して照明パラメタ(論文で使用されているのは37次元の球面調和関数)がアノテーションされたデータセットがないため、
3Dモデルを使用してFeature Netと呼ばれるネットワークで中間特徴量を取得し、
中間特徴量からLightning Netを用いて照明パラメタの推定を学習。
続いて人の顔画像に対して、既存手法を用いてノイズが乗った照明パラメタを取得し、
人の顔画像に対してもFeature Netを新しく学習し、
3D モデルから得られた中間特徴量と共にGANに入力することでドメインの変換を行うことでノイズが除去された照明パラメタを取得。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Label_Denoising_Adversarial_Network_LDAN_for_Inverse_Lighting_of_Face_Images.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>単一画像からの照明パラメタの推定という問題に対して、初めて学習ベースの手法を提案。</li><li>結果の比較は19の照明環境が用意されているMultiPieデータセットで行い、推定されたパラメータに対する識別を行うことで精度を評価。state-of-the-artに比べて識別精度およびユークリッド距離・Q値におけるAUCで最も高い精度を達成。</li><li>同問題を扱う既存手法が最適化ベースということもあり、既存手法と比べて10万倍のスピードで実行可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>GANを使って異なるドメインの特徴量を同じ空間にマップする考え方は既に<a href="https://arxiv.org/abs/1702.05464">Adversarial Discriminative Domain Adaptation</a>によって提案されているが、異なる点としては[Eric et al.]はGANのロスしか使っていないが、この方法では写像がうまく行かず、
A→A', B→Bと学習して欲しいところをやA→B', B→A'といった写像を学習してしまう。
これを解消するために、lightning netで得られたパラメータに対するL2ロスを取ることでこれを解消。</li><li><a href="https://arxiv.org/abs/1802.06713">論文</a></li></ul></div></div><div class="slide_index">[#180]</div></div></section><section id="Disentangling_3D_Pose_in_A_Dendritic_CNN_for_Unconstrained_2D_Face_Alignment"><div class="paper-abstract"><div class="title">Disentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face Alignment</div><div class="info"><div class="authors">Amit Kumar, Rama Chellappa</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔向きをコンディションとして与え木構造で表された顔のランドマークを学習させることで、顔のランドマーク推定を行うPose Conditioned Dendritic CNN(PCD-CNN)を提案。
顔のコンディションはPoseNetにより出力された値を使用する。
顔のランドマークを木構造として与えることで、ランドマークの位置関係を利用してCNNを学習させた。
また提案ネットワークはPCD-CNNと通常のCNNの二段階になっており、
後段のCNNをファインチューニングすることでランドマークのポイント数が違うデータセットや顔向き推定などの他のタスクにも適用可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Disentangling_3D_Pose_in_A_Dendritic_CNN_for_Unconstrained_2D_Face_Alignment.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ネットワークをPCD-CNNとCNNの二段階で構成することで、異なるランドマークのポイント数や顔向き推定といった他のタスクにも適用可能。</li><li>顔向きをコンディションとして与えることで推定精度が向上。また、20FPSで実行が可能。　</li><li>AFLW, AFWデータセットにおいてランドマークの推定精度がstate-of-the-artよりも高い推定精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.06713">論文</a></li></ul></div></div><div class="slide_index">[#181]</div></div></section><section id="Multi-Image_Semantic_Matching_by_Mining_Consistent_Features"><div class="paper-abstract"><div class="title">Multi-Image Semantic Matching by Mining Consistent Features</div><div class="info"><div class="authors">Qianqian Wang, Xiaowei Zhou and Kostas Daniilidis</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Kazuho Kito</div><div class="item1"><div class="text"><h1>概要</h1><p>ノイズを考慮しつつ、数千もの画像セット全てにおいて一致する(信頼できる)特徴を見出すことで、画像間の対応を図るマッチング手法。マッチングはセマンティック性を考慮することができる（目と目、耳先と耳先など）これにより、一貫性がある画像セット内で信頼できる特徴の関係を確立。何千もの画像を処理する場合にスケーラブルな手法。つまりは数に頑健。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Multi-Image_Semantic_Matching_by_Mining_Consistent_Features.PNG" alt="Multi-Image_Semantic_Matching_by_Mining_Consistent_Features.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来手法では、全てのペアで対応する関係を最適化していたが、本手法では、特徴の選択とラベリングに着目し、信頼度の高い特徴のみを用いた疎なセットのみで識別、マッチングする。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>図は中の左が出力結果であり、目は青、耳は黄色、鼻は赤など各特長の意味を理解し、マッチングを成功させている。</p><ul><li><a href="https://arxiv.org/pdf/1711.07641.pdf">論文</a></li></ul></div></div><div class="slide_index">[#182]</div><div class="timestamp">2018.5.21 11:27:27</div></div></section><section id="Learning_Intrinsic_Image_Decomposition_from_Watching_the_World"><div class="paper-abstract"><div class="title">Learning Intrinsic Image Decomposition from Watching the World</div><div class="info"><div class="authors">A. Uthors, B. Uthors and C. Uthors</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Intrinsic Image Decompositionのために，時間経過とともに照明が変化するビデオを使ったCNNの学習方法を提案．正解の Intrinsic Imageが不要な点が強みである．学習が完了したモデルは単一画像に対して適用できるよう汎化しており，いくつかのベンチマークに対して良い結果となった．<br>Contribution：<br>・データセット（BigTime）の公開．室内，室外両方での照明変化のあるビデオと画像シーケンスのデータセット．<br>・このGround Truthを含まないデータを使った手法の提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Intrinsic_Image_Decomposition_from_Watching_the_World_fig.png" alt="Image"><br>学習時：ラベル無しで，視点が固定され照明が変化するビデオを学習に利用する．<br>テスト時：単一画像からintrinsic image decompositionを行う．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>最適化ベースのIntrinsic Decomposition手法と，機械学習手法の間に位置する手法と言える．<br>・U-netに似た構造のCNN．<br>・Lossの工夫：画像ペア全てを考慮するall-pairs weighted least squares lossとシーケンス全体のピクセル全てを考慮するdense, spatio-temporal smoothness loss．最適化ベースのlossをフィードフォワードネットワークのlossとして利用する．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Intrinsic image decompositionとは，入力された1枚の画像をreflectance画像とshading画像の積に分解する問題のこと．<br>intrinsic imagesのGround Truthを大規模に揃えることは困難．</p><ul><li><a href="https://arxiv.org/abs/1804.00582">arXiv</a></li></ul></div></div><div class="slide_index">[#183]</div><div class="timestamp">2018.5.21 11:26:41</div></div></section><section id="Photographic_Text-to-Image_Synthesis_with_a_Hierarchically-nested_Adversarial_Network"><div class="paper-abstract"><div class="title">Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network</div><div class="info"><div class="authors">Zizhao Zhang, Yuanpu Xie, Lin Yang</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>階層的入れ子構造の識別器を使用し，テキストから高解像画像を生成するGANを提案．end-to-endの学習で高解像画像の統計量を直接モデルリングすることが可能な手法．これは，step-by-stepで高解像画像を生成するStackGANとは異なる点である．複数のスケールの中間層に対して階層的入れ子構造の識別器を使用することで中間サイズレベルでの表現に制約を加え，生成器が真の学習データの分布を獲得しやすくする．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Photographic_Text-to-Image_Synthesis_with_a_Hierarchically-nested_Adversarial_Network_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>新しい構造と，lossの工夫でtext-to-imageのタスクで高解像画像の生成を可能とした．<br>・hierarchical-nested Discriminatorを使用．<br>・lossには，pair lossとlocal adversarial lossを使用する．pair lossでは入力テキストと生成画像が一致しているかを評価．local adversarial lossでは生成画像の細部の質を評価する．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.09178">arXiv</a></li></ul></div></div><div class="slide_index">[#184]</div><div class="timestamp">2018.5.21 11:22:05</div></div></section><section id="Connecting_Pixels_to_Privacy_and_Utility_Automatic_Redaction_of_Private_Information_in_Images"><div class="paper-abstract"><div class="title">Connecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images</div><div class="info"><div class="authors">Tribhuvanesh Orekondy, Mario Fritz, Bernt Schiele</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>プライバシー保護のために画像に含まれる個人的な情報を自動的に改変する手法の提案．プライバシーを守りつつ画像の有用性を保つためのトレードオフが問題となる．有用性を保つためには改変する領域サイズが最小限である必要があり，これをセグメンテーションの問題として取り組む．</p><p>Contribution:</p><ul><li>データセットの公開．様々な種類のプライバシーのラベルが，ピクセルレベルとインスタンスレベルで与えられている自然画像の初のデータセット．</li><li>モデルの提案．多様な個人情報を自動的に改変するモデルを提案する．正解のアノテーションに対して83％の正解率を達成した．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Connecting_Pixels_to_Privacy_and_Utility_Automatic_Redaction_of_Private_Information_in_Images_fig.png" alt="Image"><br>指紋，日時，人，顔，ナンバープレートを黒く塗りつぶせている．<br>他にも，住所やメールアドレスのようなテキスト情報や顔や車椅子などの視覚情報，あるいはテキストと視覚情報を合わせたものなど，多様な個人情報に対応するデータセットとモデルを提案．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>どのような対象(Textual, Visual, Multimodal)を扱うかで使用するモデルは異なる．<br>Textualな対象では，Sequence Labelingを使用する．<br>VisualとMultimodalな対象では，Fully convolutional instance-aware semantic segmentationを使用する．<br>Nearest Neighborなどのベースライン手法と比較を行なっている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>画像全体を黒く塗ればプライバシーは保護されるが，画像の価値がなくなるので，トレードオフが存在する．<br>データセットを作った貢献がメイン．プライバシー保護のためのアノテーションを行ったことで，それなりの正解率で個人情報の改変を行えるようになった．</p><ul><li><a href="https://arxiv.org/abs/1712.01066">arXiv</a></li></ul></div></div><div class="slide_index">[#185]</div><div class="timestamp">2018.5.21 11:17:12</div></div></section><section id="Disentangling_Structure_and_Aesthetics_for_Style-aware_Image_Completion"><div class="paper-abstract"><div class="title">Disentangling Structure and Aesthetics for Style-aware Image Completion</div><div class="info"><div class="authors">Andrew Gilbert, John Collomosse, Hailin Jin, and Brian Price</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ノンパラメトリックのInapinting手法を提案．<br>視覚的な構造とスタイルをdeep embeddingすることで，パッチの検索と選択の際に視覚的なスタイルを考慮することが可能で，さらに，パッチのコンテンツを補完画像のスタイルに合わせるためのneural stylizationが可能となる．この手法は，patch-basedの手法とgenerativeベースの手法の架け橋的な補完手法である．<br>技術的貢献：<br>・style-aware optimization<br>・adaptive stylization</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Disentangling_Structure_and_Aesthetics_for_Style-aware_Image_Completion_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>以下の手順で画像補完を行う．<br>１．スタイルを考慮して穴に埋める候補を検索する<br>２．補完画像と構造とスタイルが合うパッチをMRFで複数集め，選択する<br>３．選択されたパッチを補完画像のスタイルに変換する</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://personal.ee.surrey.ac.uk/Personal/J.Collomosse/pubs/Gilbert-CVPR-2018.pdf">論文pdf</a></li></ul></div></div><div class="slide_index">[#186]</div><div class="timestamp">2018.5.21 11:09:50</div></div></section><section id="DeblurGAN_Blind_Motion_Deblurring_Using_Conditional_Adversarial_Networks"><div class="paper-abstract"><div class="title">DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks</div><div class="info"><div class="authors">Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, Jiˇri Matas</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>motion deblurringのためのGAN(DeblurGAN)を提案．structural similarity measureとアピアランスでSoTA．ブラーを除去した画像で物体検出の精度を出すことで，ブラー除去モデルの質を評価するという方法を提案．提案手法は，質だけでなく実行速度も優れており，従来手法の５倍の速さがある．モーションブラーのかかった画像を合成するための方法を紹介し，そのデータセットもコード，モデルとともに公開．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DeblurGAN_Blind_Motion_Deblurring_Using_Conditional_Adversarial_Networks_fig.png" alt="Image"><br>ブレを除去してからYOLOで検出すると精度が良くなることを示している．これをDeblurモデルの指標にすることができると主張．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>loss：WGANによるAdversarial lossとPerceptual loss</li><li>構造：畳み込み，instance normalization層，ReLU関数から成るResBlockの繰り返しがメインで，出力するときに入力画像を加算するglobal skip connectionを持つ．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>最近のGAN手法やテクニックを詰め込んで，新しく作ったデータセットを利用したらSoTAがでたという感じ．テクニカルな貢献はあまりなさそう．</p><ul><li><a href="https://github.com/KupynOrest/DeblurGAN">GitHub</a></li><li><a href="https://arxiv.org/abs/1711.07064">arXiv</a></li></ul></div></div><div class="slide_index">[#187]</div><div class="timestamp">2018.5.21 11:05:29</div></div></section><section id="Learning_to_Understand_Image_Blur"><div class="paper-abstract"><div class="title">Learning to Understand Image Blur</div><div class="info"><div class="authors">Shanghang Zhang, Xiaohui Shen, Zhe Lin, Radom ́ır Meˇch, Joa ̃o P. Costeira, Jose ́ M. F. Moura</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ボケ(blur)が望ましいのか否かと，そのボケが写真のクオリティーにどのような影響を与えているのかを，自動的に理解するアルゴリズムは少ない．この論文では，blur mapの推定とこのボケの望ましさの分類を同時に行うフレームワークを提案する．</p><p>貢献：</p><ul><li>ボケを検出することと，画像の質という点でボケを理解することを同時に行うのは，おそらく初めての研究．ABC-FuseNetというネットワークを提案．</li><li>１万枚のデータセット（SmartBlur）の公開．ピクセルごとにボケがかかっているか３段階でラベルづけ．さらに，画像ごとにボケの望ましさ(desirability)をラベルづけ．</li><li>SmartBlurと他の公開データセットで実験を行い．blur mapの推定とボケの望ましさの分類がSoTAを超えた．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Understand_Image_Blur_fig.png" alt="Item3Image"><br>ボケ具合をピクセルごとに３段階で示し，ボケの望ましさも出力する．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>ABC-FuseNetでは，低レベルのボケの推定と高レベルの画像内で重要コンテンツの理解の二つを行う．<br>A: attention map，FCNである．<br>B: blur map，Dilated Convolutionとpyramid pooling, Boundary Refinement用の層を使ってblurの推定を行う．<br>C: content feature map，ResNet-50を使ってコンテンツの特徴を抽出．<br>ボケの推定はBによって行い，ボケの望ましさの分類はA, B, Cから得られた特徴を用いて行う．ネットワーク全体をEnd-to-endで学習することができる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ボケを軽減するための研究は多いが，ボケが全て邪魔とは言えない．ボケを効果的に利用することで，写真の印象が良くなることもある．いいボケなのか，悪いボケなのかの判断も必要だというモチベーションがある．</p><p>コード，データセットは以下に公開予定</p><ul><li><a href="https://github.com/Lotuslisa/Understand_Image_Blur">GitHub</a></li><li><a href="http://users.eecs.northwestern.edu/~xsh835/assets/cvpr2018_smartblur.pdf">論文</a></li></ul></div></div><div class="slide_index">[#188]</div><div class="timestamp">2018.5.21 10:50:21</div></div></section><section id="Tags2Parts_Discovering_Semantic_Regions_from_Shape_Tags"><div class="paper-abstract"><div class="title">Tags2Parts: Discovering Semantic Regions from Shape Tags</div><div class="info"><div class="authors">Sanjeev Muralikrishnan, Vladimir G. Kim, Siddhartha Chaudhuri</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>指定された形状のタグに強く関係する領域を検出する手法の提案．明示的に領域ごとのラベリングはなく，さらにあらかじめセグメンテーションされていない状況で，形状のタグを与えた時に領域を発見するという問題設定．難しい点は，オブジェクトのタグという弱い教師情報からポイントごとのラベルを細かく出力する必要があること．このために分類とセグメンテーションを同時に行うネットワークを使う．形状ごとのタグからポイントごとの予測を得るためのネットワーク構造（WU-net）を提案したことがメインの貢献．</p><p>学習が完了すれば，タグが不明な形状に対しても手法を適用することができる．また，元々Weakly-supervised用に提案しているが，strongly-supervised用としても利用できる手法となった．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Tags2Parts_Discovering_Semantic_Regions_from_Shape_Tags_fig.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>U-net風のWU-netを提案．U-netから修正した点は，<br>・浅いU型の構造を3回くりかし，skip-connectionで密に繋がっている．深いU型1回の場合との結果の違いを図示している.<br>・セグメンテーションの用の隠れ層にタグ分類用の層を追加．(元々のは，strongly-supervised セグメンテーション用に設計されているので．)</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>３D形状としてはボクセル表現を使用．64×64×64 cubical gridを入力する．</p><ul><li><a href="https://arxiv.org/abs/1708.06673">arXiv</a></li></ul></div></div><div class="slide_index">[#189]</div><div class="timestamp">2018.5.21 10:40:57</div></div></section><section id="Neural_3D_Mesh_Renderer"><div class="paper-abstract"><div class="title">Neural 3D Mesh Renderer</div><div class="info"><div class="authors">Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ニューラルネットワークに組み込むことができる3Dメッシュのレンダラーである Neural Renderer を提案。レンダリングの『逆伝播』と呼ばれる処理をニューラルネットワークに適した形に定義し直した．そしてこのレンダラーを<br>・一枚の画像からの3Dメッシュの再構成（ボクセルベースの再構成との比較あり）<br>・画像から3Dへのスタイル転移と3D版ディープドリーム<br>に応用できることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Neural_3D_Mesh_Renderer_fig.png" alt="Image"><br>2D-to-3Dスタイルトランスファーの例</p></div></div><div class="item3"><div class="text"><h1>方法</h1><p>従来のままでレンダリングの操作が処理の途中にあると逆伝播が行えない状態であるので，レンダリングのための勾配を定義することでニューラルネットワークの中にレンダリング操作を加えても学習を行えるようにした．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://hiroharu-kato.com/projects/neural_renderer.html">プロジェクトサイト</a></li><li><a href="https://github.com/hiroharu-kato/neural_renderer">GitHub</a></li><li>３Dの形式には様々ある（ポイントクラウド，ボクセル，メッシュなど）が，3Dメッシュは効率的で表現能力が高く直感的な形式だそう．</li></ul></div></div><div class="slide_index">[#190]</div><div class="timestamp">2018.5.21 10:28:19</div></div></section><section id="Demo2Vec_Reasoning_Object_Affordances_from_Online_Videos"><div class="paper-abstract"><div class="title">Demo2Vec: Reasoning Object Affordances from Online Videos</div><div class="info"><div class="authors">Kuan Fang, Te-Lin Wu, Daniel Yang, Silvio Savarese and Joseph J. Lim</div><div class="conference">CVPR2018</div><div class="paper_id">1387</div></div><div class="slide_editor">KazuhoKito</div><div class="item1"><div class="text"><h1>概要</h1><p>商品などのデモンストレーションの映像の特徴を通してその商品などのアフォーダンスを推論する研究．映像から埋め込みベクトルを抜き出すことで，ヒートマップと行動のラベルとして特定のもののアフォーダンスを予測するDemo2Vecモデルを提案．また，YouTubeの製品レビュー動画を集め，ラベリングすることでOnline Product Review detaset for Affordande(OPRA)を構築．</p></div></div><div class="item2"><div class="text"><p> </p><img src="slides/figs/Demo2Vec_Reasoning_Object_Affordances_from_Online_Videos.PNG" alt="Demo2Vec_Reasoning_Object_Affordances_from_Online_Videos.PNG"></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>アフォーダンスのヒートマップと行動のラベルの予測に関し，RNNの基準よりよいパフォーマンスを達成</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>YouTubeで公開されている動画では，Demo2Vecを用いてある物体のデモ動画からSawyer robotのEnd Effectorを予測したヒートマップの地点に移動するように制御させている様子を見ることができる．</p><ul><li><a href="http://ai.stanford.edu/~kuanfang/pdf/demo2vec2018cvpr">論文</a></li><li><a href="https://sites.google.com/view/demo2vec/">ProjectPage</a></li><li><a href="https://www.youtube.com/watch?v=UT1QohPIioU">YouTube</a></li></ul></div></div><div class="slide_index">[#191]</div><div class="timestamp">2018.5.20 22:42:02</div></div></section><section id="Probabilistic_Plant_Modeling_via_Multi-View_Image-to-Image_Translation"><div class="paper-abstract"><div class="title">Probabilistic Plant Modeling via Multi-View Image-to-Image Translation</div><div class="info"><div class="authors">Takahiro Isokane, Fumio Okura, Ayaka Ide, Yasuyuki Matsushita, Yasushi Yagi</div><div class="conference">CVPR 2018</div><div class="paper_id">368</div></div><div class="slide_editor">Goshi Sasaki</div><div class="item1"><div class="text"><h1>概要</h1><p>葉に隠れていても３次元の枝構造を多視点画像から推測できるようにした。多視点からの植物画像を入力として枝構造の２次元確率マップをdropoutを取り入れたPix2Pixで推測して、それらから３次元の確率構造を作成した。最後にpartical floｗシュミレーションによって明確な３次元の枝構造を生成した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Probabilistic_Plant_Modeling.png" alt="architecture"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>葉や他の枝によって隠れてしまっていても枝構造を生成できるようにした。ベイジアンPix2Pixを利用することで植物の３次元構造をより正確に表せるようにした。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.09404.pdf">論文</a></li></ul></div></div><div class="slide_index">[#192]</div><div class="timestamp">2018.5.20 20:53:44</div></div></section><section id="ROAD_Reality_Oriented_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes"><div class="paper-abstract"><div class="title">ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban Scenes</div><div class="info"><div class="authors">Yuhua Chen, Wen Li, Luc Van Gool</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>synthetic-to-realな変換を行う際に、1)モデルがsyntheticにoverfitするstyleの側面と、2)syntheticとrealの分布の違いの側面から発生する2つの問題があることに著者らは着目している。解決するために、前者はtarget guided distillation、後者はspatial-aware adaptationという手法を提案し、それを組み合わせた Reality Oriented ADaptation Network(ROAD-Net)を考案。GTAV/SYNTHIA - Cityscapesの適合タスクで評価し、sotaのsemantic segmentationモデルの汎化性能を向上したことを確認。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/ROAD_Reality_Oriented_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes.PNG" alt="ROAD_Reality_Oriented_Adaptation_for_Semantic_Segmentation_of_Urban_Scenes.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Semantic SegmentationへのDomain Adaptationの適用が新しい。</li><li>結果もまたNonAdaptなPSPNetからmIoUが約11.6%向上している。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://cvpaperchallenge.github.io/CVPR2018_Survey/#/Learning_to_Adapt_Structured_Output_Space_for_Semantic_Segmentation">Learning to Adapt Structured Output Space for Semantic Segmentation</a>と目的と対象が似通っている。どちらもクラス分類で得られる特徴(ImageNetで学習されたpretrain model)がsegmentationでは有効ではないという主張であり、これをもとにそれぞれmulti-scaleな手法と、distillationによる手法と異なるアプローチをとっているのが興味深い。</li><li>spatial-aware adaptationはPatchGANと似通っており同様の性質を持つ？</li></ul><ul><li><a href="https://arxiv.org/abs/1711.11556">arxiv</a></li></ul></div></div><div class="slide_index">[#193]</div><div class="timestamp">2018.5.20 19:20:29</div></div></section><section id="Gated_Fusion_Network_for_Single_Image_Dehazing"><div class="paper-abstract"><div class="title">Gated Fusion Network for Single Image Dehazing</div><div class="info"><div class="authors">Wenqi Ren Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu, Ming-Hsuan Yang</div><div class="conference">CVPR2018</div><div class="paper_id">404</div></div><div class="slide_editor">Kazuma Asano</div><div class="item1"><div class="text"><h1>概要</h1><p>霧がかかった画像(hazy input)から更に３つの入力，White balanced input，Contrast enhanced input，Gamma corrected inputを計算して導出し，これらの異なる入力間の外観差に基づきピクセル単位のConfidence Mapを計算する．これらを学習することで鮮明な画像を生成するMulti-scale Gated Fusion Network(GFN)を開発した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180404GFN_result.png" alt="Item3Image"><img src="slides/figs/180404GFN_network.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来手法と比較し，実装や再現が容易であり，また出力結果もPSNR，SSIMともに従来手法より高い評価となっている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00213">arXiv</a></li><li><a href="https://sites.google.com/site/renwenqi888/research/dehazing/gfn">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#194]</div><div class="timestamp">2018.5.14 12:31:27</div></div></section><section id="AdaDepth_Unsupervised_Content_Congruent_Adaptation_for_Depth_Estimation"><div class="paper-abstract"><div class="title">AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation</div><div class="info"><div class="authors">J.Nath, K.Phani, K.Uppala, A.Pahuja and R.V.Babu</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1803.01599</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>教師あり深層学習による手法は単眼カメラ画像における深さ推定に対して良い結果を出している．しかし．grand truthを得るためにはノイズに影響され，コストもかかる．合成データセットを用いた場合の深度推定では固有のドメインにしか対応していなく，自然なシーンに対して対応するのが難しいと言われる．この問題に対応するため，Adversalな学習と対応したターゲットの明確な一貫性をかすこと事によりAdaDepthを提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/AdaDepth_Unsupervised_Content_Congruent_Adaptation_for_Depth_Estimation.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>高次元の構造化エンコーダ表現に作用する，教師なしの敵対的適応設定AdaDepthを提案．</li><li>新規の特徴を再構成する正則化フレームワークを使用して適応表現にコンテンツ一貫性を課すことでモード崩壊の問題に取り組んだ．</li><li>最小限の教師データでの自然シーンの深度推定タスクにおいてSoTAを達成．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.01599">Paper</a></li></ul></div></div><div class="slide_index">[#195]</div><div class="timestamp">2018.5.20 15:52:52</div></div></section><section id="End-to-end_learning_of_keypoint_detector_and_descriptor_for_pose_invariant_3D_matching"><div class="paper-abstract"><div class="title">End-to-end learning of keypoint detector and descriptor for pose invariant 3D matching</div><div class="info"><div class="authors">Georgios Georgakis, Srikrishna Karanam,Ziyan Wu,Jan Ernst,Jana Kosecka</div><div class="conference">CVPR 2018</div><div class="paper_id">227</div></div><div class="slide_editor">Goshi Sasaki</div><div class="item1"><div class="text"><h1>概要</h1><p>End-to-Endで3次元空間における特徴点の抽出とマッチングを行う手法を提案した。2つの距離画像を入力とし、VGG-16 を利用したFaster R-CNNを基本構造としている。
２つの距離画像からそれぞれVGG−16を利用して特徴マップを作成し、RPNにより領域候補を推定して、ROIプーリング層、全結合層を経て特徴量ベクトルを作り出す。最終的にcontrastive lossを利用して得られた特徴量間の対応関係を求めた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/keypoint_detector.png" alt="architecture"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>初めてEnd-to-Endで3次元マッチングを行えるようにした。ノイズ環境下においてキーポイントマッチングで従来手法のHarris3D +FPFHなどよりも10％以上高い精度を出した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1802.07869.pdf">論文</a></li></ul></div></div><div class="slide_index">[#196]</div></div></section><section id="AttnGAN_Fine-Grained_Text_to_Image_Generation_with_Attentional_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks</div><div class="info"><div class="authors">Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>アテンションドリブン，複数ステージでのRefineによって，テキストから詳細な画像を生成するGANを提案．CUBデータセットとCOCOデータセットでinception scoreがstate of the artを超えた．生成画像の特定の位置をワードレベルで条件付けしていることを示した．</p><p>貢献：<br>・Attentional Generative Adversarial NetworkとDeep Attentional Multimodal Similarity Model(DAMSM)の提案．<br>・実験でstate-of-the-art GAN modelsを超えたことを示す．<br>・ワードレベルで自動的に生成画像の一部をアテンションするのは初である．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/AttnGAN_Fine-Grained_Text_to_Image_Generation_with_Attentional_Generative_Adversarial_Networks_fig.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>・Attentional Generative Networkはセンテンスの特徴から始めて段階的に画像を高精細にしていくネットワークで，途中にアテンションレイヤーからのワード特徴を入力して条件付けする．<br>・各解像度に対してそれぞれDiscriminatorがある．<br>・最終的な解像度になったあと，Image Encoderにて局所的な画像特徴量とし，ワード特徴量とDAMSMにて比較することで，生成画像の細部がどれくらい単語に忠実であるか評価する．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・StackGANの著者も共著にいる．<br>・アテンションにより生成箇所を局所に向けることで，COCOのような複雑なシーンでも対応できるようになっている．</p><ul><li><a href="https://arxiv.org/abs/1711.10485">arXiv</a></li></ul></div></div><div class="slide_index">[#197]</div><div class="timestamp">2018.5.19 13:50:16</div></div></section><section id="From_source_to_target_and_back_Symmetric_Bi-Directional_Adaptive_GAN"><div class="paper-abstract"><div class="title">From source to target and back: Symmetric Bi-Directional Adaptive GAN</div><div class="info"><div class="authors">Paolo Russo, Fabio M. Carlucci, Tatiana Tommasi and Barbara Caputo</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>SBADA-GANの提案．（Symmetric Bi-Directional ADAptive Generative Adversarial Network）<br>unsupervised cross domain classificationにフォーカス.<br>ラベルが与えられるSourceのサンプルを利用して，最終的にはTargetの分類問題を解く．SourceのサンプルをTargetのドメインに(Image-to-Imageの)マッピングをし，同時に逆方向も行う．分類器の学習に利用するのは，Sourceサンプル，TargetをSource風にしたもの，SourceをTarget風にしてさらにSource風に戻した３種類を使う．それぞれにラベルもしくは擬似ラベルを付与して学習する．テスト時はTargetサンプルのクラスを予測したいので，Target用の分類器と，TargetサンプルをSource風にしてから入力するSource用の分類器の２つを使用する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/From_source_to_target_and_back_Symmetric_Bi-Directional_Adaptive_GAN_fig.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>セルフラベリングの使用．Source用の分類器に制約を課す</li><li>class consistency lossの導入．Generatorとともに利用することで両方向のドメイン変換がお互いに影響し合うようになる．安定性と質向上の効果．最終的な目標である分類問題を解くことに有効．</li><li>例えばSource側のDiscriminatorは，RealサンプルとしてSource画像を使い，FakeサンプルとしてTarget画像をSource画像風にGeneratorでドメイン変換した画像を使う．</li><li>（問題設定的に）Source側の分類器にはクラスラベルによる学習ができる．</li><li>SourceとTargetの双方向のサンプル生成のための二つadversarial lossと，二つのclassification lossを同時に最小化する．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1705.08824">arXiv</a></li></ul></div></div><div class="slide_index">[#198]</div><div class="timestamp">2018.5.19 14:15:18</div></div></section><section id="Deep_Photo_Enhancer_Unpaired_Learning_for_Image_Enhancement_from_Photographs_with_GANs"><div class="paper-abstract"><div class="title">Deep Photo Enhancer: Unpaired Learning for Image Enhancement from Photographs with GANs</div><div class="info"><div class="authors">Yu-Sheng Chen, Yu-Ching Wang, Man-Hsin Kao, Yung-Yu Chuang</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>学習ベースで画像のエンハンスメントを行う手法の提案．入力として「良い」写真のセットを使う．このセットに含まれる特色を持つように変換することが「エンハンスメント」に繋がると定義する．エンハンスメント問題をimage-to-imageの問題として扱い，提案手法は「良い」写真のセットの中で共通の特色を発見することを狙っている．普通の写真のドメインを「良い」写真のドメインに変換すれば良いとし，（CycleGANのような）２方向GANを以下の３つの工夫とともに利用する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Photo_Enhancer_Unpaired_Learning_for_Image_Enhancement_from_Photographs_with_GANs_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>Contribution</h1><ul><li>global featureを使ったU-netの利用．これがシーンの状況，照明条件，対象のタイプの情報を捉える．</li><li>WGANのためのadaptive weighting schemeを提案．収束を早める．</li><li>individual batch normalization layersの利用．Generatorは入力データの分布により適応するようになる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>Flickerのレタッチされた写真を利用するなどしている．</li><li>Adobeがプロ写真家一人一人のレタッチ方法を再現するという機能を実装するのも近いかもしれない．</li><li>ハイダイナミックレンジの写真にしたらエンハンスされていると思っている節がある．</li><li><a href="https://www.csie.ntu.edu.tw/~cyy/publications/papers/Chen2018DPE.pdf">論文</a></li></ul></div></div><div class="slide_index">[#199]</div><div class="timestamp">2018.5.19 13:33:54</div></div></section><section id="Imagine_it_for_me_Generative_Adversarial_Approach_for_Zero-Shot_Learning_from_Noisy_Texts"><div class="paper-abstract"><div class="title">Imagine it for me: Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts</div><div class="info"><div class="authors">Yizhe Zhu, Mohamed Elhoseiny, Bingchen Liu, and Ahmed Elgammal</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Wikipediaのようにノイズの多いテキストからzero-shot learningを行うためのGAN用いる方法を提案．GANを使ってテキストが表現するオブジェクトのビジュアル的な特徴を生成する．オブジェクトのクラスごとに特徴を近い位置にembeddingできれば良い．これができれば後は教師あり手法で分類を行えることになる.<br>コントリビューション：</p><ul><li>zero-shot learningにおいてUnseenであるクラスのテキスト記述からvisual featureを生成することで，zero-shot learningを従来の分類問題にしてしまう．generative adversarial approach for ZSL (GAZSL) ．</li><li>ノイズを抑制するためのFC層と埋め込み後のクラス識別性を高めるvisual pivot regularizationの提案．</li><li>zero-shot recognition, generalized zero-shot learning, and zero-shot retrievalという複数のタスクでstate-of-the-art手法を超えた．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Imagine_it_for_me_Generative_Adversarial_Approach_for_Zero-Shot_Learning_from_Noisy_Texts_fig.png" alt="Image"></p><p>左上段がFakeデータを作るストリーム．左下段がRealデータを作るストリーム．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>Unseenクラスについてのノイズを含むテキスト記述を入力とし，このクラスのvisual featureを生成するGANを提案．テキストから生成されるvisual featureをFakeデータとし，真の画像から得られるvisual featureをRealデータとしてGANを学習．</p><ul><li>テキストのembedding後，FC層で次元圧縮をし，ノイズの影響を軽減．</li><li>生成された特徴のクラス間の識別性を保存するために, visual pivot regularizationを利用．Generatorの更新に利用．</li><li>Realデータとして真の画像からvisual feature得る際にはVGGを利用．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.01381">arXiv</a></li></ul></div></div><div class="slide_index">[#200]</div><div class="timestamp">2018.5.19 13:28:25</div></div></section><section id="MoCoGAN_Decomposing_Motion_and_Content_for_Video_Generation"><div class="paper-abstract"><div class="title">MoCoGAN: Decomposing Motion and Content for Video Generation</div><div class="info"><div class="authors">Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz</div><div class="conference">CVPR2018</div></div><div class="slide_editor"><a href="https://twitter.com/akmtn_twi">Naofumi Akimoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>教師不要でコンテンツとモーションという要素に分解し，ビデオを生成するGANを提案．コンテンツを固定しモーションのみ変化させることや，逆も可能．広範囲の実験を行い，量と質ともにSoTAであることを確認．人の服装とモーションの分離や，顔のアイデンティティーと表情の分離が可能であることを示している．</p><p>Contribution:・ノイズからビデオを生成する，条件なしでのビデオ生成GANの提案．
・従来手法では不可能である，コンテンツとモーションのコントロールが可能なこと
・従来のSoTA手法との比較</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/MoCoGAN_Decomposing_Motion_and_Content_for_Video_Generation_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>GAN．</li><li>ランダムベクトルのシーケンスをビデオフレームのシーケンスにマッピングするGenerator．ランダムベクトルの一部はコンテンツ，もう一部はモーションを指定するもの．</li><li>コンテンツの部分空間はガウス分布でモデル化．モーションの部分空間はRNNでモデル化．</li><li>Generatorは一つのフレーム分をベクトルからフレームにマップする働きだけなので，モーションを決めるのは連続するベクトルを生成するRNN部分となる．</li><li>1枚のフレームを入力とするDiscriminatorと連続した数フレームを入力とするDiscriminatorを使うGAN構造を新たに提案．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>ビデオはコンテンツとモーションに分けられるという前提（prior）からスタート</li><li><a href="https://arxiv.org/abs/1707.04993">arXiv</a></li></ul></div></div><div class="slide_index">[#201]</div><div class="timestamp">2018.5.19 13:08:06</div></div></section><section id="Finding_It_Weakly-Supervised_Reference-Aware_Visual_Grounding_in_Instructional_Videos"><div class="paper-abstract"><div class="title">Finding “It”: Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos</div><div class="info"><div class="authors">De-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg, Li Fei-Fei, Juan Carlos Niebles</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>言語的な文脈の中で指示語からそれが何であるかを特定する問題（Visual Grounding; 「それを取ってください」の「それ」を動画中から探索するなど）を扱う論文である。この問題に対してMIL（Multiple Instance Learning）を参考にした弱教師付き学習であるReference-aware MIL（RA-MIL）を用いて解決する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180518VisualGrounding.png" alt="180518VisualGrounding"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>画像に対するVisual Groundingが空間的な関係性を捉えるのに対して、Visual Groundingは時間的な関係性を捉える課題である。YouCookII/RoboWatch datasetにて処理を行った結果、弱教師付き学習であるRA-MILを適用するとVisual Groundingに対して精度向上することを明らかにした。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Language and Visionの課題はすでに動画にまで及んでいる。Visual Groundingのみならず、新規問題設定を試みた論文として精読してもよいかも？それと視覚と言語のサーベイ論文は読んでみたい</p><ul><li><a href="http://ai.stanford.edu/~dahuang/papers/cvpr18-ramil.pdf">論文</a></li><li><a href="http://ai.stanford.edu/~dahuang/">著者</a></li><li><a href="http://aclweb.org/anthology/D15-1021">視覚と言語のサーベイ論文</a></li></ul></div></div><div class="slide_index">[#202]</div><div class="timestamp">2018.5.18 16:30:52</div></div></section><section id="Practical_Block-wise_Neural_Network_Architecture_Generation"><div class="paper-abstract"><div class="title">Practical Block-wise Neural Network Architecture Generation</div><div class="info"><div class="authors">Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, Cheng-Lin Liu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ブロック単位でのアーキテクチャ生成手法であるBlockQNNを提案。Q学習（Q-Learning）を参考にして高精度なニューラルネットを探索的（ここではEpsilon-Greedy Exploration Strategyと呼称）に生成する。基本的には生成したブロックを積み上げることによりアーキテクチャを生成するが、早期棄却の枠組みも設けることで探索を効率化している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517BlockQNN.png" alt="180517BlockQNN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ブロック単位でニューラルネットのアーキテクチャを探索するBlockQNNを提案した。同枠組みはHand-craftedなアーキテクチャに近い精度を出しており（CIFAR-10のtop-1エラー率で3.54）、探索空間を削減（32GPUを3日間使用するのみ！）、さらに生成した構造はCIFARのみならずImageNetでも同様に高精度を出すことを明らかにした。ネットワーク構造の探索問題においてブロックに着目し、性能を向上させると同時に同様の枠組みを複数のデータセットにて成功させる枠組みを提案したことが、CVPRに採択された基準である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ここから数年で、practicalなGPU数（8GPUや4GPUなど）、1日以内の探索で解決するようになると予想される（し、してくれないと一般の研究者/企業が参入できない）。</p><ul><li><a href="https://arxiv.org/abs/1708.05552">論文</a></li></ul></div></div><div class="slide_index">[#203]</div><div class="timestamp">2018.5.17 13:12:12</div></div></section><section id="Residual_Dense_Network_for_Image_Super-Resolution"><div class="paper-abstract"><div class="title">Residual Dense Network for Image Super-Resolution</div><div class="info"><div class="authors">Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>低解像画像から高解像画像（SR; super-resolution image）を復元するための研究で、DenseNet（論文中の参考文献7）を参考にしたResidual Dense Networks (RDN)を提案して同課題にとりくんだ。異なる劣化特徴をとらえたモデルであること、連続的メモリ構造（Contiguous Memory Mechanism）やコネクションを効果的にするResidual Dense Blockを提案したこと、Global Feature Fusionにより各階層から総合的な特徴表現、を行い高解像画像を復元した。DenseNetで提案されているDense Blockと比較すると、提案のResidual Dense Blockは入力チャネルからもスキップコネクションが導入されているため、よりSRの問題設定に沿ったモデルになったと言える。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517ResidualDenseNetwork.png" alt="180517ResidualDenseNetwork"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>高解像画像を復元するための改善として、DenseNetを改良したRDNを提案した。Dense Blockを置き換え、より問題に特化したResidual Dense Blockを適用。実験で使用した全てのデータセット（Set5, Set14, B100, Urban100, Manga109）の全てのスケール（x2, x3, x4）にて従来手法よりも良好なAverage PSNR/SSIMを記録した。結果画像はGitHubのページなどを参照されたい。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>課題の肝をつかんで、従来提案されている効果的な手法を改善できるセンスを磨きたい。</p><ul><li><a href="https://arxiv.org/abs/1802.08797">論文</a></li><li><a href="https://github.com/yulunzhang/RDN">GitHub</a></li></ul></div></div><div class="slide_index">[#204]</div><div class="timestamp">2018.5.17 12:47:07</div></div></section><section id="Three_Dimension_Human_Pose_Estimation_in_the_Wild_by_Adversarial_Learning"><div class="paper-abstract"><div class="title">Three Dimension Human Pose Estimation in the Wild by Adversarial Learning</div><div class="info"><div class="authors">Wei Yang, Wanli Ouyang, Xiaolong Wang, Jimmy Ren, Hongsheng Li, Xiaogang Wang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>現在でもチャレンジングな課題として位置付けられる人物に対する3次元姿勢推定に関する研究で、Adversarial Learning (AL)を用いて学習を実施。問題設定としては「多量の」2次元姿勢アノテーション+「少量の」3次元姿勢アノテーションを使用することで、新規環境にて3次元姿勢推定を実行することである。本論文で提案するALではG（生成器）として、2D/3Dのデータセットからそれぞれ2D/3Dの姿勢を推定、実際のデータセットからアノテーションを参照（リアル）して、生成されたものか、データセットのアノテーションなのかを判断（D; 識別器）させることで学習する。G側の姿勢推定ではHourglassによるConv-Deconvモデルを採用、D側には3つの対象ドメイン（オリジナルDB、関節間の相対的位置、2D姿勢位置と距離情報）を入れ込んだMulti-Source Discriminatorを適用する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517PoseGAN.png" alt="180517PoseGAN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>GANに端を発する敵対的学習を用いて、3次元姿勢に関するアノテーションが少ない場合でもドメイン依存をすることなく3次元姿勢推定を可能にする技術を提案した。また、もう一つの新規性としてドメインに関する事前知識を識別器に入れ込んでおくmulti-source discriminatorについても提案した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>少量のラベル付きデータが用意できていれば、ドメイン関係なく推定ができるという好例である。データとアノテーションに関連するのはCG/敵対的学習/教師なし/ドメイン適応などで、これらは現在のCVにおいても重要技術。少なくともお金がないとクラウドソーシングでデータが集められないという構図を変えたいと思っている。</p><ul><li><a href="https://arxiv.org/abs/1803.09722">論文</a></li><li><a href="http://www.cs.cmu.edu/~xiaolonw/publication.html">著者</a></li></ul></div></div><div class="slide_index">[#205]</div><div class="timestamp">2018.5.17 12:03:18</div></div></section><section id="Gesture_Recognition_Focus_on_the_Hands"><div class="paper-abstract"><div class="title">Gesture Recognition: Focus on the Hands</div><div class="info"><div class="authors">Pradyumna Narayana, J. Ross Beveridge, Bruce A. Draper</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>手部領域に着目してチャネルを追加することにより、ジェスチャ認識自体の精度を高めていくという取り組み。従来型のマルチチャネル（rgb, depth, flow）のネットワークでは限定的な領域を評価して特徴評価を行なっていたが、提案のFOANetでは注目領域（global, right hand, left hand）に対して分割されたチャネルの特徴を用いて特徴評価を行い識別を実施する。図に示すアーキテクチャがFOANetである。FOANetでは12のチャネルを別々に処理・統合し、統合を行うネットワークを通り抜けて識別を実施する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517FOANet.png" alt="180517FOANet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>手部領域に着目し、よりよい特徴量として追加できないか検討した、とういアイディア自体が面白い。また、ChaLearn IsoGD datasetの精度を従来の67.71%から82.07まで引き上げたのと、同じようにNVIDIA datasetに対しても83.8%から91.28%に引き上げた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>あまりメジャーに使用されているDBではないが、重要課題を見つけてアプローチする研究は今後さらに必要になってくる？一番最初に問題を解いた人ではないが、二番目に研究をして実利用まで一気に近づけられる人も重宝される。</p><ul><li><a href="http://www.cs.colostate.edu/~draper/papers/narayana_cvpr18.pdf">論文</a></li></ul></div></div><div class="slide_index">[#206]</div><div class="timestamp">2018.5.17 11:20:46</div></div></section><section id="Direct_Shape_Regression_Networks_for_End-to-End_Face_Alignment"><div class="paper-abstract"><div class="title">Direct Shape Regression Networks for End-to-End Face Alignment</div><div class="info"><div class="authors">X. Miao, X. Zhen, V. Athitsos, X. Liu, C. Deng and H. Huang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>顔のアライメントにおいて，Direct shape regression networkを提案．いくつかの新しい構造を組み合わせている．(1)二重Conv，
(2)フーリエ特徴プーリング，
(3)線形低ランク学習．
顔画像-顔形状間の高い非線形関係性（初期化への強い依存性，ランドマーク相関導出の失敗）の問題を解決する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Direct_Shape_Regression_Networks_for_End-to-End_Face_Alignment_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>複数の新しい構造の定義</li><li>いくつかのケースでSoTAを超える性能．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://xinxinmiao.github.io/pdfs/1607.pdf">論文</a></li></ul></div></div><div class="slide_index">[#207]</div><div class="timestamp">2018.5.18 11:57:02</div></div></section><section id="Scale-recurrent_Network_for_Deep_Image_Deblurring"><div class="paper-abstract"><div class="title">Scale-recurrent Network for Deep Image Deblurring</div><div class="info"><div class="authors">X. Tao, H. Gao, Y. Wang, X. Shen, J. Wang, J. Jia</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>coarse-to-filneに単画像デブラーリングする，Scale-recurrent Network (SRN-DeblurNet)を提案．</p><p>構造的には，(1)入出力がピラミッド画像，
(2)中間はUnet，
(3)最終層の出力を第1層に注入（Recurrent）し，ピラミッド画像の枚数分実行．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Scale-recurrent_Network_for_Deep_Image_Deblurring_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>シンプルでパラメータ数が少ない．</li><li>SoTAを超える性能．例もすごいきれいになっているように見える．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>見た目明らかにきれいになっていると，やはり評価したくなる．</p><ul><li><a href="https://arxiv.org/abs/1802.01770">arXiv</a></li></ul></div></div><div class="slide_index">[#208]</div><div class="timestamp">2018.5.18 11:02:11</div></div></section><section id="Convolutional_Neural_Networks_with_Alternately_Updated_Clique"><div class="paper-abstract"><div class="title">Convolutional Neural Networks with Alternately Updated Clique </div><div class="info"><div class="authors">Yibo Yang et al., </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>従来のCNNの構造では基本的に決められた方向へのみのforwardを行うのに対して、すべてのレイヤー間で結合を持つClique blockで構成されるClique Netの提案。CIFAR-10でSoTA、その他ImangeNetやSVHNでも少ないパラメータでSoTAに匹敵する精度を記録。</p></div></div><div class="item2"><img src="slides/figs/Convolutional_Neural_Networks_with_Alternately_Updated_Clique.png" alt="Convolutional_Neural_Networks_with_Alternately_Updated_Clique.png"></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>Clique blockでは以下のような処理が行われる。</p><ul><li>畳み込み層によってすべての層を共通の特徴マップで初期化。</li><li>ある層に対して、他のすべての層から畳み込み結合した値で更新。これを各層に対して順次行い、すべての層で更新したら1つのStageが終了。</li><li>上記を決められたStage数行う。畳み込み結合の重みはStage間で共有する。</li></ul><p>DenseNetの拡張に近い構造のため妥当性があり、実際に精度が出ている点が強い。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.10419">論文</a></li></ul></div></div><div class="slide_index">[#209]</div></div></section><section id="Geometry_Guided_Convolutional_Neural_Networks_for_Self_Supervised_Video_Representation_Learning"><div class="paper-abstract"><div class="title">Geometry Guided Convolutional Neural Networks for Self-Supervised Video Representation Learning </div><div class="info"><div class="authors">Chuang Gan et al., </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>合成画像のペア間のフローと教師ラベルのない実画像のペア間のデプスを推定することによってシーン認識、行動認識のための表現学習を行う研究。フロー推定を行ったのち、デプス推定にfine-tuningし、さらに目的となるタスクにfine-tuningする。
直感的には、低レベルな特徴が獲得されそうだが、行動認識などの高次な問題設定でも効果を発揮した。</p></div></div><div class="item2"><img src="slides/figs/Geometry_Guided_Convolutional_Neural_Networks_for_Self_Supervised_Video_Representation_Learning.png" alt="Geometry_Guided_Convolutional_Neural_Networks_for_Self_Supervised_Video_Representation_Learning.png"></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>多段にfine-tuningするため、初期の問題設定によって獲得した特徴が失われてしまう可能性があるので、２段目のfine-tuning時にはfine-tuning前の出力結果への蒸留を同時に行う。ImageNetのpretrainingとも行動認識において補間的な関係がある。表現学習自体での使用データが少ないのに関わらず高い精度向上が実験的に示されたことが大きなcontributionだと考えられる。        </p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>特徴のforgetを防ぐ手法は、複数のタスクで学習済みモデルを作成する際に、その順番が重要となるような状況で有用だと思われる。既存手法との比較においては今回は+αのデータを利用している点はフェアではないと感じた。
また、目的のタスクへのfine-tuningの際のフレームペアの選び方などの詳細な設定が記されていなかった。主に精度評価のみで、高次なタスクでうまくいく考察が少なく、疑問もあった。</p><ul><li><a href="http://cseweb.ucsd.edu/~haosu/papers/cvpr18_geometry_predictive_learning.pdf">論文</a></li></ul></div></div><div class="slide_index">[#210]</div></div></section><section id="Learning_to_Compare_Relation_Network_for_Few-Shot_Learning"><div class="paper-abstract"><div class="title">Learning to Compare: Relation Network for Few-Shot Learning</div><div class="info"><div class="authors">F. Sung, Y. Yang, L. Zhang, T. Xiang, P.H.S. Torr, T.M. Hospedales</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>メタ学習を用いたFew-shot learningの新しい枠組み，Relation Networkの提案．一度学習されれば，ネットワークのアップデートの必要なしに新しいクラスの画像分類ができるようになる．</p><p>1エピソードにおける少数の画像の比較によって距離メトリックを学習するメタラーニングを行う．少数の新クラスの代表画像群とクエリ画像の関連性スコアの比較により，追加学習なしに新クラス画像分類が行える．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Compare_Relation_Network_for_Few-Shot_Learning_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>再学習しなくても，データさえ用意しておけば未知のクラスも分類可能な画像分類器ができる．</li><li>Zero-shot learningにも拡張可能．</li><li>シンプルで，高速に動作し，拡張性も高い．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>テスト時も少数のデータを用意しておけば，という考え方はイマドキ感がある．</p><ul><li><a href="https://arxiv.org/abs/1711.06025">arXiv</a></li><li><a href="https://github.com/lzrobots/LearningToCompare_ZSL">GitHub</a></li></ul></div></div><div class="slide_index">[#211]</div><div class="timestamp">2018.5.18 10:30:48</div></div></section><section id="MegaDepth_Learning_Single-View_Depth_Prediction_from_Internet_Photos"><div class="paper-abstract"><div class="title">MegaDepth: Learning Single-View Depth Prediction from Internet Photos</div><div class="info"><div class="authors">Z.Li and N.Snavely</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1804.00607</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>画像における深度予測はCV分野において基本的なタスクである．既存の手法は学習データによる制約が伴う．今回提案する手法では，インターネットの画像をデータセットとするMVSの手法を改良し，既存の3D reconstructionとsemantic ラベルを組みわせて大規模な深度予測モデルであるMegaDepthを提案．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/MegaDepth_Learning_Single-View_Depth_Prediction_from_Internet_Photos.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>セマンティックセグメンテーションを用いた順序による深度関係を自動で拡張</li><li>MegaDepthが強力なモデルであることを示すために膨大なインターネット画像を使い検証</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>深度予測にsemantic ラベルを取り入れることで精度が向上．</li><li>semanticラベルを用いており，複雑背景における物体検出にも応用可能かも！！</li><li><a href="https://arxiv.org/pdf/1803.01599.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#212]</div><div class="timestamp">2018.5.18 02:33:22</div></div></section><section id="Real-Time_Rotation-Invariant_Face_Detection_with_Progressive_Calibration_Networks"><div class="paper-abstract"><div class="title">Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks</div><div class="info"><div class="authors">FXuepeng Shi, Shiguang Shan, Meina Kan, Shuzhe Wu, Xilin Chen</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>リアルタイムで顔の回転に頑健な顔検出を行うProgressive Calibration Network(PCN)を提案。PCNは3つのステージで構成されており、それぞれのステージでは検出された領域を0° or 180°回転させる、
0° or 90° or -90°回転させる、頭が上にくるように顔を回転させる、という処理をそれぞれ行う。
また各ステージ共通で検出された領域が顔であるか顔でないかという識別を行う。第1,2ステージで粗く回転を行うことで第3ステージにおける回転量と、
各ステージにおける顔識別の学習が容易になったことで、高精度かつリアルタイムに顔検出を行うことが可能となった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Real-Time_Rotation-Invariant_Face_Detection_with_Progressive_Calibration_Networks.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来手法であるデータオーギュメンテーション、角度の値域を分割してそれぞれの検出器を学習させる方法、角度の回転角を推定する流手法では、どれもネットワークが大きくなりすぎるためにリアルタイムでの実行が難しかった。</li><li>解像度が40x40以上の顔を検出。</li><li>state-of-the-artの手法と比べて同等の精度を達成し、かつGPUを使用した際の実行スピードは4.2倍となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li> <a href="https://github.com/Jack-CV/PCN">GitHub with Demos</a></li><li><a href="https://arxiv.org/abs/1804.06039">論文</a></li></ul></div></div><div class="slide_index">[#213]</div></div></section><section id="Partially_Shared_Multi-Task_Convolutional_Neural_Network_with_Local_Constraint_for_Face_Attribute_Learning"><div class="paper-abstract"><div class="title">Partially Shared Multi-Task Convolutional Neural Network with Local Constraint for Face Attribute Learning</div><div class="info"><div class="authors">Jiajiong Cao, Yingming Li, Zhongfei Zhang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔のアトリビュート推定に有効なネットワークであるPS-MCNN/-LCを提案。従来手法のMCNNでは、類似度の高いアトリビュートの識別率を高めるために、
類似度の高いアトリビュートのごとにグループを形成し、MCNNの高い層では各グループごとにCNNを形成して学習を行なっていた。
そのため低い層で得られていた特徴量が消失するという問題が起きていた。
これを解決するために、MCNNに対して各レベルで得られた特徴量を教諭するShared Netを導入したPS-MCNNを提案。
また同一人物において推定されたアトリビュート同士のロスをとるPS-MCNN-LCも提案した。
ネットワークの構築に関する議論も行なっている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Partially_Shared_Multi-Task_Convolutional_Neural_Network_with_Local_Constraint_for_Face_Attribute_Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>同一人物において推定されたアトリビュート同士のロスをとることで、アトリビュートの空間を限定することが可能となるという考えのもとPS-MCNN-LCを提案している。</li><li>state-of-the-artに比べて、CelebAデータセットではPS-MCNN-LCが40種全てのアトリビュートにおいて最も高い精度を達成、LFWAデータセットではPS-MCNN/-LCを合わせて37種において最も高い精度を達成。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>精度が上がったことはもちろんだが、既存研究であるMCNNのリミテーションを正確に見抜いてネット枠を改善している点が採択につながったと考えられる。</li><li><a href="http://person.zju.edu.cn/attachments/2018-04/01-1524825782-717898.pdf">論文</a></li></ul></div></div><div class="slide_index">[#214]</div></div></section><section id="Deep_Semantic_Face_Deblurring"><div class="paper-abstract"><div class="title">Deep Semantic Face Deblurring</div><div class="info"><div class="authors">Ziyi Shen, Wei-Sheng Lai, Tingfa Xu, Jan Kautz, Ming-Hsuan Yang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔に対してセマンティックセグメンテーション(face sparsing)を利用することで、モーションブラーが加えられた正面顔画像に対するCNNベースのデブラーリング手法を提案。
face sparsingによって顔のパーツの位置関係や形といった情報を利用することができると主張。
また学習の際には様々なカーネルサイズによるブラー画像を同時に与えるのではなく、
小さなカーネルサイズのブラー画像から順々に学習させるincremental trainingことでデブラーリング精度を向上させた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Semantic_Face_Deblurring.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ブラー画像はランダムな3D cameraの軌道によって与えられ、カーネルサイズは13x13~27x27までを学習させた。</li><li>ロスとしてデブラーリング画像のL1 loss, face parsing画像のL1 loss, adversarial loss, CNNの特徴量マップのL2 ロスを使用。</li><li>tate-of-the-artに比べてデブラーリング画像とソース画像のPSNR、SSIM、顔の検出率、個人認証の精度においてもっとも良い精度を達成し、それぞれ約5%, 5%, 28%, 4%向上した。</li><li>state-of-the-artに比べて実行スピードが約44%向上した。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>学習データを少しずつ変化させて、順々に最適化を行うincremental trainingは、学習データをパラメトリックに変化可能な他の問題に対しても有用なトレーニング方法だと思われる。</li><li><a href="https://arxiv.org/abs/1803.03345">論文</a></li></ul></div></div><div class="slide_index">[#215]</div></div></section><section id="Learning_to_Adapt_Structured_Output_Space_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Learning to Adapt Structured Output Space for Semantic Segmentation</div><div class="info"><div class="authors">Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, Manmohan Chandraker</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Hiroaki Aizawa</div><div class="item1"><div class="text"><h1>概要</h1><p>Semantic Segmentationに関するDomain Adaptationの研究。Semantic Segmentationをsource domainとtarget domain間の空間的な類似性を持つ構造的な出力として考え、出力空間(prediction map)でのDomain Adaptationを行う敵対的学習手法を提案。低次特徴は利用せず、高次特徴のみを複数のDiscriminatorにより異なる空間解像度ごとに適応させる(Multi-level Adversarial Learning)。実験ではsynthetic-to-realとcross-cityでの比較を行っている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Adapt_Structured_Output_Space_for_Semantic_Segmentation.png" alt="Learning_to_Adapt_Structured_Output_Space_for_Semantic_Segmentation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>画像分類タスクを中心に発展していたDomain Adaptationを画素単位の構造予測が必要なSemantic Segmentationに適用した点。</p><p>Semantic Segmentationに限らず構造予測をするタスクへも容易に拡張ができる。</p><p>画像分類と比較して、アノテーションの労力がかかるため実用性・将来性がある。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.10349">arxiv</a></li><li><a href="https://github.com/wasidennis/AdaptSegNet">github</a></li></ul></div></div><div class="slide_index">[#216]</div><div class="timestamp">2018.5.17 20:28:45</div></div></section><section id="Multi_Task_Learning_Using_Uncertainty_to_Weigh_Losses_for_Scene_Geometry_and_Semantics"><div class="paper-abstract"><div class="title">Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</div><div class="info"><div class="authors">Alex Kendall et al., </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>学習時のタスクごとの重みによって精度がかなり変化する。そこでNNのマルチタスクモデルにおいて各出力を分布表現にし、その同時確率を最尤推定するように学習することで結果的にタスクごとの不確実性を考慮した重み付けを損失関数に課す。実験ではSemantic Segmentation, Instance Segmentation, Depth estimationのマルチタスク学習を行い、等しい重みや手動での重み設計時よりも良い結果となった。</p></div></div><div class="item2"><img src="slides/figs/Multi_Task_Learning_Using_Uncertainty_to_Weigh_Losses_for_Scene_Geometry_and_Semantics.png" alt="Multi_Task_Learning_Using_Uncertainty_to_Weigh_Losses_for_Scene_Geometry_and_Semantics.png"></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>モデルから各タスクに対して不確実性を表す値を同時に出力させる。回帰タスクの場合はこれが分散を表し、最終的には回帰出力値を平均とするガウス分布として表現する。識別タスクについては不確実性が分布の温度パラメータとして扱われる。これらの同時確率を最尤推定すると、通常の損失に対してタスクごとに適応的に重み付けされた損失を最適化していることになる。理論的にも妥当であり、精度向上は大きくチューニングの手間が省けるという点でかなり便利である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>簡単な実装でハイパーパラメータが減るという点でかなり有用に感じた。様々なマルチタスクで行った訳ではないのでこの手法の汎用性がきになる。結局、識別の場合は通常でも不確実性は考慮しているので、本質的に新しいのは回帰の場合である。</p><ul><li><a href="https://arxiv.org/abs/1705.07115">論文</a></li></ul></div></div><div class="slide_index">[#217]</div></div></section><section id="Compare_and_Contrast_Learning_Prominent_Visual_Differences"><div class="paper-abstract"><div class="title">Compare and Contrast: Learning Prominent Visual Differences</div><div class="info"><div class="authors">S.Chen and K.Grauman</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1804.00112</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>2つの画像間で最も顕著な違いは表せられるがその他の細かい違いは示されないことが多い．それに対して，より多くの違いによって画像を比較できるようなモデルの構築をした．また，そのモデルを使って，UT-Zap50K shoesとthe LFW10のデータセットを用いて評価したところSoTAであった．構築したモデルを画像記述と画像検索に導入し，拡張を図った．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Compare_and_Contrast_Learning_Prominent_Visual_Differences.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>画像中から目立つ部分をアノーテーションで収集し，ランク付けすることでモデルの構築．</li><li>UT-Zap50K shoes（靴）とthe LFW10（顔）のデータセットを用いて評価．</li><li>画像記述と画像検索のタスクに応用し，拡張を図る</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>画像説明文に応用できればキャプショニングの幅を広げられそう．</p><ul><li><a href="https://arxiv.org/pdf/1804.00112.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#218]</div><div class="timestamp">2018.5.17 16:18:51</div></div></section><section id="Learning_Rich_Features_for_Image_Manipulation_Detection"><div class="paper-abstract"><div class="title">Learning Rich Features for Image Manipulation Detection</div><div class="info"><div class="authors">P. Zhou, X. Han, V.I. Morariu and L.S. Davis</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>画像修正検出．修正箇所をちゃんと注目すべきで，リッチな特徴の学習が必要．修正後画像から修正領域を検出するtwo-stream Faster R-CNNを提案．
RGB stream：コントラスト差，不自然境界とかを捉える．Noise stream：ノイズの非一貫性を捉える．<a href="https://ieeexplore.ieee.org/document/6197267/">Steganalysis Rich Model</a>でとれたノイズ特徴に基づく．
そして，両者のバイリニアプーリングで共起性を捉える．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Rich_Features_for_Image_Manipulation_Detection.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>修正箇所のノイズ感の差を見るアイデアは昔にあったが，それを導入したという温故知新．</li><li>実験によりリサイズや圧縮に対するロバスト性におけるSOTAを確認．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1805.04953">arXiv</a></li><li><a href="http://www.cosy.sbg.ac.at/~uhl/mmsec/LukFriSPIE06_v9.pdf">過去のノイズ感の差を使った画像加工領域検出の例</a></li></ul></div></div><div class="slide_index">[#219]</div><div class="timestamp">2018.5.17 15:16:28</div></div></section><section id="Real-Time_Seamless_Single_Shot_6D_Object_Pose_Prediction"><div class="paper-abstract"><div class="title">Real-Time Seamless Single Shot 6D Object Pose Prediction</div><div class="info"><div class="authors">Bugra Tekin et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>1枚のRGB画像から物体の6次元姿勢を推定する研究. CNN を用いた単一のネットワーク (YOLO v2 ベース) で RGB 画像から物体の 3D bounding box を直接推定する. post-process 無しで高精度な姿勢推定が可能なため, 実時間（従来手法の約５倍速）で従来手法と同程度の推定精度を達成した. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Real-Time-Seamless-Single-Shot-6D-Object-Pose-Prediction.png" alt="fukuhara-Real-Time-Seamless-Single-Shot-6D-Object-Pose-Prediction.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ネットワークはRGB画像1枚の入力に対して, 各物体の制御点（3D bounding box 8点 と centroid 1点）の位置, カテゴリー, 推定の確信度を出力する.</li><li>推定された物体の9つの制御点の位置に対して PnP 問題を解くことで6次元姿勢を推定する.</li><li>物体の bounding box の情報から学習を行うので物体の詳細な3次元モデルが必要無い. また, テクスチャーが殆ど無い物体に対しても適用が可能.</li><li>物体が複数あった場合でも PnP 以外の部分の計算量は増えないので, 物体数に関わらず計算時間はほぼ一定.(従来手法の SSD-6D は線型に増加.)</li><li>LINEMOD や OCCLUSION データセットを用いた評価実験では従来手法 (BB8 や SSD-6D) と同等かそれ以上の精度を 50fps (SSD-6Dの約５倍) で達成.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.08848" target="blank">[論文] Real-Time Seamless Single Shot 6D Object Pose Prediction</a></li><li><a href="https://btekin.github.io/" target="blank">[著者HP] Bugra Tekin</a></li></ul></div></div><div class="slide_index">[#220]</div><div class="timestamp">2018.5.17 12:19:55</div></div></section><section id="Video_Captioning_via_Hierarchical_Reinforcement_Learning"><div class="paper-abstract"><div class="title">Video Captioning via Hierarchical Reinforcement Learning</div><div class="info"><div class="authors">Xin Wang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Video captioning のための階層型強化学習フレームワークを提案. Caption を複数のセグメントに分割し, High-level の Manager Module が各セグメントのコンテキストをデザインし, Low-level の Worker Modeule が単語を生成することで順次セグメントを作成する. 提案手法は MSR-VTT データセット を用いた評価実験で既存手法よりも複数の評価尺度で良い結果となった. また, video captioning のための新しい大規模データセットを公開. </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Video-Captioning-via-Hierarchical Reinforcement-Learning.png" alt="fukuhara-Video-Captioning-via-Hierarchical Reinforcement-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Video captioning の問題を強化学習の問題として定式化し直し, 効率的に学習をすることができる階層型強化学習手法を提案した.</li><li>High-level の Manager Module が目標を達成するために必要なゴールを設定し, Low-level の Worker Modeule がゴールを達成するための基本行動を行う. また, Internal Critic がゴールが達成されたかどうかの評価を行う.</li><li>Action recognition や segmentation で主に用いられている Charades データセットをもとにvideo captioning のための新しい大規模データセットを作成. 既存の MSR-VTT データセットよりも詳細で長い caption が与えられている.</li><li>MSR-VTT データセットを用いた評価実験では, 既存手法（Mean-Pooling, Soft-Attention, S2VT等）と比較して複数の評価尺度で最も良い結果を得た.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.11135" target="blank">[論文] Video Captioning via Hierarchical Reinforcement Learning</a></li><li><a href="http://www.cs.ucsb.edu/~xwang/#" target="blank">[著者HP] Xin Wang</a></li></ul></div></div><div class="slide_index">[#221]</div><div class="timestamp">2018.5.17 12:11:55</div></div></section><section id="Multi-view_Consistency_as_Supervisory_Signal_for_Learning_Shape_and_Pose_Prediction"><div class="paper-abstract"><div class="title">Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction</div><div class="info"><div class="authors">Shubham Tulsiani et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>１枚のRGB画像から物体の形状とカメラ姿勢の両方を推定する研究. 異なる視点から見たときの一貫性(具体的には物体の輪郭または深度情報の一貫性)を教師情報として用いるため, 従来手法と異なり学習時に物体の３次元形状と姿勢のいずれについても直接の教師データも必要としない.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Multi-view-Consistency-as-Supervisory-Signal-for-Learning-Shape-and-Pose-Prediction.png" alt="fukuhara-Multi-view-Consistency-as-Supervisory-Signal-for-Learning-Shape-and-Pose-Prediction.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>物体の形状とカメラ姿勢の両方を推定するタスクに置いて, 直接の教師データを用いずに学習する方法を提案した.</li><li>学習時の入力は同一の物体を異なる位置から撮影したRGB画像２枚と２枚目の画像の物体の Mask または Depth 画像.</li><li>１枚目の画像から３次元形状, ２枚目の画像からカメラ姿勢をそれぞれ推定し, 推定された形状を推定された姿勢から見た時に, 与えられたマスク画像と同じ結果が得られるように学習を行う.</li><li>ShapeNet データセットを用いた評価実験では, 直接の教師あり学習を行った手法とほぼ同等の結果であった.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1801.03910" target="blank">[論文] Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction</a></li><li><a href="https://shubhtuls.github.io/mvcSnP/" target="blank">[Project Page]</a></li><li><a href="https://github.com/shubhtuls/mvcSnP" target="blank">[Code]</a></li></ul></div></div><div class="slide_index">[#222]</div><div class="timestamp">2018.5.12 13:05:55</div></div></section><section id="PAD_Net_Multi-Tasks_Guided_Prediction_and_Distillation_Network_for_Simultaneous_Depth_Estimation_and_Scene_Parsing"><div class="paper-abstract"><div class="title">PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing</div><div class="info"><div class="authors">Dan Xu et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"></div><h1>概要</h1><p>CNNに対して中間的に法線方向推定と輪郭推定も加えることで最終的にdepth推定とscene parsingの精度を向上させる。法線方向と輪郭についてはdepthとscene parsingのラベルから計算可能であるので追加にアノテーションする必要はない。
NYUD-v2とCityscapesにおいてSoTA。  </p></div><div class="item2"><img src="slides/figs/PAD_Net_Multi-Tasks_Guided_Prediction_and_Distillation_Network_for_Simultaneous_Depth_Estimation_and_Scene_Parsing.png" alt="PAD_Net_Multi-Tasks_Guided_Prediction_and_Distillation_Network_for_Simultaneous_Depth_Estimation_and_Scene_Parsing.png"></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>中間的に推定した結果を元に最終的な目的タスクを出力するが、その中間出力として3つのパターンを考えた(タスクをに分けずconcat, タスクごとにconcat, attention機構を取り入れたconcat)。 attention機構を取り入れたconcatが最も良い結果となった。シンプルな手法だが、実験結果が良いので評価されたと考えられる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>「distillation」という言葉を用いているが、生徒モデルと教師モデルがあるようなdistillation手法は使われておらず、単に複数の中間タスクからのMulti-modalな情報の統合に対してその言葉が使用されている。
単に通常のマルチタスク推定に中間タスクを導入したのみでかなりシンプルな印象。</p><ul><li><a href="https://scirate.com/arxiv/1805.04409">論文</a></li></ul></div></div><div class="slide_index">[#223]</div></div></section><section id="Convolutional_Sequence_to_Sequence_Model_for_Human_Dynamics"><div class="paper-abstract"><div class="title">Convolutional Sequence to Sequence Model for Human Dynamics</div><div class="info"><div class="authors">Chen Li, Zhen Zhang, Wee Sun Lee, Gim Hee Lee</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>時空間的な特徴を捉えて、長期のモーション予測を行う研究である（ここではいかに最初の限られた情報量のみでシーケンスを推定できるかどうかについて検証を行なっている）。この課題に対し、Convolutional Long-term Encoderを用いてより長期的な隠れ変数をデコーダにより推定する。このエンコーダ-デコーダ構造にて短期〜より長期的な変数の予測を可能にする。本手法では主にRNNベースのSequence-to-SequenceなモデルにConvolutionalな要素を加えたことが技術的発展であると主張。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180517ConvSeq2Seq.png" alt="180517ConvSeq2Seq"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>より長期の（といっても数秒間のシーケンス？）人物モーション予測（ここでは人物姿勢位置を予測）を実現したことが課題設定として大きい。手法としてはConvolutional Long-term Encoderやその抽象化された特徴をデコーダにより長期隠れ変数を推定。Human3.6MやCMU Motion Capture datasetにて高い精度を実現した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Short-termからLong-term（Short-term: 〜3秒、Long-term: 5秒〜; 明確な定義はなされていないが。。）の行動/姿勢の予測はまだまだ未解決だし、何を予測するかに関しての定義づけ自体の整備も曖昧なままである。まだまだ参入の余地が残されているように見える。</p><ul><li><a href="https://arxiv.org/abs/1805.00655">論文</a></li><li><a href="https://github.com/chaneyddtt/Convolutional-Sequence-to-Sequence-Model-for-Human-Dynamics">GitHub</a></li></ul></div></div><div class="slide_index">[#224]</div><div class="timestamp">2018.5.17 01:05:51</div></div></section><section id="LSTM_Pose_Machines"><div class="paper-abstract"><div class="title">LSTM Pose Machines</div><div class="info"><div class="authors">Yue Luo, Jimmy Ren, Zhouxia Wang, Wenxiu Sun, Jinshan Pan, Jianbo Liu, Jiahao Pang, Liang Lin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Convolutional Pose Machine (CPM)のCNN部分を再帰的ネットであるLSTM (Long-short term memory)により置き換えた人物姿勢推定手法。時系列的に連続するフレーム（e.g. t, t+1, t+2）の入力に対して処理を実行し姿勢を推定する。CPMとは基本となるアーキテクチャの考え方（multi-stage algorithm）は同様であるが、それぞれのステージ間でパラメータを共有している点で異なる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180516LSTMPoseMachine.png" alt="180516LSTMPoseMachine"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>CPMと同じmulti-stageの姿勢推定学習を、LSTMの構造にて実現したことが技術的なポイントである。さらに、CPMとは異なりステージ間でパラメータを共有することで精度向上が見られたと説明。Penn Action datasetやJHMDB datasetにて最高精度を叩き出した。JHMDBにて93.6@PCK(=0.2)、Penn Actionにて97.7@PCK(=0.2)を記録。さらに、各フレーム時のメモリチャンネルの挙動も可視化し、どのような際に成功するか/失敗するかを明らかにした。複雑姿勢（複雑背景？）の際にはエッジに着目していて、姿勢推定が成功する際にはピンポイントで関節位置を回帰する傾向にある。処理速度の面においても本論文の技術では25.6msで動作した（CPMは48.4ms）。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>アーキテクチャ自体上手くいったモデルを異なるアプローチ（この場合はConvolutionalなモデルをRecurrentに変更）で実行して、どういった改善ができるかを試しているところが面白い。LSTMの場合にはステージ間でパラメータを共有するところがポイントであった。その上で精度を向上している点が実行力に優れていると言える。</p><ul><li><a href="https://arxiv.org/abs/1712.06316">論文</a></li><li><a href="http://www.jimmyren.com/">著者</a></li><li><a href="https://github.com/lawy623/LSTM_Pose_Machines">GitHub</a></li><li><a href="https://www.youtube.com/watch?v=-sP3LWl6Ul0">YouTube</a></li></ul></div></div><div class="slide_index">[#225]</div><div class="timestamp">2018.5.16 17:25:49</div></div></section><section id="DecideNet_Counting_Varying_Density_Crowds_Through_Attention_Guided_Detection_and_Density_Estimation"><div class="paper-abstract"><div class="title">DecideNet: Counting Varying Density Crowds Through Attention Guided Detection and Density Estimation</div><div class="info"><div class="authors">Jiang Liu, Chenqiang Gao, Deyu Meng, Alexander G. Hauptmann</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>混雑時の人数カウントにおける問題点を解決するため、End-to-Endで学習可能なDecideNet（DEteCtIon and Density Estimation Network）を提案する。混雑時の人数カウントでは、従来（１）人物検出では認識ミスによる過不足によりカウントを誤ってしまう、（２）回帰ベースの手法では人物が存在しない領域が蓄積されると実際のカウントよりも多く集計されてしまう、という問題が存在した。DecideNetでは検出ベース/回帰ベースを別々に行い、それらの結果を総合してカウントを行うという点で従来法を解決していると言える。実験では本論文で提案のDecideNetが混雑時の人数カウントにおいてもっとも優れた精度を達成したと主張。検出/回帰の手法としてはFaster R-CNN/RegNetを適用している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180516DecideNet.png" alt="180516DecideNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>3つのベンチマーク（Mall, ShanghaiTech PartB, WorldExpo10 dataset）においてState-of-the-artな精度を達成すると同時に、混雑時の人数カウントの問題と異なるアプローチを同時実行して相補的なアプローチDecideNetを提案したことが採択された大きな理由である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>異なる複数のアプローチを統合して最高精度を達成するためには、その分野における積み重ねと実装力が必要である。論文の書き方と合わせて鍛えていくことで毎回難関国際会議に突破できる力がつくと思われる。</p><ul><li><a href="https://arxiv.org/pdf/1712.06679.pdf">論文</a></li><li><a href="www">Project</a></li><li><a href="www">GitHub</a></li></ul></div></div><div class="slide_index">[#226]</div><div class="timestamp">2018.5.16 11:59:31</div></div></section><section id="Cascaded_Pyramid_Network_for_Multi-Person_Pose_Estimation"><div class="paper-abstract"><div class="title">Cascaded Pyramid Network for Multi-Person Pose Estimation</div><div class="info"><div class="authors">Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu and J. Sun</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>複数人ポーズ推定には，キーポイントの半／全遮蔽や，複雑な背景といった要素(hard keypoints)が問題になる．Cascaded Pyramid Networkを提案．
hard keypointに対応するためのもの．2つの構造からなる．</p><ul><li>GlobalNet<br>ピラミッド構造をしていて，遮蔽などの無いシンプルなキーポイントの検出として作用する．この時点ではhard性にはあまり対応していない．</li><li>RefineNet<br>hard keypointを考慮した層．
GlobalNetのピラミッドな特徴を拾って，ResNetのBottleneckにかける．
ここで，何もしないとシンプルキーポイントだけ見てしまうので，損失関数の計算時，online hard keypoints miningする．
テスト時のロスを参考にオンラインでhard keypointを選択，選んだキーポイントのものだけバックプロパゲーションにまわすという作業．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Cascaded_Pyramid_Network_for_Multi-Person_Pose_Estimation_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>新規ネットワーク構造の提案</li><li>MS COCO keypoint benchmarkにてSOTA</li><li>実験を結構頑張っている様子．online hard keypoint miningの有無に関する議論などある．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>online hard keypoint miningについて実装可能なレベルでは詳しく書いてなかった．コード読めということか．</p><ul><li><a href="https://arxiv.org/abs/1711.07319">arXiv</a></li><li><a href="https://github.com/chenyilun95/tf-cpn">GitHub</a></li></ul></div></div><div class="slide_index">[#227]</div><div class="timestamp">2018.5.16 18:24:47</div></div></section><section id="One-shot_Action_Localization_by_Learning_Sequence_Matching_Network"><div class="paper-abstract"><div class="title">One-shot Action Localization by Learning Sequence Matching Network</div><div class="info"><div class="authors">H. Yang et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">ある長い動画中から指定した対象動画と同じActionを探してくるOne-shot Action Localizationの研究．
Matching Networkという手法がベースになっていて，それを動画のAction Localizationに応用．
基本的には動画をEncoding (Video Encoder) して，
類似度を計算 (Similarity Network) して，ラベリング (Labeling Network)．
長い方の動画はSliding Windowで分割 (Proposals) して，Proposalsと指定動画の間で類似度を計算．
Encoderは動画でよくやられるTwo-stream CNNとLSTMを利用．
学習はMeta Learningの形式で定式化され，End-to-Endで学習可能．
</div></div><div class="item2"><img src="slides/figs/One-shot_Action_Localization_by_Learning_Sequence_Matching_Network.png" alt="One-shot_Action_Localization_by_Learning_Sequence_Matching_Network.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>Deep時代になってからほとんどやられていなかったOne-shot Action Localization (Action search)</li><li>ProposalsのEncoding，類似度計算，ラベリングと3つすべてが微分可能でEnd-to-Endで学習可能</li><li>普通のTemporal Action LocalizationのSOTA手法よりもOne-shotの設定では高い性能を実現</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="http://www.porikli.com/mysite/pdfs/porikli%202018%20-%20One-shot%20action%20localization%20by%20learning%20sequence%20matching%20network.pdf">論文（著者ページ）</a></li><li>やっている事自体は至って普通のアプローチに感じる</li><li>End-to-End, Meta Learningと今風の形で実現できているのが評価されているのかな</li></ul></div></div><div class="slide_index">[#228]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="Exploit_the_Unknown_Gradually_One-Shot_Video-Based_Person_Re-Identification_by_Stepwise_Learning"><div class="paper-abstract"><div class="title">Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning</div><div class="info"><div class="authors">Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wanli Ouyang, Yi Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ワンショット学習（One-shot Learning）により動画像における人物再同定（person re-identification）を実行する論文。ラベルなしのtracklets（人物から抽出した動線）が容易かつ事前に手に入ることから、このtrackletsを徐々に改善しつつ人物同定率を高めるようにCNNを学習していく手法を提案する。本論文での学習では、最初にひとつのラベルを用いて初期化したあと、（１）信頼度の高い少量のサンプル（簡単なサンプル）に対して擬似ラベルを付与、（２）擬似ラベルを含めたラベルを元にカテゴリを更新してより難しいサンプルも取り込む、を繰り返して学習を行う。実験的に擬似ラベルを選択する方法についても議論している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180516OneShotREID.png" alt="180516OneShotREID"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>正解ラベルが付与されたある画像一枚を準備するだけで擬似ラベルを推定して徐々に学習を進めていくワンショット学習を提案した。人物再同定の問題においては有効な解決策であることを示したことがCVPRに採択された基準である。ワンショット学習によりrank-1の精度が21.46@MARS dataset、16.53@DukeMTMC-VideoReID datasetであり、コードも公開されている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>ワンショットのラベルと信頼できる擬似ラベルから徐々に概念を獲得するのはうまいやり方。あらゆる枠組みで用いることができそう。</p><ul><li><a href="https://yu-wu.net/pdf/CVPR2018_Exploit-Unknown-Gradually.pdf">論文</a></li><li><a href="http://xuanyidong.com/publication/cvpr-2018-eug/">Project</a></li><li><a href="https://github.com/Yu-Wu/Exploit-Unknown-Gradually">GitHub</a></li><li><a href="https://yu-wu.net/">著者</a></li></ul></div></div><div class="slide_index">[#229]</div><div class="timestamp">2018.5.16 11:18:38</div></div></section><section id="PoseTrack_A_Benchmark_for_Human_Pose_Estimation_and_Tracking"><div class="paper-abstract"><div class="title">PoseTrack: A Benchmark for Human Pose Estimation and Tracking</div><div class="info"><div class="authors">Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, Bernt Schiele</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>動画シーケンスにおいて2D姿勢推定のベンチマークを提供する。本論文で提案するベンチマークでは特に、人物の重なりを含む混雑シーン、密なアノテーションを提供する。さらに右の画像で示すようにドメイン依存していない多様な（diverse）シーンを捉えつつ姿勢アノテーション数でも有数、1画像に対する複数人物/ビデオに対するラベルづけにも対応している。トータルでは23,000画像に対して153,615人の姿勢アノテーションを行なった。チャレンジとしては単一フレームに対する姿勢推定（single-frame pose estimation）、ビデオに対する姿勢推定（pose estimation in videos）、姿勢トラッキング（pose tracking）を提供し、評価用サーバも提供する。同DBに対するベンチマーキングではOpenPoseにも導入されているPAFを改良したML-LAB（引用52）がトップ（70.3@mAP）、Mask R-CNNをベースにしたProTracker（引用11）は64.1@mAPであった。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180515PoseTrackBenchmark.png" alt="180515PoseTrackBenchmark"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>大規模かつ静止画ではなく動画に対する人物姿勢データセットを構築し、さらには評価サーバを提供、さらに最先端手法に関するベンチマーキングを行なっていることが新規性およびCVPRに通った理由であると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>データセットの比較図に多様なドメインから収集（diverse）と書かれているが、これらをすべて統合すると相当な量のデータになるのでは？（だれかやってそう）もしくはドメインを合わせれば学習の効果がありそう。</p><ul><li><a href="https://arxiv.org/abs/1710.10000">論文</a></li><li><a href="www.posetrack.net">Project</a></li><li><a href="https://scholar.google.com/citations?hl=ja&amp;user=6rl-XhwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">著者</a></li></ul></div></div><div class="slide_index">[#230]</div><div class="timestamp">2018.5.15 12:16:20</div></div></section><section id="Camera_Style_Adaptation_for_Person_Re-identification"><div class="paper-abstract"><div class="title">Camera Style Adaptation for Person Re-identification</div><div class="info"><div class="authors">Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li, Yi Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>Person Re-ID（人物再同定）は異なるカメラ間で同一人物を対応づける問題設定であり、画像の質や形式が異なるため非常に困難である。本論文ではカメラ間のスタイル変換を行うことでカメラに依存せず安定して認識できる特徴抽出（camera-invariant descriptor subspace）を行い、人物再同定の問題を高度に解決することを目的とする。この問題に対してCycleGANを適用することでカメラ間の特徴変換を捉えた上で、データ拡張を行う。存在するノイズへの対策として有効と思われる正則化:Label Smooth Regularization (LSR)を適用する。LSRを使用する場合では学習データに対するオーバーフィッティングが見られず、有効な手法であることが判明した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180515CamStyleTransferREID.png" alt="180515CamStyleTransferREID"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>CycleGANによるカメラ間のスタイル変換を実現してデータ拡張、LSRによりノイズへの対応を行いオーバーフィッティングを回避していることが新規性である。また、人物再同定においてその高い精度（Market-1501のrank-1にて89.49%、DukeMTMC-reIDのrank-1にて78.32%）を実現している。さらに、LSRを用いることでベースラインからの精度向上が見られる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>CycleGANが学習データを増やすという意味でも取り上げられている。例えば東大井上氏の論文もCycleGANを用いてデータ拡張を行い、スタイルの異なる画像に変換することでデータアノテーションの労力を削減している。</p><ul><li><a href="https://arxiv.org/abs/1711.10295">論文</a></li><li><a href="https://github.com/zhunzhong07/CamStyle">Project</a></li><li><a href="http://zhunzhong.site/">著者</a></li><li><a href="https://github.com/junyanz/CycleGAN">CycleGAN</a></li><li><a href="https://naoto0804.github.io/cross_domain_detection/">東大井上氏論文</a></li></ul></div></div><div class="slide_index">[#231]</div><div class="timestamp">2018.5.15 11:50:25</div></div></section><section id="Dense_3D_Regression_for_Hand_Pose_Estimation"><div class="paper-abstract"><div class="title">Dense 3D Regression for Hand Pose Estimation</div><div class="info"><div class="authors">Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>単眼距離画像から簡易的かつ効果的に3次元手部姿勢推定を実施する技術について提案する。従来の3D手部姿勢回帰の手法と比較して、本論文ではピクセルごとの（pixel-wise）解析を可能とする。手法としては2D/3Dの関節点を返却するカスケード型の多タスクネットワーク（multi-task network cascades）を提案し、End-to-Endでの学習を行う。その後MeanShiftによりピクセルごとの姿勢位置を推定する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180515DenseHandReg.png" alt="180515DenseHandReg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来のほとんどの手法では関節レベルの手部姿勢推定であったのに対して、本論文で提供する技術はピクセルベースの3D手部姿勢推定であることが新規性である。ピクセルごとの回帰はノンパラメトリックな手法を構築した。MSRA/NYU hand datasetにてすべての従来手法よりも高い精度で手部姿勢推定を実行した。また、ICVL hand datasetでは（頭打ちになっていると思われる）論文5には及ばなかったが、接近した精度を叩き出すことに成功した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>HandTrackingも激戦であるが、本論文ではピクセルベースの回帰とState-of-the-art（最高精度）という強みを活かして論文を通している。</p><ul><li><a href="https://arxiv.org/abs/1711.08996">論文</a></li><li><a href="https://github.com/melonwan/denseReg">GitHub</a></li></ul></div></div><div class="slide_index">[#232]</div><div class="timestamp">2018.5.15 11:20:15</div></div></section><section id="Disentangling_Features_in_3D_Face_Shapes_for_Joint_Face_Reconstruction_and_Recognition"><div class="paper-abstract"><div class="title">Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition</div><div class="info"><div class="authors">Feng Liu, Ronghang Zhu, Dan Zeng, Qijun Zhao, Xiaoming Liu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔画像からshapeの三次元復元を行う際に、画像から個人性(顔の形など)を反映した3Dモデルと、個人性以外(表情など)を反映した3Dモデルをencoderで別々に生成しdecoderで三次元復元を行う手法を提案。
生成された顔のshapeは三次元復元におけるstate-of-the-artよりも高い精度を達成し、
また生成されたshapeによる顔認証においても多くの既存手法より高い精度を達成した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Disentangling_Features_in_3D_Face_Shapes_for_Joint_Face_Reconstruction_and_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来の三次元復元の手法では顔のディティールは再現するものの、アラインメントなどの個人性の再現が完全ではなかった。提案手法では個人性を反映したモデルとそうでないモデルを分離して学習させることで、この問題を解決した。</li><li>様々なデータセットにおいて、生成された顔の3D shapeはstate-of-the-artに比べて最も低いaccuracyを達成。</li><li>生成された3D shapeにおけるランドマークなどのaccuracyにおいてももっとも低い値を獲得。</li><li>生成された3D shapeによる個人認証においても、多くの既存手法よリも高い精度となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>disentangleのファクターとして個人性を選んだのはあくまで人間であって、今後の発展ではもっと優秀なファクターを深層学習が導き出してくれるかもしれない。</li><li><a href="https://arxiv.org/abs/1803.11366">論文</a></li></ul></div></div><div class="slide_index">[#233]</div></div></section><section id="Seeing_Small_Faces_from_Robus_Anchors_Perspective"><div class="paper-abstract"><div class="title">Seeing Small Faces from Robust Anchor’s Perspective</div><div class="info"><div class="authors">Chenchen Zhu, Ran Tao, Khoa Luu, Marios Savvides</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>アンカーベースで画像中の小さな顔に対する検出精度を向上させる手法を提案。アンカーベースの手法では画像中に等間隔で並べられた点(アンカー)を中心とした矩形によって物体を検出する。
アンカーによる検出精度を評価する数値としてExpected Max Overlapping(EMO) scoreを提案し、
EMOを深層学習に学習させることで、小さな顔(16X16)に対する検出精度を向上した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Seeing_Small_Faces_from_Robus_Anchors_Perspective.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来のアンカーベースの手法ではIoUを学習させていたため、解像度が16x16などの小物体に対する学習が困難であったが、EOM scoreを学習させることで小物体の検出精度が大きく向上。</li><li>従来のアンカーベースの手法よりも検出精度が向上、特に小さな顔に対する検出精度が大きく向上したが、実行時におけるスピードは従来手法と同程度。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.09058">論文</a></li></ul></div></div><div class="slide_index">[#234]</div></div></section><section id="Exploring_Disentangled_Feature_Representation_Beyond_Face_Identification"><div class="paper-abstract"><div class="title">Exploring Disentangled Feature Representation Beyond Face Identification</div><div class="info"><div class="authors">Yu Liu, Fangyin Wei, Jing Shao, Lu Sheng, Junjie Yan, Xiaogang Wang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔に関するタスクに汎用的な特徴量を得ることができるDistilling and Dispelling Autoencoder(D2AE)を提案。Encoderによって顔から個人性を表現する特徴量(性別など)と個人性を排除した特徴量(表情など)を抽出する。
取得された特徴量により、個人識別、アトリビュートの識別、顔のアトリビュート編集、顔の生成を行うことができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Exploring_Disentangled_Feature_Representation_Beyond_Face_Identification.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Encoderによって顔から個人性を表現する特徴量と個人性を排除した特徴量を抽出することで、これらの特徴量により様々な顔のタスクを行うことが可能となった。</li><li>LFWデータセットにおける個人識別でaccuracyが約99.0%、TPRが約98.0%であり、既存手法と同等の精度を達成。</li><li>LFWA、CelebAデータセットにおける顔のアトリビュート認識は83.16%となり、アトリビュートを学習していないにも関わらず、アトリビュートを学習した既存手法と同等の精度を達成した。</li><li>顔のアトリビュートの編集、アトリビュートを保ったアイデンティティーの転写といった編集が可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>このネットワークを用いて他の物質の個人性を抽出して何が出てくるのか興味がある。例えば顔の代わりに魚を学習させて、鯛ごとの個人性、マグロごとの個人性を抜き出してみるなど。</li><li><a href="https://arxiv.org/abs/1804.03487">論文</a></li></ul></div></div><div class="slide_index">[#235]</div></div></section><section id="Robust_Facial_Landmark_Detection_via_a_Fully-Convolutional_Local-Global_Context_Network"><div class="paper-abstract"><div class="title">Robust Facial Landmark Detection via a Fully-Convolutional Local-Global Context Network</div><div class="info"><div class="authors">D. Merget, M. Rock and R. Gerhard</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>FCNの中にKernel convolutionを暗黙的に入れ込み，大域的特徴情報を残すというアイデアを提案．Conv層で局所特徴を取り，KernelConvでそれをブラーにかけ，DilatedConv層で大局的特徴をリファインするという構造．</p><p>特に解像度に独立・きっちりROIがとれない・要複数検出対応・要遮蔽対応な顔ランドマーク検出タスクに有効．KernelConvによって勾配平滑化と過学習抑制が働き収束しやすくなる．
アウトライア弾きのために，事前処理ステップにおいて，ネットワーク出力をシンプルなPCAベース2D形状モデルにフィットしておく．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Robust_Facial_Landmark_Detection_via_a_Fulaly-Convolutional_Local-Global_Context_Network_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>従来は階層構造やプーリング，統計モデルへのフィッティングで対応していたところを，FCNに直に大域的特徴を入れ込むようにした．</li><li>構造単純化により，学習パラメータが少なくなる．</li><li>顔ランドマーク検出に適用してみて，いくつかのSOTAな手法より良い性能を出した．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://www.mmk.ei.tum.de/fileadmin/w00bqn/www/Verschiedenes/cvpr2018.pdf">論文</a></li></ul></div></div><div class="slide_index">[#236]</div><div class="timestamp">2018.5.15 13:31:33</div></div></section><section id="Direction-aware_Spatial_Context_Features_for_Shadow_Detection"><div class="paper-abstract"><div class="title">Direction-aware Spatial Context Features for Shadow Detection</div><div class="info"><div class="authors">X.Hu, L.Zhu, C.W.Fu, J.Qin, and P.A.Heng</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1712.04142</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>影の周りには様々な背景があり，セマンティクスを理解しなければならないため，影の検出は基本的のようで困難である．それに対して，方向認識の方法で画像のコンテキストを解析することで影検出手法を提案する．空間のRNN内のコンテキスト特徴が密集している箇所にアテンションを導入することで方向認識の手法を定式化する．97％の検出精度と38％のバランスエラー率の低減を実現．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Direction-aware_Spatial_Context_Features_for_Shadow_Detection.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>空間的なRNNに対してアテンション機構を設計しdirection-aware spatial context (DSC)モジュールを構築することで方向認識の方法で空間的なコンテキストを学習．</li><li>重み付き交差エントロピー損失が影と影でない領域における検出精度のバランスが取れるように設計．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>影の検出だけでなく，顕著性検出およびセマンティックセグメンテーションなどの他のアプリケーションで使用する事もできそう．</p><ul><li><a href="https://arxiv.org/pdf/1712.04142.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#237]</div><div class="timestamp">2018.5.15 02:31:23</div></div></section><section id="Learning_to_Act_Properly_Predicting_and_Explaining_Affordances_from_Images"><div class="paper-abstract"><div class="title">Learning to Act Properly: Predicting and Explaining Affordances from Images</div><div class="info"><div class="authors">Ching-Yao Chuang, Jiaman Li, Antonio Torralba and Sanja Fidler</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Kazuho Kito</div><div class="item1"><div class="text"><h1>概要</h1><p>現実の多様な場面での環境の物体に対するアフォーダンスの推定する研究。ADE20kを基にしたADE-Affordanceというデータセットの提案。このデータセットはリビングなどの屋内から、道路や動物園などの屋外まで幅広いタイプの画像とそのannotationで構成。また、画像中の物体に対してアフォーダンスの推理を行うための，画像からcontextual informationを伝えるGraph Neural Networksの提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Act_Properly.PNG" alt="Learning_to_Act_Properly.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・ある場面の状況下での適切でない行動の理由について身体的や社会的な観点から説明・画像上のある物体に対してだけでなくその場面を全体としてとらえてアフォーダンスの推論を行っている．
・物体間の依存関係をモデル化することでアフォーダンスとその説明を生成</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.07576">論文</a></li><li><a href="http://www.cs.utoronto.ca/~cychuang/learning2act/">Project Page</a></li></ul></div></div><div class="slide_index">[#238]</div><div class="timestamp">2018.5.14 19:28:40</div></div></section><section id="Discriminability_objective_for_training_descriptive_captions"><div class="paper-abstract"><div class="title">Discriminability objective for training descriptive captions</div><div class="info"><div class="authors">R.Luo, B.Price, Scott Cohen and G.Shakhnarovich</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1803.04376</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>現在のキャプショニング方法は，2つの異なる画像であるにも関わらず，同じキャプションを生成してしまうなどの弁別性にかけている．それに対して，学習の際に画像とキャプションの一致度を直接関連付けるLossを組み込むことによって他のキャプションよりも弁別性のあるキャプションを生成している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Discriminability_objective_for_training_descriptive_captions.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>機械翻訳の評価指標であるBLEU，METEOR，ROUGE，CIDErやSPICEにおいても既存のキャプショニング手法よりも高いスコアを示している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>これにより，同じような画像に対するバリエーションが増え，ユニークなイメージキャプショニングの幅が広がった!!</p><ul><li><a href="https://arxiv.org/pdf/1803.04376.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#239]</div><div class="timestamp">2018.5.14 19:39:23</div></div></section><section id="A_Face-to-Face_Neural_Conversation_Model"><div class="paper-abstract"><div class="title">A Face-to-Face Neural Conversation Model</div><div class="info"><div class="authors">Hang Chu, Daiqing Li, Sanja Fidler</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>入力された会話文に対して、その返答と適切な顔のジェスチャーを生成する手法。映画データセットを元にトレーニングデータセットを構築。
RNNに対してディスクリミネータの出力を報酬とした強化学習を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/A_Face-to-Face_Neural_Conversation_Model.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力は会話文のみ、あるいは動画。動画が入力の場合には同じテキストでも発話者の表情によって出力される返答文が変化する。</li><li>出力が会話文だけの場合よりも、同時に顔のジェスチャを生成した方が生成された会話文がよりGTの会話文に近くなったことを主張。</li><li>データセットは250種類の映画データセットMovieQAにおいて単一人物が写っているシーンにおいて顔向、ジェスチャカテゴリ、タイムスタンプを取得することで構築した。</li><li>生成された返答文の妥当性を評価するためにamazon mechanical turkを実施。GANを導入したことで返答文の多様性、妥当性がstate-of-the-artの手法に勝った。</li><li>このモデルで学習したボットとリアルタイムで会話することも可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>デモを見るとまだ返答文自体には違和感があるが、顔のジェスチャがつくことで会話している気分になる。ボットのモデルが謎のおじさん。</li><li><a href="http://chuhang.github.io/files/publications/CVPR_18_2.pdf">論文</a></li><li><a href="http://www.cs.toronto.edu/face2face">Project page</a></li></ul></div></div><div class="slide_index">[#240]</div></div></section><section id="CosFace_Large_Margin_Cosine_Loss_for_Deep_Face_Recogntion"><div class="paper-abstract"><div class="title">CosFace: Large Margin Cosine Loss for Deep Face Recognition</div><div class="info"><div class="authors">Syed Zulqarnain Gilani, Ajamal Mian</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔認識のための新たなロス関数としてソフトマックス関数をベースとしたLarge Margin Cosine Loss(LMCL)を提案した研究。LMCLはソフトマックス関数の指数部分を重みベクトルWと特徴量ベクトルxの内積においてWとxのノルムを1とし、定数mを引いた関数。
認識タスクでは異なるクラスタ間の距離を遠く、同じクラスタ間の距離を近くする、という基本的な考えがある。
LMCLはこの考えを元に上記のようにL2正則化を施すことで、Wとxのノルムに左右されることなくWとxの角度空間においてクラスタの分離を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/CosFace_Large_Margin_Cosine_Loss_for_Deep_Face_Recogntion.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>ソフトマックス関数において重みベクトルの大きさ、入力特徴量のノルムを除外することで、cosの影響を最大限に大きくしWとxの角度空間におけるマージンの最大化を提案。</li><li>face identification(この人はAさんであるか？)、face verification(この人は女性であるか？)の多くのタスクにおいて,ソフトマックス関数由来のロス関数、state-of-the-artの手法よりも良い精度となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>汎用的な認識タスクに使用できそうだが、顔認識に限定したのはデータセットや既存研究との比較のため？</li><li><a href="https://arxiv.org/abs/1801.09414">論文</a></li></ul></div></div><div class="slide_index">[#241]</div></div></section><section id="Sparse_Photometric_3D_Face_Reconstruction_Guided_by_Morphable_Models"><div class="paper-abstract"><div class="title">Sparse Photometric 3D Face Reconstruction Guided by Morphable Models</div><div class="info"><div class="authors">Xuan Cao, Zhang Chen, Anpei Chen, Xin Chen, Cen Wang, jingyi Yu</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>異なる位置の点光源１個によって照らされた５枚の正面顔画像から高品質な３次元顔形状を最適化によって復元する研究。被写体の正面に5つのLED点光源が配置されいている照明環境で撮影を行う。
入力画像に対して3D morphable modelを適用することで簡易的な3次元顔形状を生成し、法線マップ組み合わせることで点光源の位置をピクセル単位で推定する。
またセマンティックセグメンテーションを行うことで体毛が生えいてる領域とそうでない領域に分割し、体毛が生えている領域にはフィルタ処理を行うことでノイズを除去する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Sparse_Photometric_3D_Face_Reconstruction_Guided_by_Morphable_Models.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>顔画像からいきなり光源位置を推定するのではなく、一度morphalbe モデルに生成することで推定精度が大きく向上。</li><li>3Dスキャンなどの大掛かりな装置を必要としない。</li><li>顔の小じわ、毛穴、まつ毛なども再現するほど高品質な3次元顔形状を復元。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>推定された光源位置自体の精度結果を見てみたかった。</li><li>配置する点光源の位置については特に言及がなかったが、配置による影響の比較結果がみてみたかった。</li><li><a href="https://arxiv.org/abs/1711.10870">論文</a></li></ul></div></div><div class="slide_index">[#242]</div></div></section><section id="FSRNet_End-to-End_Learning_Face_Super-Resolution_with_Facial_Priors"><div class="paper-abstract"><div class="title">FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors</div><div class="info"><div class="authors">Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, Jian Yang</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔の超解像度化を学習させる際にランドマーク、パーツの位置推定を同時に行うネットワーク(FSR Net)を提案した研究。同ネットワークをベースにFSR GANも提案。
また生成された高解像度画像に対する評価尺度として生成画像とGTにおけるランドマークのNRMSE、顔パーツに対するセマンティックセグメンテーション画像(parsing)に対するPSNR、SSIM、MSEを提案。
GANベースの手法では高精細な画像が生成されるがPSNR、SSIMが低くなり、MSEをロスとしたネットワークではPSNR、SSIMは高いがボケた画像になってしまう、というジレンマから上記の評価尺度を導入。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/FSRNet_End-to-End_Learning_Face_Super-Resolution_with_Facial_Priors.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力画像は16x16の様々な顔むきの画像、出力は128x128に超解像度化された画像。</li><li>state-of-the-artの手法よりもSSIM、PSNRが高く、また新たな評価尺度として提案したランドマーク、face parsingの位置推定も既存手法よりも高い精度となった。</li><li>新たに提案した評価指標自体の妥当性は、FSR GANとFSR Netを比べた際に、FSR Netの方がボケた画像を生成したにも関わらずSSIM、PSNRが高く、一方でFSR GANの方がランドマーク、face parsingの推定精度が高かったことを根拠に主張している。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>比較画像において既存手法の画像があまりにもボケているため、既存手法のコントリビューションを確かめるという意味でも調査が必要と感じた。</li><li><a href="https://arxiv.org/abs/1711.10703">論文</a></li><li><a href="https://github.com/tyshiwo/FSRNet">GitHub</a></li></ul></div></div><div class="slide_index">[#243]</div></div></section><section id="_2D3D_Pose_Estimation_and_Action_Recognition_using_Multitask_Deep_Learning"><div class="paper-abstract"><div class="title">2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning</div><div class="info"><div class="authors">Diogo C. Luvizon, David Picard, Hedi Tabia</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>相互に関連性がある2D/3D姿勢推定+人物行動認識を多タスク学習（Multi-task Learning）により最適化した論文である。それぞれで学習を行ったときよりも高い精度を実現することを明らかにし、複数のデータセットにてState-of-the-artな性能を叩き出した。2Dと3Dの姿勢推定、人物行動の特徴量が相補的に補完し合い特徴学習をより高度にしている？</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180514PoseActionMultiTask.png" alt="180514PoseActionMultiTask"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>姿勢推定（しかも3D姿勢推定も含めて）や人物行動認識を単一の枠組みで解決、さらには多タスク学習により別々に学習したときよりも高い精度でふたつの問題を解決した。さらに複数のベンチマーク（姿勢推定：Human3.6M, MPII/行動認識：PennAction, NTU）にて最高精度も叩き出したことが採択の理由である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>動画シーケンスから姿勢と行動を同時出力する、ありそうでなかった研究である。先にやったもの勝ちだが、高度な最適化を実施し特に最高精度を出すのは難しい。CVPRではState-of-the-artとなるかどうかがひとつの採点基準でもある（が、全てではない）ため、実装力をつけておくに越したことはない。</p><ul><li><a href="https://arxiv.org/abs/1802.09232">論文</a></li><li><a href="https://www.youtube.com/watch?v=MNEZACbFA4Y">Youtube</a></li></ul></div></div><div class="slide_index">[#244]</div><div class="timestamp">2018.5.14 13:04:47</div></div></section><section id="Maximum_Classifier_Discrepancy_for_Unsupervised_Domain_Adaptation"><div class="paper-abstract"><div class="title">Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</div><div class="info"><div class="authors">Kuniaki Saito et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>目的のタスクに特化した２つの分離境界を利用したドメイン適応手法。従来の埋め込み空間においてドメイン間の分布を単に近づける方法に対して、あるタスクと解くための分離境界を考慮して適応を行う。この枠組みでの適応はtargetでの損失の上界を下げる埋め込み空間への写像を求める作業と類似している。さまざまなドメイン適応のベンチマークにおいてSoTA。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Maximum_Classifier_Discrepancy_for_Unsupervised_Domain_Adaptation.png"></p></div></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>Source(S)で学習を行った二つの識別境界を作成する。その識別器がTarget(T)で異なる判断を行ったサンプル(discrepancy)はSの分布とは乖離している領域であると考えられる。以下のような敵対的な適応を行う。(1) TにおけるDiscrepancyが増加するよう識別境界を学習。(2) Discrepancyが減少するように埋め込み空間を学習。(3)Sでの識別は常にうまくいくよう学習。
識別境界を考慮した適応という新規性、理論的な背景、論文の明快さ、精度としての結果が揃っている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>アイデアの面白さと同時に論文が非常にわかりやすかった。識別境界はあくまで埋め込み関数を適化するために得たものなので、この枠組みで得られる最終的なもの以外(得られた埋め込み空間上で新たに学習したもの)でもうまくいくのではないかと感じた。</p><ul><li><a href="https://arxiv.org/abs/1712.02560">論文</a></li></ul></div></div><div class="slide_index">[#245]</div></div></section><section id="Generative_Non-Rigid_Shape_Completion_with_Graph_Convolutional_Autoencoders"><div class="paper-abstract"><div class="title">Generative Non-Rigid Shape Completion with Graph Convolutional Autoencoders</div><div class="info"><div class="authors">Or Litany, Alex Bronstein, Michael Bronstein, Ameesh Makadia</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>非剛体的な変形を伴う３Dオブジェクトの形状補完．部分的な形状補完のための学習ベースの手法としてgraph-convolutionを含むVAEを提案した．推論時には，既知の部分的な入力データに合う形状を生成できる変数を潜在空間で探すように最適化する．結果として人体と顔の合成データ，リアルなスキャンデータに対する補完が可能であることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Generative_Non-Rigid_Shape_Completion_with_Graph_Convolutional_Autoencoders_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>従来手法よりも優れている点</h1><ul><li>訓練中に部分的な形状を見る必要なしに，任意スタイルで一部として切り出されたデータを扱えること</li><li>人間以外にも，任意の種類の３Dデータに適用できる手法であること</li><li>形状補完はデータに適合する解が複数ある問題であり，複数のもっともらしい解を生成し，この問題に対応できること</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.00268">arXiv</a></li></ul></div></div><div class="slide_index">[#246]</div><div class="timestamp">2018.5.13 16:22:39</div></div></section><section id="Eye_In-Painting_with_Exemplar_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Eye In-Painting with Exemplar Generative Adversarial Networks</div><div class="info"><div class="authors">Brian Dolhansky, Cristian Canton Ferrer</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要，新規性</h1><p>eye-Inpaintingを行う手法．顔のようなそれぞれ固有の特徴を持つ画像においてのInpaintingで，従来のDNNによる手法は新しい顔を生成するなどidentityを保たなかった．exemplar informationを利用するconditional GAN（ExGANs）を提案．参照画像やperceptual codeというidentifying information（exemplar information）をGANの複数の箇所で利用することで，perceptualに優れ，identityを反映した結果を生成することができた．identifying informationをGANの複数の箇所で利用することが新しい．さらに，将来の比較のためにEye-Inpaintingのタスクの新しいベンチマークとデータセットを用意した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Eye_In-Painting_with_Exemplar_Generative_Adversarial_Networks_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法概要</h1><p>cGANの一種．参照画像のIdentityを符号化するネットワークと，Generator，Discriminatorから成る．identifying informationを生成に利用するだけでなく，DiscriminatorやPerceptual lossの算出にも利用している．参照画像をベースにした場合と符号をベースにした場合にアプローチを分けている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.03999">arXiv</a></li></ul></div></div><div class="slide_index">[#247]</div><div class="timestamp">2018.5.13 16:12:11</div></div></section><section id="Logo_Synthesis_and_Manipulation_with_Clustered_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks</div><div class="info"><div class="authors">Alexander Sage, Eirikur Agustsson, Radu Timofte, Luc Van Gool</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>特徴ベクトルのクラスタリングでGANの入力ベクトルを作成する学習方法で，ロゴの生成と操作が可能とした．ロゴのデータは高マルチモーダルのデータであり，従来のSoTAではmode collapseを起こしてしまうが，提案する学習方法では多様なロゴを生成する．iWGANをCIFER-10で学習するとき，提案する学習方法によって，Inception scoreでSoTA達成．Contribution:</p><ul><li>600k以上のロゴを収集してデータセットを構築</li><li>マルチモーダルなロゴデータでのGANの学習方法</li><li>潜在空間の探索によって，インタラクティブなロゴ生成</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Logo_Synthesis_and_Manipulation_with_Clustered_Generative_Adversarial_Networks_fig.png" alt="Image"></p><p>上段はデータセットから．下段が生成結果．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>Clustered GAN Trainingと読んでいる．GANのネットワークは，DCGANとimproved Wasserstein GAN with gradi- ent penalty (iWGAN)を利用．オートエンコーダーの中間特徴ベクトルもしくは，Resnetの特徴ベクトルをクラスタリングして，Generatorの入力ベクトルとする．このクラスタリングでセマンティックに意味のあるクラスタを形成し，GANの学習を向上させることが可能．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://data.vision.ee.ethz.ch/sagea/lld/">データセット</a></li><li>ロゴ・ジェネレーター・インターフェースも用意されている．スライダーを動かして，生成結果を操作できる</li><li><a href="https://arxiv.org/abs/1712.04407">arXiv</a></li></ul></div></div><div class="slide_index">[#248]</div><div class="timestamp">2018.5.13 16:03:23</div></div></section><section id="Multi-Agent_Diverse_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Multi-Agent Diverse Generative Adversarial Networks</div><div class="info"><div class="authors">Arnab Ghosh, Viveka Kulharia, et al.</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>多様で意味のあるサンプルを生成可能な，複数のGeneratorと１つのDiscriminatorから成るGAN(MAD-GAN)を提案．一つのGeneratorが一つの構成要素を担当する混合モデルとしてはたらく．いくつかの従来のGAN手法と比較実験を行い，MAD-GANは多様なモードを獲得できることを確認．さらに，理論的な分析も行っている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Multi-Agent_Diverse_Generative_Adversarial_Networks_fig.png" alt="Image">それぞれの行が異なるGeneratorによって生成した結果．行はそのGeneratorにランダムなノイズzを入力して生成した結果．マルチビューなデータセットから異なるモードを異なるGeneratorが学習していることを確認できる．</p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>Multi-agent GAN．複数のGeneratorと１つのDiscriminatorで構成．</li><li>Generator同士は，最終層以外は重みを共有している．</li><li>複数のGeneratorの生成サンプルと真のサンプルをDに入力し，Discriminatorは，FakeとRealの判別だけではなくて，そのFakeの生成サンプルを与えるGeneratorがどれであるかも予測する．これによって，複数のモードがある時，個別のモードに対してそれぞれのGeneratorを振り分けるようにDiscriminatorが学習する．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>image-to-image変換,multi-view生成， face generationなど多数の実験を行っている．</li><li>展望は，MAD-GANでは複数のGeneratorを使うことになるが，いくつのGeneratorが必要なのかを推定できるようにすること．</li><li><a href="https://arxiv.org/abs/1704.02906">arXiv</a></li></ul></div></div><div class="slide_index">[#249]</div><div class="timestamp">2018.5.13 15:50:21</div></div></section><section id="SketchyGAN_Towards_Diverse_and_Realistic_Sketch_to_Image_Synthesis"><div class="paper-abstract"><div class="title">SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis</div><div class="info"><div class="authors">Wengling Chen, James Hays</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>スケッチから写真を生成する手法の提案．50のカテゴリの写真を生成することができる．スケッチに対して，自動でデータ拡張をする方法を示し，その拡張方法がタスクに有効であることを示す．さらに追加の目的関数と新しいネットワーク構造も提案．マルチスケールの入力画像を入れることで情報の流れを向上させている．結果はまだphotorealisticとは言えないが，従来手法よりリアルでinception scoreの高い結果を得た．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SketchyGAN_Towards_Diverse_and_Realistic_Sketch_to_Image_Synthesis_fig.png" alt="Image"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><ul><li>データ拡張の方法として，エッジ検出などのいくつかの処理を組み合わせている．</li><li>ネットワーク構造はU-net構造だが，各ブロックで入力画像で条件付けを行うのが特徴．以前の層で抽出された特徴マップと比べ新しい特徴量を入力画像から選択的に抽出するための内部マスクを学習するため，Masked Residual Unitというブロックモジュールを導入した．（DCGAN, CRN, ResNetとの比較がある）</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>GeneratorにもDiscriminatorにも途中で画像やラベルの情報をinjectionする方法が増えている印象．</li><li>sketchから似ている写真を検索してくるという方法がこれまでよく研究されていた．今回は，スケッチから新しく写真を生成する（質はまだ低い）</li><li><a href="https://arxiv.org/abs/1801.02753">arXiv</a></li></ul></div></div><div class="slide_index">[#250]</div><div class="timestamp">2018.5.13 15:37:35</div></div></section><section id="ScanComplete_Large-Scale_Scene_Completion_and_Semantic_Segmentation_for_3D_Scans"><div class="paper-abstract"><div class="title">ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans</div><div class="info"><div class="authors">Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, Juergen Sturm, Matthias Nießner</div><div class="conference">CVPR 2018</div><div class="paper_id">584</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>部分的なシーンの3Dデータからシーンの幾何及びボクセルごとのセマンティック情報をコンプリートする手法ScanCompleteを提案した．</li><li>従来，シーンの3次元情報を完全に収集するのが非常に困難，シーンの３次元のデータの膨大さや形状情報のバリエーションの多さは従来のシーン補完に対して困難な問題設定である．そういったため，シーンのコンプリートでは出力の質が低いという問題点がある(contentsとして応用するレベルではない)．こういった困難を解決するため，提案手法は①trainとtestデータの入力解像度を異なる値に設定し， testの場合シーンのサイズの変化を対応できるようにする．②coarse-to-fineなfully convolution 3DCNNを用いて，グローバルなシーンの構造特徴および精密な局所的補間をできるようにする．</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/scancomplete.png" alt="scancomplete"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>異なる入力シーンのサイズを自由に対応できる（最大70×60×3m くらいまでできる）</li><li>従来の手法：3D-EPN,SSCNetなどの従来手法と比べ，scene completion, semantic labeling両方精度がSOTA</li><li>出力結果が3D Contentsとして応用できるレベル</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.10215">論文</a></li></ul></div></div><div class="slide_index">[#251]</div><div class="timestamp">2018.5.14 14:43:15</div></div></section><section id="Learning_from_Millions_of_3D_Scans_for_Large-scale_3D_Face_Recognition"><div class="paper-abstract"><div class="title">Learning from Millions of 3D Scans for Large-scale 3D Face Recognition</div><div class="info"><div class="authors">Syed Zulqarnain Gilani, Ajamal Mian</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>大規模3D顔データセットを構築し、そのデータによってトレーニングされたCNNが高い3D顔認識精度を持つことを示した論文。従来の3D顔データセットはデータ数が少なく、最も多いND-2006でも888アイデンティティー・13540種類のみであったが、本論文で構築されたトレーニング用データセットはおよそ10万アイデンティティー・310万種類。
このトレーニングデータを用いてCNNを学習させることで、認識精度は98.74%となりstate-of-the-artよりも優っていることを確認した。
また既存の3D顔データセットをマージすることで、1853アイデンティティー・31K種類のテスト用3D顔データセットを構築した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180513Learning_from_Millions_of_3D_Scans-scale_3D_Face_Recognition.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>トレーニング用の3D顔データは1000人の3Dスキャンデータに対して、変形に要するエネルギーがもっとまた商用ソフトを使用すること300種類の顔のうち顔の形状・表情が似ている顔を合成して生成。も高くなる顔のペアを合成して生成。また商用ソフトを使用すること300種類の顔のうち顔の形状・表情が似ている顔を合成して生成。
前者は別の顔を識別するため、後者は似た顔を識別する目的で用意されたデータである。
生成された顔に対して水平方向、垂直方向から15度ずつ撮影することで、計100,005アイデンティティー・3,169,275種類の3D顔データを生成。</li><li>既存の3D顔認識・2D顔認識手法に対してオープン・クローズドテスト両方における精度を比較したところ、提案モデルがもっとも良い精度となった。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.05942">論文</a></li></ul></div></div><div class="slide_index">[#252]</div></div></section><section id="Learning_to_Generate_Time-Lapse_Videos_Using_Multi-Stage_Dynamic_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks</div><div class="info"><div class="authors">Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, Jiebo Luo</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>高解像(128x128)のリアルタイムなタイムラプス動画の生成をするGANを提案．最初のフレームを与えると，近未来のフレームを生成する．新規性としては，</p><ul><li>タイムラプスデータセットを作成</li><li>タイムラプス向きの近未来予測ネットワークを提案（Multi-stage Dynamic Generative Adversarial Network (MD-GAN) ）</li><li>モーションのモデリングにGram matrixを導入し，実世界ビデオのモーションを模倣するためのadversarial ranking lossを提案</li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_to_Generate_Time-Lapse_Videos_Using_Multi-Stage_Dynamic_Generative_Adversarial_Networks_fig.png" alt="fig"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>corse-to-fineの２ステージアプローチのGAN．ステージを分けた狙いとしては，１ステージ目でコンテンツの生成を行い，２ステージ目でモーションのモデリングを行うこと．１ステージ目のU-net風のネットワークでは3D convolutions と deconvolutions を含んでいる．</p><p>２ステージ目のDiscriminatorとして，モーションパターンをモデル化するためにGram matrix使って，adversarial ranking lossを算出する．1ステージの出力ビデオ，2ステージ目の出力ビデオ，真のビデオからランキングをとる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1709.07592">arXiv</a></li></ul><p>タイムラプス用のGANが初めて提案されたことが評価されたのかなという印象．定量的な評価はメインがPreference Opinion Scoreで, 他はMSE, PSNR and SSIM．</p></div></div><div class="slide_index">[#253]</div><div class="timestamp">2018.5.13 12:45:36</div></div></section><section id="Hyperparameter_Optimization_for_Tracking_with_Continuous_Deep_Q-Learning"><div class="paper-abstract"><div class="title">Hyperparameter Optimization for Tracking with Continuous Deep Q-Learning</div><div class="info"><div class="authors">Xingping Dong et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>Object Tracking 手法において用いられる複数の Hyperparameter を強化学習によって各シークエンス毎に最適化する手法を提案. Hyperparameter の選択を Action, Tracking の精度の良さを Reward として, Normalized Advantage Functions (NAF) を用いた強化学習を行なっている. また, Heuristic を導入することで, 学習の遅さの問題を緩和した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuhara-Hyperparameter-Optimization-for-Tracking-with-Continuous-Deep-Q-Learning.png" alt="fukuhara-Hyperparameter-Optimization-for-Tracking-with-Continuous-Deep-Q-Learning.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Object Tracking における Hyperparameter の最適化問題を強化学習の問題として定式化した.</li><li>上記の問題を既存の強化学習手法である NAF　(連続な行動が取れるように拡張された Q 学習の手法) を用いて解いた.</li><li>強化学習を適用した際に, 状態空間の次元の多さなどに由来する学習速度の遅さを huristic を導入することで緩和した.</li><li>OTB-2013 や VOT-2015 などのデータセットを用いて既存研究(Siam-py等)と比較. 同程度の速度で, 正確性とロバスト性の両方に置いて既存手法を上回った. </li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://www.porikli.com/mysite/pdfs/porikli%202018%20-%20Hyperparameter%20optimization%20for%20tracking%20with%20continuous%20deep%20Q-learning.pdf" target="blank">[論文] Hyperparameter Optimization for Tracking with Continuous Deep Q-Learning</a></li><li><a href="https://arxiv.org/abs/1603.00748" target="blank">[関連論文: NAF] Continuous Deep Q-Learning with Model-based Acceleration</a></li></ul></div></div><div class="slide_index">[#254]</div><div class="timestamp">2018.5.12 13:05:55</div></div></section><section id="Tangent_Convolutions_for_Dense_Prediction_in_3D"><div class="paper-abstract"><div class="title">Tangent Convolutions for Dense Prediction in 3D</div><div class="info"><div class="authors">Maxim Tatarchenko et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="item1"><div class="text"><h1>概要</h1><p>３次元データを扱う新しい convolutional の方法 "Tangent Convolution" を提案. 全ての点の近傍点を仮想的な接平面上に射影し, 接平面上で畳み込みを行う. 接平面は法線ベクトルが計算できれば構成する事ができるため, 複数のデータ形式に対して同様に適用が可能. また, 事前計算を行う事によって大規模なデータベースに対しても効率的に計算を行う事が可能となった.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/fukuahra-Tangent-Convolutions-for-Dense-Prediction-in-3D.png" alt="fukuahra-Tangent-Convolutions-for-Dense-Prediction-in-3D.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力データの形式は法線ベクトルを近似的に求められるもの (point clouds, meshes, dpolygon soup) であればなんでも良い.</li><li>事前計算を行う事によって大規模なデータ（数百万オーダーの点群）も効率的に扱う事ができる.</li><li>提案手法の有効性を示すために Tangent Convolution を用いたネットワークを Semantic 3D Scene Segmentation のタスクに置いて既存手法 (PointNet, ScanNet, OctNet) と比較し, 複数の評価尺度に置いて最も良い精度となった.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://vladlen.info/papers/tangent-convolutions.pdf">[論文]</a></li></ul></div></div><div class="slide_index">[#255]</div><div class="timestamp">2018.5.12 11:33:55</div></div></section><section id="Im2Pano3D_Extrapolating_360_Structure_and_Semantics_Beyond_the_Field_of_View"><div class="paper-abstract"><div class="title">Im2Pano3D: Extrapolating 360 Structure and Semantics Beyond the Field of View</div><div class="info"><div class="authors">Shuran Song, Andy Zeng, Angel Chang, Manolis Savva, Silvio Savarese, Thomas Funkhouser</div><div class="conference">CVPR 2018</div><div class="paper_id">466</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・部分的に観測されたシーン(RGB-D)から，full sceneの構造及びセマンティックラベルを推定する新規な問題設定”semantic-structure view extrapolation”及びフレームワークを提案した．</p><p>・従来のview extrapolationは画像のboundryの色情報しか行わず，シーンのセマンティック構造に対してextrapolationを行う研究がない．そこで，この論文で，著者達がsemantic-structure view extrapolationを提案し，50%以下のシーンの観測データから構造及びセマンティックをextrapolation予測する．</p><p>・提案フレームワークは：①一枚のマルチチャンネルpanorama画像でシーンの情報(RGB，構造，セマンティック)を表示する；②3次元構造をデプスのような詳細な三次元情報を用いずに，3次元平面方程式で表示する．③マルチロス関数(ピクセルレベル，グローバルコンテキスト)を用いる．</p><p>・提案フレームワークの考え方は入力と出力を一枚のマルチチャンネルpanorama画像として表示し，encoder-decoderにより，欠損した入力からfullなpanorama画像を出力する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Im2Pano3D.png" alt="Im2Pano3D"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・CG データセットSUNCG及びリアルシーンデータセットMatterport3Dを用いて従来手法よりシーンの構造及びセマンティックの予測が優位．</p><p>・一枚のマルチチャンネルpanorama画像でシーンの情報を表示し，シーンの情報を固定なサイズにできるので，2次元畳み込みを用いられる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・マルチチャンネルpanorama画像でシーンの情報を保存するところが賢い</p><p>・提案フレームワークは構造的に理解しやすい，実装してみたい</p><ul><li><a href="https://arxiv.org/abs/1712.04569">論文</a></li><li><a href="http://im2pano3d.cs.princeton.edu/">ポロジェクト</a></li></ul></div></div><div class="slide_index">[#256]</div><div class="timestamp">2018.5.11 17:40:13</div></div></section><section id="Deep_image_prior"><div class="paper-abstract"><div class="title">Deep Image Prior </div><div class="info"><div class="authors">Dmitry Ulyanov et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>「CNNは理論上任意の関数を近似できるが、その構造自体に汎化性能をあげるようなPriorが含まれている」という考えのもと、ランダム初期化されたCNNを用いて高いレベルの画像復元、ノイズ除去などを行った。また、CNNのPrior をさらに裏付けるものとして、自然画像を復元するより、ノイズ画像を復元する学習の方がiteration数がかかることも示された。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_image_prior.png"></p></div></div><div class="item3"><div class="text"><h1>手法・なぜ通ったか？</h1><p>ノイズ画像ｚをencoder-decoderモデルに入力して、生成された画像を欠損画像にMSEで近づけるように学習するだけである。注意点として、完全に学習仕切ってしまうと欠損画像と同じものが出るだけなので、学習をある程度のiterationで止めると、復元されたような画像が得られる。また、CNNのPrior をさらに裏付けるものとして、自然画像を復元するより、ノイズ画像を復元する学習の方がiteration数がかかることも示された。着眼点や面白い実験方法に加え結果も伴っている研究</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>畳み込み処理×SGDの異常なまでの汎化性能を実験的に裏付けていると思われ非常に面白い。逆にCNNのPriorの苦手なところとして、Adversarial exampleやGANのチェッカーボード現象も関係してそう。畳み込み処理の派生(Deformable convなど)でのpriorの検証も気になる。</p><ul><li><a href="https://arxiv.org/abs/1711.10925">論文</a></li></ul></div></div><div class="slide_index">[#257]</div></div></section><section id="Edit_Probability_for_Scene_Text_Recognition"><div class="paper-abstract"><div class="title">Edit Probability for Scene Text Recognition</div><div class="info"><div class="authors">F. Bai, Z. Cheng, Y. Niu, S. Pu and S. Zhou</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>OCRのstate-of-the-artな手法として，encoder-decoderで文字カテゴリごとのAttentionを取ってからテキスト認識をするvisual attentionベーステキスト認識があるが，
ある文字がよく見えなかったり1文字でも複数ピークが出てしまったりする問題はある．
GTとの差を取るとして，エンコード後の文字列で比較する<a href="https://ja.wikipedia.org/wiki/%E3%83%AC%E3%83%BC%E3%83%99%E3%83%B3%E3%82%B7%E3%83%A5%E3%82%BF%E3%82%A4%E3%83%B3%E8%B7%9D%E9%9B%A2">編集距離</a>を取ることが考えらえるが，
本稿ではVAで出る尤度分布で比較する，編集確率（Edit Probablity）を提案する．
これにより，字抜けや余分な字を拾ってしまうような誤認識に強い文字認識を実現可能．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Edit_Probability_for_Scene_Text_Recognition_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>Attentionベーステキスト認識においてstate-of-the-artな性能．</li><li>まさに正統進化といえる．</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>正統進化を，他のラボが，1年未満に行ってしまっているあたり，CV分野の流れの早さがうかがえる．</p><ul><li><a href="https://arxiv.org/abs/1805.03384">arXiv</a></li><li><a href="https://arxiv.org/abs/1706.01487">Visual attention models for scene text recognition</a>（<strong>ICDAR2017</strong>）</li></ul></div></div><div class="slide_index">[#258]</div><div class="timestamp">2018.5.10 18:29:27</div></div></section><section id="iVQA_Inverse_Visual_Question_Answering"><div class="paper-abstract"><div class="title">iVQA: Inverse Visual Question Answering</div><div class="info"><div class="authors">Feng Liu, Tao Xiang, Timothy Hospedales, Wankou Yang, Changyin Sun</div><div class="conference">CVPR 2018</div><div class="paper_id">1199</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・VQA問題の逆問題iVQA設定及びモデルを提案し (画像及び回答文から，質問文を生成する)，更に iVQAもVQAと同じく“視覚-言語”の理解のベンチマック問題設定になれると指摘した．</p><p>・iVQAタスクに用いられるmulti-modal dynamic inferenceなフレームワークを提案した．提案フレームワークは回答文を生成する段階で，“回答文”，“生成した部分的な質問文”によって導かれ動的に画像attentionを調整できる．</p><p>・更に，回答文の従来の自然言語的評価に， ランキングベースなiVQAタスクの回答文を評価できる指標を提案した．その指標により，などの面を評価できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/iVQA.png" alt="iVQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・近年，従来のVQAの成功がデータセットバイアス及び質問文からの情報理解，画像の内容に対する理解がまだVQAにおいて深く利用されていないことが指摘された．そこで，画像と回答文から質問文を予測する問題設定iVQAを提案した， iVQAタスクにおいてはVQAと比べ，①画像内容の理解の要求が高い，②また回答文が常に短いので，質問文と比べよりスパースな情報抽出しかできないため，回答文に頼りすぎることにならない．③モデルの推定及びreasoning能力が更に必要である．</p><p>・提案フレームワークの各パーツ(dynamic attention, multi-modal inferenceなど)の有効性に関してAblation　studyを詳しく行った. 説得力がある．</p><p>・Dynamic attention mapsの可視化分析により問題文を生成する段階で，動的に関連する画像領域にattentionすることを指摘した．</p><p>・実験を通して，iVQAをVQAとヒュージョンしたら， VQAの精度を挙げられることを証明した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・VQAの問題点を深く理解した上での新規問題設定．</p><p>・Dynamic attention mapsの可視化分析により問題文を生成する段階で，動的に関連する画像領域にattentionすることを指摘した．</p><p>・新奇な考え方・詳しい分析実験・論文の理解しやすさなどが非常に良い</p><ul><li><a href="https://arxiv.org/abs/1710.03370">論文</a></li></ul></div></div><div class="slide_index">[#259]</div><div class="timestamp">2018.5.10 15:08:46</div></div></section><section id="Sketch-a-Classifier_Sketch-based_Photo_Classifier_Generation"><div class="paper-abstract"><div class="title">Sketch-a-Classifier: Sketch-based Photo Classifier Generation</div><div class="info"><div class="authors">C. Hu, D. Li, Y. Song, T. and T.M. Hospedales</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>手書き画像から，書いたものの判別をする画像分類器を出力するメタ学習の提案．学習していない手書きカテゴリでも，そのカテゴリの画像分類器が出力される．3つの枠組みが作れる．
(1)スケッチ画像カテゴリ分類モデルを入力
(2)スケッチ画像を入力
(3)コースなリアル画像分類モデル＋スケッチ画像を入力</p><p>枠組みとしては，<a href="https://www.robots.ox.ac.uk/~vgg/rg/papers/eccv2016_learntolearn.pdf">Model Regression Network</a>による．論文では，SVMパラメータの学習を行っている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Sketch-a-Classifier_Sketch-based_Photo_Classifier_Generation_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>多様性がある．作ったモデルの性質がよく把握されている</li><li>知識転用の新しい形が見える</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.01837">arXiv</a></li><li><a href="https://chenhsuanlin.bitbucket.io/spatial-transformer-GAN/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#260]</div><div class="timestamp">2018.5.10 13:49:15</div></div></section><section id="ST-GAN_Spatial_Transformer_Generative_Adversarial_Networks_for_Image_Compositing"><div class="paper-abstract"><div class="title">ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing</div><div class="info"><div class="authors">C. Lin, E. Yumer, O. Wang, E. Shechtman and S. Lucey</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>画像合成の際に，背景に対して位置やサイズ感などが正しくなるように幾何的変換を求め，修正を加えてくれるGANを提案．たとえば，家具が適切な場所に置かれたり，メガネが適切に掛けられたりする．</p><p>構造的には複数のSpatial Transformer Networkをジェネレータとして組み込んでいることが特徴．複数のSTNにおける，反復<a href="https://en.wikipedia.org/wiki/Image_warping">画像ワーピング</a>（画像変形方法の一つ）と逐次学習を導入している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/ST-GAN_Spatial_Transformer_Generative_Adversarial_Networks_for_Image_Compositing_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>画像変換が得られるので，間接的に高解像度画像に適用可能</li><li>ナイーブな単ジェネレータよりも高性能．</li><li>大きな差には弱い．奇抜なデザインのものや，大きな移動</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.01837">arXiv</a></li><li><a href="https://chenhsuanlin.bitbucket.io/spatial-transformer-GAN/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#261]</div><div class="timestamp">2018.5.10 12:27:25</div></div></section><section id="Two_can_play_this_Game_Visual_Dialog_with_Discriminative_Visual_Question_Generation_and_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Two can play this Game: Visual Dialog with Discriminative Visual Question Generation and Visual Question Answering</div><div class="info"><div class="authors">Unnat Jain, Lana Lazebnik, Alex Schwing</div><div class="conference">CVPR 2018</div><div class="paper_id">705</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・Visual Dialogタスクに用いられる質問の回答文と質問文を両方予測できるネットワークを提案した．</p><p>・提案フレームワークは100個の回答文(質問文)から正解を予測する(discriminative).  提案フレームワークは質問文，画像，キャプション，QA履歴，選択などの情報をsimilarity+Fusionネットにより100次元のベクトルを生成し，正解ラベルとのcross-entropy誤差を求める．</p><p>・また，従来Visual Dialogの質問文を評価する指標がない，著者達が質問文を評価できる“VisDial-Q evaluation protocol”を提案した．提案protocolは質問文を100個に固定し，予測した質問文がどれくらい通常の人により提出される可能性が高いかにより評価を行っている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/VisualDialog_DVQG_DVQA.png" alt="VisualDialog_DVQG_DVQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・同じネットワークで質問文と回答文を両方予測できる．</p><p>・質問文を評価できる指標の提案．</p><p>・Discriminative VQAタスクにおいて， VisDial評価指標は従来手法(HRE, MN, HCIAE-D-NP-ATT)より良い性能を達成した．</p><p>・VQGタスクにおいて，提案した評価指標“VisDial-Q evaluation protocol”により55.17% recall@5 と 9.32 mean rankを達成した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.11186">論文</a></li></ul></div></div><div class="slide_index">[#262]</div><div class="timestamp">2018.5.10 04:08:59</div></div></section><section id="Social_GAN_Socially_Acceptable_Trajectories_with_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks</div><div class="info"><div class="authors">Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese and Alexandre Alahi</div><div class="conference">CVPR2018</div><div class="paper_id">234</div></div><div class="slide_editor">Ryosuke Araki</div><div class="item1"><div class="text"><h1>概要</h1><p>人や自律移動プラットフォームが，移動している人を避けるにはいくつかの経路が考えられる．本手法は，人間の経路予測にシーケンス予測とGANを組み合わせたツールを用いて，複数の経路予測を行う．Recurrent sequence-to-sequence modelは，複数の人の間で情報を集約するための新しいプーリング手法を用いて，観測者の行動を予測する．そして，GANを用いてもっともらしい行動をいくつか予測する．予測された経路はDiscriminatorへ入力され，Fake/Real判別をしGANを訓練していく．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180509_SocialGAN1.jpg" alt="20180509_SocialGAN1.jpg"><img src="slides/figs/20180509_SocialGAN2.jpg" alt="20180509_SocialGAN2.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Generatorでは，複数の人が同時にどう動くか予測するために，Encoderの各LSTMの出力をまとめるプーリングモジュールを導入した．Discriminatorは，経路そのものがFake（人として社会的にあり得ない行動）またはReal（あり得る行動）を判断する．ETHやHOTELなどのデータセットを用いて評価実験を行った．12ステップ後のAverage Displacement Error（全ての時間での真値と予測値の誤差）は0.58（Social LSTM: 0.72），Final Displacement Error（最終目的とでの真値と予測値の誤差）1.18（Social LSTM: 1.54）となった．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>GANを使う手法は多く出てきているが，これは面白い応用方法だと思った．Discriminatorをどうやって学習していくかが肝になりそう．</p><ul><li><a href="https://arxiv.org/abs/1803.10892">arXiv</a></li></ul></div></div><div class="slide_index">[#263]</div><div class="timestamp">2018.5.9 01:45:09</div></div></section><section id="Neural_Baby_Talk"><div class="paper-abstract"><div class="title">Neural Baby Talk</div><div class="info"><div class="authors">Jiasen Lu, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Takumu Ikeya</div><div class="item1"><div class="text"><h1>概要</h1><p>画像内で検出した物体から文章を生成するイメージキャプショニングタスクを行うための新たなフレームワークの構築を行った．単語が格納されるスロットを文章内に生成し，生成したスロットを満たすように検出した物体を当てはめていくことでキャプションを行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180508NBT.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>検出された物体の名称が入るスロットを最初に生成し，生成したスロットを満たしていくことでキャプションを行う手法が新しい．</p><p>イメージキャプショニングタスクにおいてFlickr30KとCOCOデータセットでSOTAを達成した.</p></div></div><div class="item4"><div class="text"><p></p><h1>コメント・リンク集<li><a href="https://arxiv.org/pdf/1803.09845.pdf">論文</a></li><li><a href="https://github.com/jiasenlu/NeuralBabyTalk">github</a></li></h1></div></div><div class="slide_index">[#264]</div></div></section><section id="Attentive_Generative_Adversarial_Network_for_Raindrop_Removal_from_a_Single_Image"><div class="paper-abstract"><div class="title">Attentive Generative Adversarial Network for Raindrop Removal from a Single Image</div><div class="info"><div class="authors">Rui Qian, Robby T. Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><ul><li>写真から雨粒を除去する手法の提案</li><li>このタスクが難しいのは，<ol><li>どの領域が，雨粒によって隠されているか不明なこと</li><li>雨粒に隠された背景側の情報がないこと</li></ol></li><li>GAN，LSTMを利用</li><li>Generatorは，Attentive-Reccurent networkとContextual Autoencoderから構成</li><li>はじめにAttentive-Reccurent networkでattention mapを生成　次にContextual Autoencoderで，mapと入力画像から雨粒除去後の画像を生成
　attention mapは，Discriminatorの中間出力とMSE lossを取る際にも利用</li><li>visual attentionという情報によって，<ol><li>Generatorでは雨粒の領域と，周辺の構造にアテンションをより向けることができる</li><li>Discriminatorは復元した領域をより局所的に評価を行える</li></ol></li></ul></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Attentive_Generative_Adversarial_Network_for_Raindrop_Removal_from_a_Single_Image_fig.png" alt="Item2Image"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><ul><li>GeneratorとDiscriminatorの両方でvisual attentionを利用するようにしたこと</li><li>自作の1119枚の雨粒ありと無しのペア画像を用意し学習に利用</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.10098">arxiv</a></li></ul></div></div><div class="slide_index">[#265]</div><div class="timestamp">2018.5.8 21:05:34</div></div></section><section id="Deformable_GANs_for_Pose-based_Human_Image_Generation"><div class="paper-abstract"><div class="title">Deformable GANs for Pose-based Human Image Generation</div><div class="info"><div class="authors">Aliaksandr Siarohin, Enver Sangineto, Ste ́phane Lathuilie`re, and Nicu Sebe</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Naofumi Akimoto</div><div class="item1"><div class="text"><h1>概要</h1><p>与えられたポーズ情報を条件として人物画像を生成するタスクを扱う．任意ポーズへの変形タスクで発生する，（服などの）変換前のピクセルと変換後のピクセルの対応が不整列である問題に対応するために，deformable skip connectionを対案する．
従来手法と比べ，条件画像の服の色・テクスチャを保存して別ポーズの画像を生成できている．
人物画像の生成に限らず，キーポイントを与えることのできる不整列のオブジェクトであれば，この手法が適用できると著者らは考えている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deformable_GANs_for_Pose-based_Human_Image_Generation_fig2.png" alt="fig2"><img src="slides/figs/Deformable_GANs_for_Pose-based_Human_Image_Generation_fig3.png" alt="fig3"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>U-net likeのEncoder-Decoder, GANdeformable skip connectionについて．
変換前後の両方のポーズ情報が既知なので，キーポイント周辺のピクセルが変換前から変換後にどこへ移動するか知ることができる．したがって，キーポイントの座標からアフィン変換を求め，畳み込みから得た特徴マップをアフィン変換することで，服の色やテクスチャを変換前から変換後の画像に移して生成できる．
Encoderの特徴量をアフィン変換し，Decoderの特徴量にskipするのがdeformable skip connectionである．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1801.00055">arXiv</a></li><li><a href="https://github.com/AliaksandrSiarohin/pose-gan">プログラム</a></li></ul></div></div><div class="slide_index">[#266]</div><div class="timestamp">2018.5.8 15:39:41</div></div></section><section id="VizWiz_Grand_Challenge_Answering_Visual_Questions_from_Blind_People"><div class="paper-abstract"><div class="title">VizWiz Grand Challenge: Answering Visual Questions from Blind People</div><div class="info"><div class="authors">Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo</div><div class="conference">CVPR 2018</div><div class="paper_id">491</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・盲人に集められたVQAタスクのデータセットVizWiz（画像と音声質問文）を提案した．VizWizが31,000枚の盲人が携帯により撮影し，画像ごとに画像を撮影した盲人が提出した音声質問文一つ付き．質問文ごとに，10個の回答文がアノテーションされている．</p><p>・従来のVQAデータセットほぼ人工設定により作成された方が多く，また現実環境の盲人ユーザを対象に“goal oriented”なVQAデータセット未だにない．そこで，盲人がカメラにより周囲環境を撮影し，環境を理解することを目的にして，盲人ユーザにより集められた画像及び質問文のデータセットを構築した．</p><p>・ 盲人ユーザにより撮影されたのでVizWizは画像の質が良くなく，又質問文が音声情報なので，はっきり発音が取れない場合などの問題点がある．提案データセットで現状のVQAモデルで検証した結果，性能が従来のデータセットで検証した性能より劣るので， VizWizが将来的の盲人のためのVQA応用に新たな挑戦を提出した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/VizWiz.png" alt="VizWiz"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・初めての盲人により撮影及び質問したVQAデータセット．</p><p>・従来のVQAデータセットと比べ，もっと画像の周りの環境に関する質問文が多い．</p><p>・従来のVQAデータセットとの質問文の詳細的な特徴比べも行っている．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・盲人のためのVQAシステム構築に有力なデータセット．</p><ul><li><a href="https://arxiv.org/pdf/1802.08218.pdf">論文</a></li></ul></div></div><div class="slide_index">[#267]</div><div class="timestamp">2018.5.8 14:33:52</div></div></section><section id="Glimpse_Clouds_Human_Activity_Recognition_from_Unstructured_Feature_Points"><div class="paper-abstract"><div class="title">Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points</div><div class="info"><div class="authors">F. Baradel et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">RNNベースの行動認識を提案．
学習はRGB-Dを使うが，テスト時にはRGBのみを使うという設定．
テスト時にRGB-Dが使えてPose情報が使えればそれを使えばいいが，
それが使えないときもあるからそれに変わる手法を提案するという主張．
Poseでの間接位置に代わって，
Attentionベースでフレーム中から重要な局所要素 (Glimpse) を抽出＆トラッキング．
Glimpseの集合に基いて行動を認識するというフレームワーク．
Glimpseの抽出やトラッキングはそれぞれRNNベースで行う手法になっている．
</div></div><div class="item2"><img src="slides/figs/Glimpse_Clouds_Human_Activity_Recognition_from_Unstructured_Feature_Points.png" alt="Glimpse_Clouds_Human_Activity_Recognition_from_Unstructured_Feature_Points.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>姿勢の代わりに別の局所要素を使うフレームワークを提案</li><li>Attention, External Memoryといった流行り?の要素が詰め込んである</li><li>RGB-D行動認識データセットにおいてRGBのみの利用でSOTAを達成</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://perso.liris.cnrs.fr/christian.wolf/papers/cvpr2018.pdf">論文（著者版）</a></li><li><a href="https://arxiv.org/abs/1802.07898">論文 (Long-ver., arXiv)</a></li><li><a href="https://www.youtube.com/watch?v=7yPDYYhaYI4">動画 (YouTube)</a></li><li>姿勢ベースの行動認識を姿勢を使わずにやるような話に近い印象</li></ul></div></div><div class="slide_index">[#268]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="High-Resolution_Image_Synthesis_and_Semantic_Manipulation_with_Conditional_GANs"><div class="paper-abstract"><div class="title">High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs</div><div class="info"><div class="authors">Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>GANの枠組みにてセマンティックラベルからの高精細画像（HD-Image）生成に関する研究。意味ラベルからリアルな画像を生成するのみならず、インタラクティブな操作で画像生成をコントロールすることも可能。Residual blocksにより構成されるエンコーダ/デコーダ構造を（入力をスケールが異なる画像として）入れ子構造にしデコーダ直前の中間層で統合して画像生成を実行する。さらに、ラベルのみならずインスタンスレベルの特徴量を用いることで写実性が向上したと主張（論文中図4では物体境界面あたりに出ているボケが綺麗になっている）。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180508HDImageGAN.png" alt="180508HDImageGAN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来法より、見た目の画像生成が明らかに良くなり、高画質の画像を対象にしても画像生成ができるようになった。従来手法（pix2pix（論文中文献21）, CRN（論文中文献5））さらに、インタラクティブな操作により生成画像を所望の結果に近づけることができる。動画像を見れば従来手法よりも鮮明になっていることは明らかであり、アーキテクチャや生成に関する知見も得ている。CVPRでoralになるための準備やプレゼンが論文中にも書かれていると感じた。やはりNVIDIAはずるいと言われるくらいの計算機環境が揃っているのではないか。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>これはもう、学習画像として使えるのでは？（すでにだれか使って精度検証しているのでは？）</p><ul><li><a href="https://arxiv.org/abs/1711.11585">論文</a></li><li><a href="https://tcwang0509.github.io/pix2pixHD/">Project</a></li><li><a href="https://github.com/NVIDIA/pix2pixHD">GitHub</a></li><li><a href="https://github.com/arXivTimes/arXivTimes/issues/544">arXivTimes</a></li><li><a href="https://www.youtube.com/watch?v=3AIpPlzM_qs">YouTube</a></li></ul></div></div><div class="slide_index">[#269]</div><div class="timestamp">2018.5.8 12:46:17</div></div></section><section id="Five-point_Fundamental_Matrix_Estimation_for_Uncalibrated_Cameras"><div class="paper-abstract"><div class="title">Five-point Fundamental Matrix Estimation for Uncalibrated Cameras</div><div class="info"><div class="authors">D. Barath</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>2つの未キャリブレーションカメラにおいて，<strong>5点のみ</strong>で基礎行列を推定する手法を提案．</p><p>回転不変な特徴点（SIFT等）を使う．3点は平面にあれば，他2点はどこでも可能．グラフカットRANSACのようなロバスト対応点推定と組み合わせれば，state-of-the-artな性能が出る．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Five-point_Fundamental_Matrix_Estimation_for_Uncalibrated_Cameras_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>通常，7点や8点取るアルゴリズムが用いられるが，リーズナブルな制約で，少ない情報のみでキャリブレーションできるのはうれしい．例えば図のようにキャリブレーションボードを小さくできたりする．
大変有用な研究成果．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.00260">arXiv</a></li></ul></div></div><div class="slide_index">[#270]</div><div class="timestamp">2018.5.8 12:37:50</div></div></section><section id="Defense_against_Adversarial_Attacks_Using_High-Level_Representation_Guided_Denoiser"><div class="paper-abstract"><div class="title">Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser</div><div class="info"><div class="authors">Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Jun Zhu and Xiaolin Hu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kodai Nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>画像分類におけるadrversarial attackの防御手法として, high-level representation guided denoiser (HGD) を提案.target model (メインの処理を担うネットワーク) への前処理段階で用いる.
HGDは, マルチスケールインフォメーションを得るためU-netの構造を使い,
トレーニングするための損失関数として, 元画像とノイズの乗った画像をそれぞれ入力したときの出力差を用いる.
右図に提案手法の詳細を示す.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/defence_against_adversarial_attacks_using_high-level_representation_guided_denoiser.png" alt="defence_against_adversarial_attacks_using_high-level_representation_guided_denoiser.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>pixel-levelの損失関数を課した従来のdenoiserと比べ, より良い結果が得られた.</p><p>state-of-the-artな防御手法であるensemble adversarial trainingと比べ, 3つのメリットがある.</p><ol><li>target modelがwhite-boxとblack-boxの両方に対してよりロバスト.</li><li>大規模データセットでの学習が簡単.</li><li>他のtarget modelへ使い回すことが可能.</li></ol></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.02976.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#271]</div><div class="timestamp">2018.5.8 12:24:05</div></div></section><section id="Customized_Image_Narrative_Generation_via_Interactive_Visual_Question_Generation_and_Answering"><div class="paper-abstract"><div class="title">Customized Image Narrative Generation via Interactive Visual Question Generation and Answering</div><div class="info"><div class="authors">Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada</div><div class="conference">CVPR 2018</div><div class="paper_id">1224</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・新規の“Customized画像説明文生成”タスクを提案した．また，インタラクティブにユーザに自動的に画像に関する質問をし，回答文を収集できるような仕組みを提案した．・従来の画像説明文生成タスクにおいて，異なるユーザの性質や画像の注目領域などにより，多様な説明文を生成できることが検討されていない．このような性質に応じて，多様な質問文を生成できる仕組み及びユーザとインターアクションしユーザの個性的な回答文を収集しユーザの特徴を学習することにより，Customizedで画像説明文を生成できる仕組みを提案した．
・提案仕組みは具体的に：①画像から self Q&A modelにより，画像中のマルチリジョンを注目し(attention構造を利用した)質問文を生成し， VQAモデルにより回答する(マルチ回答がある質問文だけを保留)；②　①により生成できた質問文をユーザに提示し，回答させる；③画像リジョン・質問文・回答文の統合した画像説明文を生成する．
・画像リジョン・質問文・ユーザ特有な回答文からchoice vectorを抽出し，このベクトルを利用してほかの画像が入力された場合，ユーザの個性的な画像説明文を生成できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Customized_Image_Narrative_Generation.png" alt="Customized_Image_Narrative_Generation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・新規な問題設定“Customized画像説明文生成”・提案手法により，画像からより多様でユーザの個性を含んだ説明文を生成できる．
・ Automatic　Image　Narrative　Generationにおいて，従来のデータセットCOCO, SIND, DenseCapなどと比べ”diversity”,”interesting”,”naturalness”,”expressivity”などの指標に対しパフォーマンスが良い
・ Interactive　Image　Narrative　Generationにおいて，ヒューマンテストで良い評価を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・ユーザの個性を学習できる仕組みは応用場面が広そう</p><ul><li><a href="https://arxiv.org/pdf/1805.00460.pdf">論文</a></li></ul></div></div><div class="slide_index">[#272]</div><div class="timestamp">2018.5.8 12:19:18</div></div></section><section id="First-Person_Hand_Action_Benchmark_with_RGB-D_Videos_and_3D_Hand_Pose_Annotations"><div class="paper-abstract"><div class="title">First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations</div><div class="info"><div class="authors">G Garcia-Hernando et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">一人称視点動画 (RGB-D) データセットの提供．
手（21点の3D間接位置）と物体（6D姿勢）の情報に加えて，45クラスの行動ラベルが付けられている．
データ数は1175シーケンス，10万フレーム．
手の3D姿勢と行動ラベルが付いている一人称視点動画データセットはこれまでになかった．
実験では従来手法やLSTMによるベースライン手法を合わせて18個を比較した結果が議論されており，
手の姿勢情報を使う手法が高い性能を示す傾向があることが確認されている．
</div></div><div class="item2"><img src="slides/figs/First-Person_Hand_Action_Benchmark_with_RGB-D_Videos_and_3D_Hand_Pose_Annotations.png" alt="First-Person_Hand_Action_Benchmark_with_RGB-D_Videos_and_3D_Hand_Pose_Annotations.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>手の3D姿勢を使った行動認識のためのデータセットを提供．</li><li>RGB, Depth, Poseといった様々な特徴を用いる各手法が詳細に議論されている．</li><li>一番良い手法で78%程度の認識率．</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1704.02463">論文 (arXiv)</a></li><li><a href="https://www.youtube.com/watch?v=U5gleNWjz44">動画 (YouTube)</a></li></ul></div></div><div class="slide_index">[#273]</div><div class="timestamp">2018.5.8 12:00:18</div></div></section><section id="PointFusion_Deep_Sensor_Fusion_for_3D_Bounding_Box_Estimation"><div class="paper-abstract"><div class="title">PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation</div><div class="info"><div class="authors">Danfei Xu, dragomir Anguelov, Ashesh Jain</div><div class="conference">CVPR 2018</div><div class="paper_id">50</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・画像と点群情報を利用した3D物体検出のフレームワークPointFusionを提案した．・従来のマルチセンサーの情報を利用した3D物体検出は前処理が必要、マルチセンサーを異なるパイプラインで処理し，他のセンサーのコンテキストをうまく利用できないなどの問題点がある．PointFusionは①異なるネットワーク構造を用いて画像(CNN)と点群情報(PointNet)を直接処理し，②デンスフュージョンネットワーク構造を提案し，画像と点群の抽出情報を統合しより精密な3D物体検出を行う．
・2種類のデンスフュージョンネットワークを提案した．①画像情報及びPointNetにより抽出したグローバル情報を統合し， 3Dボックスのコーナー位置を推定する．②画像情報及びPointNetにより抽出したグローバル情報、ポイントフィーチャーを統合し， 3Dボックスのオフセット及びconfidence scoresを予測する．最後の2つの結果を統合し，最終的な結果を予測する</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/PointFusion.png" alt="PointFusion"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・点群データの前処理が必要無し．・対応できるデータの形式が広い，室外環境と室内環境両方対応できる．
・多様な三次元センサーのデータを対応できる．(RGB-D, LiDar, Radar,…)
・KITTI, SUN-RGBDデータセットにおいてstate-of-the-artな結果</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・室内・外環境両方対応できるので、応用場面が広そう・将来的にend-to-endに実現できたら更に良い</p><ul><li><a href="https://arxiv.org/pdf/1711.10871.pdf">論文</a></li></ul></div></div><div class="slide_index">[#274]</div><div class="timestamp">2018.5.8 10:56:27</div></div></section><section id="Path_Aggregation_Network_for_Instance_Segmentation"><div class="paper-abstract"><div class="title">Path Aggregation Network for Instance Segmentation</div><div class="info"><div class="authors">Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, Jiaya Jia</div><div class="conference">CVPR2018, arXive:1803.01534</div><div class="paper_id">912</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>Feature Pyramid Network(FPN)ベースのMask R-CNNに，下位層の特徴マップを上位層に伝播させるPath Aggregation Networkを提案．インスタンスセグメンテーションの傾向として，上位層では物体全体に強く反応するが，下位層では物体の局所的な領域に強く反応する．
そのため，Path Aggregation Networkでは，上位層と下位層の特徴マップを用いることで，インスタンスセグメンテーションの精度を向上させている．
Path Aggregation Networkは，COCOのベンチマークで2位の性能を達成しており，CityscapeとMVDでも高い性能を達成している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/912_overview.png" alt="912_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Path Aggregation Networkの構造は右図のようなシンプルな構造になっている．(a)の部分はFPNと同様の構造となっており，FPNの特徴マップから(b)で新しい特徴マップを作成する．
ここで，(a)と(b)では，緑線と赤線のように短距離と長距離のショートカットを導入する．
これにより，下位層の特徴を上位層に伝播することが可能である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09020">論文リンク</a></li></ul></div></div><div class="slide_index">[#275]</div><div class="timestamp">2018.5.8 02:54:08</div></div></section><section id="StarGAN_Unified_Generative_Adversarial_Networks_for_Multi_Domain_Image_to_Image_Translation"><div class="paper-abstract"><div class="title">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</div><div class="info"><div class="authors">Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo</div><div class="conference">CVPR2018, arXive:1711.09020</div><div class="paper_id">872</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>1つのネットワークでマルチドメイン対応の画像変換が可能なStarGANを提案．pix2pixやCycleGANの場合，左上図のように特定の1つのドメイン変換しかできないため，複数のドメイン変換をする時には各ドメインを変換するネットワークをそれぞれ構築しなければいけない．
StarGANでは，入力する条件とロス設計を適切に設計することで，シンプルなネットワークで多ドメインな画像変換を実現している．
実験では，顔属性のCelebAと表情のRaFD Datasetを使用し，2つのデータセットでGANを学習して下図のような多様な顔画像変換を可能にしている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/872_overview.png" alt="872_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>StarGANの構造は，右上図のようになっている．ここで，入力はそれぞれのドメインの画像がランダムに入力される．
まず，real imageとfake imageでDiscriminatorを学習する．
そして，次にGeneratorを学習する．
Generatorは，生成したい顔画像の条件とreal imageを入力して，画像変換する．
ここで，変換した画像はDiscriminatorに入力される．
変換した顔画像はCycleGANのようにreal imageを再変換する．
定義するロスは，一般的なAdversarial Loss，ドメインを認識するロス，real imageと再変換したimageのL1 Lossである．
また，複数のデータセットを学習するために，各データセットのラベルとデータセットの情報が格納されたMask vectorを導入している．
これにより，多ドメインかつ複数データセットに対応したGANを構築できている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>多ドメインかつ複数データセットに対応したGAN．変換するドメインの数に依存しないので，非常に用途が広がりそう．</p><ul><li><a href="https://arxiv.org/abs/1711.09020">論文リンク</a></li><li><a href="https://github.com/yunjey/StarGAN">コードリンク</a></li></ul></div></div><div class="slide_index">[#276]</div><div class="timestamp">2018.5.8 01:27:52</div></div></section><section id="Semi-parametric_Image_Synthesis"><div class="paper-abstract"><div class="title">Semi-parametric Image Synthesis</div><div class="info"><div class="authors">Xiaojuan Qi, Qifeng Chen, Jiaya Jia, Vladlen Koltun</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>意味ラベル（Semantic Layout）から写真のようにリアルな画像をSemi-parametricな手法にて生成する。Semi-parametricはNon-parametricとParametricの強みを相補的に適用する手法である。セマンティックセグメンテーションのアノテーションとその対応する画像をペアとした外的なメモリにより対応関係を学習、Canvasとしてその順番や境界面を初期ステップとして出力する。次にCanvasと意味ラベルを入力としてConv-Deconv構造のネットワークにより写真のようにリアルな画像を出力とする。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180507SIMS.png" alt="180507SIMS"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Cityscapes, NYU, ADE20Kデータセットとセマンティックセグメンテーションに関するラベルが付与されていれば学習/テストが可能であり、同データセットにて従来法よりもさらにリアルな画像を生成するに至った。図には従来法（Chen and Koltun, ICCV 2017）との比較があり、従来法ではエッジ付近にボケが生じているが、提案法ではボケを相殺してさらに光の度合いまでもリアルに復元できている。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>意味ラベルから写真を復元することに成功した。今後、さらに生成するアピアランスや配置をコントロールする手法が登場すれば、学習データを無限に増やすことができたり、作りたい写真を再構成することが可能になる。</p><ul><li><a href="http://vladlen.info/papers/SIMS.pdf">論文</a></li><li><a href="http://vladlen.info/publications/sims/">Project</a></li><li><a href="https://github.com/xjqicuhk/SIMS">GitHub</a></li><li><a href="https://www.youtube.com/watch?v=U4Q98lenGLQ">Video</a></li></ul></div></div><div class="slide_index">[#277]</div><div class="timestamp">2018.5.7 13:32:33</div></div></section><section id="Hierarchical_Novelty_Detection_for_Visual_Object_Recognition"><div class="paper-abstract"><div class="title">Hierarchical Novelty Detection for Visual Object Recognition</div><div class="info"><div class="authors">Kibok Lee, Kimin Lee, Kyle Min, Yuting Zhang, Jinwoo Shin, Honglak Lee</div><div class="conference">CVPR 2018</div><div class="paper_id">131</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・最も近いスーパークラスを予測することにより階層的新規(novelty)物体識別及び検出のフレームワークを提案した．・従来，新規なunseen物体識別は”known”と"unknown"に回帰する問題として対応されている．この論文で，物体のクラスを階層的に取り扱い，unseen物体の最も近いスーパークラスを求める．提案フレームワークによりgeneralized zero-shot learningタスクに用いられる階層的エンベディングを得られる．
・2種類の階層的な新規(novelty)物体検出構造を提案した．①top-down構造ではconfidence-calibrated classifierにより物体を分布の一致性が高いスーパークラスに分類する．②flatten構造では階層的分類構造の全体を用いずに error aggregationを避ける単一的なclassifierを用いる．また，①と②を組み合わせすることにより，階層的検出精度を向上できることを示した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/hierarchical_detection.png" alt="hierarchical_detection"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・従来のクローズデータセットを用いた物体検出と比べ，提案手法はオープンデータセットを対応できる．・generalized zero-shot learningタスクで提案フレームワークを用いられる
・ ImageNet, AwA2, CUBなどのデータセットで階層的新規(novelty)物体識別においてベースラインより高い精度を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00722">論文</a></li></ul></div></div><div class="slide_index">[#278]</div><div class="timestamp">2018.5.7 13:08:17</div></div></section><section id="Revisiting_Salient_Object_Detection_Simultaneous_Detection_Ranking_and_Subitizing_of_Multiple_Salient_Objects"><div class="paper-abstract"><div class="title">Revisiting Salient Object Detection: Simultaneous Detection, Ranking, and Subitizing of Multiple Salient Objects</div><div class="info"><div class="authors">Md Amirul Islam, Mahmoud Kalash, Neil D. B. Bruce</div><div class="conference">CVPR 2018</div><div class="paper_id">892</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・マルチsalientオブジェクトおよびそれぞれのsalientランキングを同時に検出するネットワークを提案した．・従来のsalientオブジェクトタスクに，salientランキングは観測者によって異なる結果が出る性質があるため，オブジェクトのsalientランキングについてまだ検討されていない．この文章でsalientランキングを有効的に得られるネットワークを提案した．またsalientランキング手法の評価方法も提案した．
・具体的なネットワーク構造はまずencoderネットワークにより粗末な相対salientスタックを生成し，そしてStacked Convolutional Module (SCM)により粗末なsaliency mapを生成する．またrank-awareでstage-wiseなネットワークによりsalientスタックをリファインする．ヒュージョンレイヤーにより各stageのsaliency mapを統合する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Revisiting-Salient-Object-Detection.png" alt="Revisiting-Salient-Object-Detection"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・saliency ランキングの提案・AUC, max　F-measure, median F-measure, average F-measure,MAE, and SORなどの
評価方法により，state-of-the-artなsalientオブジェクト検出性能を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.05082">論文</a></li></ul></div></div><div class="slide_index">[#279]</div><div class="timestamp">2018.5.7 12:45:59</div></div></section><section id="Rethinking_the_Faster_R-CNN_Architecture_for_Temporal_Action_Localization"><div class="paper-abstract"><div class="title">Rethinking the Faster R-CNN Architecture for Temporal Action Localization</div><div class="info"><div class="authors">Y. Chao et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">動画中の行動のラベル，開始・終了時刻を推定するTemporal Action Localizationの研究．
Faster R-CNNによる物体検出をベースにLocalizationをする．
ここで，スケールのバリエーションが非常に大きい，前後の行動などのコンテキストが重要，
RGBとFlowをどう統合するか，といった3点の検討が重要としてこれらに取り組んでいる．
提案手法であるTAL-Netのポイントとしては，
アンカーごとに適切なスケールの受容野を持つ異なるCNNを組み合わせて利用している点．
各問題に対する設計がそれぞれ精度向上に寄与している点を実験から確認し，
THUMOS'14でのSOTAを達成．
</div></div><div class="item2"><img src="slides/figs/Rethinking_the_Faster_R-CNN_Architecture_for_Temporal_Action_Localization.png" alt="Rethinking_the_Faster_R-CNN_Architecture_for_Temporal_Action_Localization.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>行動の時間スケールについての検討をちゃんと行った点は新規性がある</li><li>提案手法の各要素についての実験がされていて，それぞれによる精度向上を確認できている</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1804.07667">論文 (arXiv)</a></li><li>目新しいアイデアはないように思うが，問題点に対する解法を検討してかっちりと評価している</li><li>この辺りのスケールの話は大事そうなのにこれまで意外とちゃんとやられてきてなかったところ</li></ul></div></div><div class="slide_index">[#280]</div><div class="timestamp">2018.5.7 12:44:45</div></div></section><section id="PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume"><div class="paper-abstract"><div class="title">PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume</div><div class="info"><div class="authors">Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>コンパクトかつ効果的なオプティカルフロー推定を実現するPWC-Netを提案する。ピラミッド構造かつ学習可能な階層的処理、射影（Warping）、コストボリュームにより設計され、軽量化しながら高精度なフロー推定を実現している。図は従来法（左図）と提案法（右図）のアーキテクチャの概略を示している。従来は画像のピラミッド構造により全てのサイズを階層的にオプティカルフローの射影や最適化を行い、最後に後処理をしていたが、提案法のPWCNetではあるひとつの階層内で後処理を行い、コンテキストを考慮したネットワーク（ContextNetwork; Dilated Convによる、各階層のオプティカルフローを入力するとそれらを総合的に解釈して最良のオプティカルフローを出力する）を通り抜けることで出力する。間には{Warping, Cont Volume, Optical flow}を行う層により構成される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180507PWCNet.png" alt="180507PWCNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>従来法であるFlowNet2よりも17分の1の軽量化モデルでありながら、MPI Sintel final pass/KITTI 2015 BenchmarkにてState-of-the-art、Sintel 1024x436の解像度にて35fpsで動作する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>オプティカルフロー/距離画像の推定など、RGBの入力から異なるチャンネルを出力する課題が登場して本論文のように精度向上やコンパクト化、処理速度向上が著しい。ただし、出力したオプティカルフローや距離画像の出力自体の正当性を保証するような評価方法が必要？特に、異なるドメイン（ドイツの道路データで学習して日本の道路データでテストするなど）での適応とその性能保証は欲しいところ。</li><li>（さすがNVIDIA！？）実験量がとても多く見える。Table1~7までびっしり実験結果が埋められている。</li><li><a href="http://xiaodongyang.org/publications/papers/pwc-cvpr18.pdf">論文</a></li><li><a href="http://research.nvidia.com/publication/2018-02_PWC-Net%3A-CNNs-for">Project</a></li><li><a href="https://github.com/deqings/PWC-Net">GitHub</a></li></ul></div></div><div class="slide_index">[#281]</div><div class="timestamp">2018.5.7 12:26:54</div></div></section><section id="LEGO_Learning_Edge_with_Geometry_all_at_Once_by_Watching_Videos"><div class="paper-abstract"><div class="title">LEGO: Learning Edge with Geometry all at Once by Watching Videos</div><div class="info"><div class="authors">Z. Yang et al.,</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><h1>概要</h1><div class="text">ラベルなし動画からの3次元幾何 (Depth, Normal) の推定．
従来研究のものだと画素ごとの誤差で最適化していたのでボケた幾何構造推定になっていたのが問題と主張．
提案手法はエッジと3次元幾何を同時に推定して最適化することで，左図 (f) のような正確な幾何構造を推定可能にした．
ベースは従来手法同様で，カメラ姿勢を推定し，それに基づくWarping結果と元のフレームとの間の誤差をとって最適化．
これに，エッジ推定と3D-ASAP (as smooth as possible in 3D) Priorを導入したところがポイント．
3D-ASAPはある2点間の間にエッジがなければその2点は同一平面上にあるという仮定に基づく提案手法．</div></div><div class="item2"><img src="slides/figs/LEGO_Learning_Edge_with_Geometry_all_at_Once_by_Watching_Videos.png" alt="LEGO_Learning_Edge_with_Geometry_all_at_Once_by_Watching_Videos.png"></div><div class="item3"><h1>新規性・結果・なぜ通ったか？</h1><div class="text"><ul><li>3次元幾何とエッジ推定を同時にする手法の提案</li><li>3D-ASAP Priorの定式化とそれによる精度向上を実現</li><li>KITTIやCityScapesでのSOTAを達成</li></ul></div></div><div class="item4"><h1>コメント・リンク集</h1><div class="text"><ul><li><a href="https://arxiv.org/abs/1803.05648">論文 (arXiv)</a></li><li><a href="https://www.youtube.com/watch?v=40-GAgdUwI0">結果動画 (YouTube)</a></li></ul></div></div><div class="slide_index">[#282]</div><div class="timestamp">2018.5.7 11:15:05</div></div></section><section id="DA-GAN_Instance-level_Image_Translation_by_Deep_Attention_Generative_Adversarial_Network"><div class="paper-abstract"><div class="title">DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Network</div><div class="info"><div class="authors">Shuang Ma, Jianlong Fu, Chang Chen, Tao Mei</div><div class="conference">CVPR 2018</div><div class="paper_id">695</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・無監督インスタンスレベルのattentionを用いたImage Translationフレームワークを提案した．・従来の無監督Image Translationではセットレベルで実現され，物体パーツレベルの対応ができないため，従来手法より生成した物体画像が幾何や意味的な情報のリアル性が低い場合がある．それと比べ，提案フレームワークは①物体をはattentionを用いた高構造化latent空間に変換し，このlatent空間によりインスタンスレベルなImage Translationを可能にした．②さらに，source samplesとtranslated samplesをセマンティック的に対応させるconsistency lossを提案した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DA-GAN.png" alt="DA-GAN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・初めてattentionをGANに導入したと宣言・MNIST , CUB-200-2011, SVHN , FaceScrub and AnimePlanet 1などのデータセットを用いて実験を行い，ドメンadaption，テキスト-画像合成，ポーズモーフィング，顔‐アニメーション化などのタスクにおいて，state-of-the-artな精度を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・attentionをGANに導入し，さらに精密で構造化した画像生成ができるので，様々なアプリで応用できそう</p><ul><li><a href="https://arxiv.org/pdf/1802.06454.pdf">論文</a></li></ul></div></div><div class="slide_index">[#283]</div><div class="timestamp">2018.5.7 10:19:19</div></div></section><section id="PhaseNet_for_Video_Frame_Interpolation"><div class="paper-abstract"><div class="title">PhaseNet for Video Frame Interpolation</div><div class="info"><div class="authors">Simone Meyer, et al.</div><div class="paper_id">1804.00884</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>様々なシーンに頑健かつ、大きな動きにも対処しながらビデオフレームの補間を行うPhaseNetの提案。中間のフレームにおける位相と階層構造を推定するnnのデコーダを搭載。これにより、既存の位相ベースの手法よりも広範囲に渡る動きに対応。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180507PhaseNet.jpg" alt="180507PhaseNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>既存のビデオフレーム補間アプローチは、フレーム間において密な対応付けが必要であり、照明変化や被写体ブレに頑健でない。カーネルに依存した深層学習ベースの手法でもある程度緩和することはできるが不十分。ピクセル単位の位相ベースの手法ならば上手くいくことが実装されている。位相ベースでnnを用いた手法を提案。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>位相のlossとノルムを組み合わせることで、チャレンジングなシーンでも視覚的に綺麗な画像を生成できる。</p><ul><li><a href="https://arxiv.org/pdf/1804.00884.pdf">論文</a></li></ul></div></div><div class="slide_index">[#284]</div></div></section><section id="Multi-scale_Location-aware_Kernel_Representation_for_Object_Detection"><div class="paper-abstract"><div class="title">Multi-scale Location-aware Kernel Representation for Object Detection</div><div class="info"><div class="authors">Hao Wang, Qilong Wang, Mingqi Gao, Peihua Li and Wangmeng Zuo</div><div class="conference">CVPR2018</div><div class="paper_id">153</div></div><div class="slide_editor">Ryosuke Araki</div><div class="item1"><div class="text"><h1>概要</h1><p>物体検出時に特徴量の高次の統計量（high-order statistics）を獲得するためのMulti-scale Location-aware Kernel Representation（MLKP)を提案する．MLKPはSSDで用いるような，複数解像度の特徴マップを結合したマルチスケール特徴マップを用いて効果的に計算できる．マルチスケール特徴マップをMLKPに入力すると，畳み込みと要素ごとの積算を行いr次の表現Z^rを得る．このとき，location-weight networkは各位置の寄与度を学習する．その後，各次の表現を重みつき結合し，RoI Poolingへ入力する．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180507_mlkp1.jpg" alt="20180507_mlkp1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>最近の分類メソッドでよく用いられる高次統計量を物体検出器の高精度化に用いる手法である．Faster R-CNNにMLKPを統合することで，Faster R-CNNよりも精度が4.9%(mAP, VOC2007），4.7%（mAP, VOC2012），5.0%（MSCOCO）向上した．DSSDやR-FCNと比較しても同等もしくはそれ以上の性能である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>流行りのマルチスケール手法をR-CNNに昇華した感じ．R-CNNベースの手法もまだまだ煮詰める余地は十分ある．</p><ul><li><a href="https://arxiv.org/abs/1804.00428">arXiv</a></li><li><a href="https://github.com/Hwang64/MLKP">コード</a>py-faster-rcnnをベースにされている．マルチGPU版もあり</li></ul></div></div><div class="slide_index">[#285]</div><div class="timestamp">2018.5.7 01:24:41</div></div></section><section id="Self_supervised_Learning_of_Geometrically_Stable_Features_Through_Probabilistic_Introspection"><div class="paper-abstract"><div class="title">Self-supervised Learning of Geometrically Stable Features Through Probabilistic Introspection</div><div class="info"><div class="authors">David Novotny et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>幾何学変換を利用したGeometrically Stable な特徴表現の獲得手法。オリジナル画像とそれに幾何学変換を施した画像を同じCNNに学習し、中間特徴マップ上で対応するpixelでの特徴量の類似度が高くなるように学習する。キーポイントマッチングなどの問題設定で教師あり学習以上の効果を発揮。Pixelによってはマッチングが困難ば場合も存在するため、不確実性を考慮した学習を提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Novotny.png" height="1000"></p></div></div><div class="item3"><div class="text"><h1>手法・新規性</h1><p>ペアとなる画像を同じNNに入力し、各pixel ペアの類似度と、不確実性を表す値を算出。不確実性を考慮した損失関数を定義することで、結果的にNNはマッチング可能かつ対応するpixelに関しては高い類似度と低い不確実性を、マッチングが困難なものに関しては高い不確実性を算出するように学習される。</p></div></div><div class="item4"><div class="text"><h1>メモ・リンク</h1><p>定義された距離尺度において対象に直接近づける枠組みが多い通常の類似度学習と異なり、連続値である類似度を確率変数とすることで、不確実性を考慮するのは興味深い。しかし、定式化としては論文内のものよりも、不確実性利用してモデルが類似度の分布を算出しているという定式化にした方がわかりやすいのではないかと思った。</p><ul><li><a href="https://arxiv.org/abs/1804.01552">論文</a></li></ul></div></div><div class="slide_index">[#286]</div></div></section><section id="Squeeze-and-Excitation_Networks"><div class="paper-abstract"><div class="title">Squeeze-and-Excitation Networks</div><div class="info"><div class="authors">Jie Hu, Li Shen, Gang Sun</div><div class="conference">CVPR2018, arXive:1709.01507</div><div class="paper_id">891</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>Residualモジュール, Inceptionモジュールに対してAttention機構を導入したネットワーク．Squeeze-and-Excitation Networks(SENet)では，生成される特徴マップのチャンネルに対してAttentionを導入している．
SENetは，ImageNetでstate-of-the-artな性能を達成している．(現在1位)
また，Place Datasetでも高い性能を達成している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/891_overview.png" alt="891_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>SENetには，右図のように2つのモジュールが提案されている．SE Inception moduleは，VGGやAlexNet等の順伝播ネットワークで使われるSEモジュール．
SE Residual moduleは，ResNet系のネットワークに使われるSEモジュールである．
基本的には，Global Average Poolingを施した後に，全結合層を何層か通してチャンネル毎のAttentionを生成する．
この構造は，ResNet等の様々なネットワークモデルにも適応できる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Attention機構を導入した物体認識法．最近，物体認識にもAttentionが流行し始めているので，その先駆けな手法になりそう．
学習モデルもGitHub上で公開．</p><ul><li><a href="https://arxiv.org/abs/1709.01507">論文リンク</a></li><li><a href="https://github.com/hujie-frank/SENet">コードリンク</a></li></ul></div></div><div class="slide_index">[#287]</div><div class="timestamp">2018.5.6 23:46:46</div></div></section><section id="ClusterNet_Detecting_Small_Objects_in_Large_Scenes_by_Exploiting_Spatio-Temporal_Information"><div class="paper-abstract"><div class="title">ClusterNet: Detecting Small Objects in Large Scenes by Exploiting Spatio-Temporal Information</div><div class="info"><div class="authors">Rodney LaLonde, Dong Zhang, Mubarak Shah</div><div class="paper_id">1704.02694</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>1平方キロメートル以上の広範囲の領域を撮影できるWide Area Motion Imagery(WAMI)の映像から、車などの小さい物体を検出する手法の提案。まず、ClusterNetでビデオフレームから、CNNを使って動きと外観情報を結合し、regions of objects of interest(ROOBI)を出力。次に、FoceaNetによって、ヒートマップ推定を介して、ROOBI内の物体の重心位置を推定する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506ClusterNet.jpg" alt="180506ClusterNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>WAMIを使った従来の物体検出は、アピアランスベースの分類器であまり精度が出ず、背景差分やフレーム間差分などの動き情報に依存しがち。Fast R-CNNなどにおけるこれらの問題を検証し、効率的かつ効果的な新たな2ステージCNNを提案。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>広範囲の情報から数百の物体を同時に検出する。他の手法では扱えない停止車両なども検出できる。</p><ul><li><a href="https://arxiv.org/pdf/1704.02694.pdf">論文</a></li><li><a href="https://www.tno.nl/en/focus-areas/defence-safety-security/roadmaps/information-sensor-systems/wide-area-motion-imagery-wami/">WAMI</a></li></ul></div></div><div class="slide_index">[#288]</div></div></section><section id="An_Analysis_of_Scale_Invariance_in_Object_Detection-SNIP"><div class="paper-abstract"><div class="title">An Analysis of Scale Invariance in Object Detection – SNIP</div><div class="info"><div class="authors">Bharat Singh, Larry S. Davis</div><div class="paper_id">1711.08189</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>極端なスケール変化に頑健な物体検出手法であるSNIPの提案。物体検出において、大きな物体と小さな物体をそれぞれ検出することは困難。そこで、学習時に異なるサイズの物体における勾配を、選択して逆伝播する。物体の幅広いスペクトルに対処し、ドメインシフトを低減する。ピラミッド型のネットワークとなっており、end-to-end学習可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506SNIP.jpg" alt="180506SNIP.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>まず、現代の物体検出手法の欠点として、スケール変化について解析している。小さい物体を検出するために“アップサンプリング画像が必要か”などを、ImageNetを使ってパフォーマンスを評価。これらの解析に基づいてSNIPを開発。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>COCO2017 challengeにおける、最優秀学生応募作品。</p><ul><li><a href="https://arxiv.org/pdf/1711.08189.pdf">論文</a></li><li><a href="http://bit.ly/2yXVg4c">コード</a></li></ul></div></div><div class="slide_index">[#289]</div></div></section><section id="The_iNaturalist_Species_Classification_and_Detection_Dataset"><div class="paper-abstract"><div class="title">The iNaturalist Species Classification and Detection Dataset</div><div class="info"><div class="authors">Grant Van Horn, el al. </div><div class="paper_id">1707.06642</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>自然界にける、“写真に写り易さ”を考慮した画像分類・検出タスク用データセットの提案。5000種類以上の植物や動物からの85万9000の画像で構成。世界各地の多種多様な種やシチュエーションで撮影され、様々なカメラタイプで収集することで画質の変化し、クラスの均衡が大きい。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506iNaturalist.jpg" alt="180506iNaturalist.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来の画像分類・検出用データセットでは、カテゴリごとに画像数が統一されている傾向にある。しかし，写真に収め易い種と、そうでない種があるため、自然界はとても不均衡。この差に着目し、現実世界の状況に近い状況で分類・検出に挑戦するデータセットを提案した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>やはり既存の手法では精度を出すのは難しそう。このデータセットで精度を出すチャレンジングな研究をするのはアリ。</p><ul><li><a href="https://arxiv.org/abs/1707.06642">論文</a></li></ul></div></div><div class="slide_index">[#290]</div></div></section><section id="Between-class_Learning_for_Image_Classification"><div class="paper-abstract"><div class="title">Between-class Learning for Image Classification</div><div class="info"><div class="authors">Yuji Tokozume, Yoshitaka Ushiku and Tatsuya Harada</div><div class="paper_id">1711.10284</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>Between-Class learning(BC learn)という画像分類タスクにおける新学習方法の提案。まず、異なるクラスの2枚の画像をランダムな比率で混合したbetween-class imageを作成。そして、画像を波形として扱うためにミキシングを行う。混合画像をモデルに入力し、学習することで混合した比率を出力する。これにより、特徴分布の形状に制約をかけることができるため、汎化性能が向上する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506Between-class.jpg" alt="180506Between-class.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>もともとは、混合できるデジタル音声のために開発された手法。CNNは“画像を波形として扱っている”という説から、本手法を提案。2つの画像を混合する意味に疑問はあるが、実際にパフォーマンスが向上している。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>混合とミキシングの提案手法によって分類精度が向上。画像の混合にどんな意味があるのかを解明してほしい。</p><ul><li><a href="https://arxiv.org/abs/1711.10284">論文</a></li></ul></div></div><div class="slide_index">[#291]</div></div></section><section id="CleanNet_Transfer_Learning_for_Scalable_Image_Classifier_Training_with_Label_Noise"><div class="paper-abstract"><div class="title">CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise</div><div class="info"><div class="authors">Kuang-Huei Lee, Xiaodong He, Lei Zhang and Linjun Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルノイズを使って、画像分類モデルを学習するCleanNetの提案。人間による“ラベルノイズの低減”という作業を低減する。事前知識として人の手で分類されたクラスの一部の情報だけを使い、ラベルノイズを他のクラスに移すことができる。また、CleanNetとCNNによるクラス分類ネットワークを1つのフレームワークとして統合。ラベルノイズ検出タスクと、統合した画像分類タスクの両方で、ノイジーなデータセットを使って精度検証。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180506CleanNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>人間がラベルのアノテーションをすると時間がかかり、学習はスケーラブルじゃない。逆に人間に頼らない手法はスケーラブルだが、有効性が低い。少し人間に頼って、あとは自動的にノイズ除去をするというハイブリットな手法。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>弱教師付き学習と比較して、ノイズを41%低減。画像分類タスクにおいて、47%パフォーマンスが向上。</p><ul><li><a href="https://arxiv.org/abs/1711.07131">論文</a></li></ul></div></div><div class="slide_index">[#292]</div></div></section><section id="Super-Resolving_Very_Low-Resolution_Face_Images_with_Supplementary_Attributes"><div class="paper-abstract"><div class="title">Super-Resolving Very Low-Resolution Face Images with Supplementary Attributes</div><div class="info"><div class="authors">Xin Yu, Basura Fernando, Richard Hartley, Faith Porikli</div><div class="conference">CVPR 2018 Poster</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>顔画像のアトリビュートを使用することでGTとなる高解像度画像(HR)を使用せずに低解像度画像(LR)を超解像度化する研究。LRとともに顔のアトリビュートも入力として使用することで超解像化における曖昧さを解決。
ネットワークの大枠はGANを採用。
ジェネレータにおいてLRをauto encoderに噛ませる際にエンコードされた特徴量にアトリビュートを付け足してでコードを行う。
ディスクリミネータはGTのHR画像なら1を、ジェネレータによる画像or画像にアトリビュートが含まれていないと判断した際には0を返す。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180506Super-Resolving_Very_Low-Resolution_Face_Images_with_Supplementary_Attributes.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>入力は16x16画像、出力は入力画像が128x128に超解像度化された画像。</li><li>PSNR、SSIMを評価指標として既存手法と比べたところもっとも良い精度を得た。</li><li>既存手法で入力されたLRに対して一意的なHRのみしか出力することができなかった。一方提案手法では入力するアトリビュートに伴って出力するHRの見た目を変更することが可能。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>トレーニングで使用したデータセットはCelebAであり、使用したアトリビュートはCelebAに付属する40種類のうちからgender, ageなど18種類。</li><li><a href="https://basurafernando.github.io/papers/XinYuCVPR18.pdf">論文</a></li></ul></div></div><div class="slide_index">[#293]</div></div></section><section id="Single-Shot_Object_Detection_with_Enriched_Semantics"><div class="paper-abstract"><div class="title">Single-Shot Object Detection with Enriched Semantics</div><div class="info"><div class="authors">Z.Zhang, S.Qiao, C.Xie, W.Shen, B.Wang and A.L.Yuille</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1712.00433</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>Detection with Enriched Semantics (DES)というシングルショットオブジェクト検出器を提案．セマンティックセグメンテーションブランチとオブジェクト検出ブランチで構成.
セマンティックセグメンテーションブランチとグローバルアクティベーションモジュールによってオブジェクト検出の特徴であるセマンティクスを向上．
既存のSSDなどのシングルショット検出器よりも速度と精度が向上．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Single-Shot_Object_Detection_with_Enriched_Semantics.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>セマンティックセグメンテーションブランチに高レベルのオブジェクト特徴のためのオブジェクト検出特徴チャンネルとオブジェクトクラスとの意味的関係を学習するためのグローバルアクティベーションブロックを加える．</li><li>一般的なシングルショット検出器と比較して大幅に検出精度が向上，</li><li>Titan Xp GPU1台で、31.7 FPSを達成し、R-FCNやResNetベースのSSDよりも高速.</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.00433.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#294]</div><div class="timestamp">2018.5.6 01:42:36</div></div></section><section id="Revisiting_Deep_Intrinsic_Image_Decompositions"><div class="paper-abstract"><div class="title">Revisiting Deep Intrinsic Image Decompositions</div><div class="info"><div class="authors">Qingnan Fan, Jiaolong Yang, Gang Hua, Baoquan Chen, David Wipf</div><div class="conference">CVPR 2018 oral</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>光の反射やシェーディングなどを再計算することで自然画像の分解と再構成（Image Decomposition）を行う問題設定である。従来型の事前情報を陽に与えるフィルタリング手法とは異なり、深層学習による提案手法では（十分なラベル付きデータが存在すれば）画像の内的な情報を効果的に捉えて画像の再構成をより自然に行うことができると主張。この問題を解決するために、２種類のカテゴリに関する問い ー（１）詳細なラベル付きデータ（２）弱教師付き学習により比較的多様なラベル付きデータを学習ー を解決することができる。これにより学習データには詳細なラベル付けを行わず弱い事前知識（Loose Prior Knowledge）のみで大量のサンプルを準備することができる。手法面において、最初は荒く光の反射（Albedo）やシェーディングを推定し、次いでエッジやテクスチャ等を推定できるようにフィルタリングを学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180505DeepIntrinsic.png" alt="180505DeepIntrinsic"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>主要な画像再構成のベンチマークにおいて全てState-of-the-artの（最先端の）結果を達成した。さらに、従来まではデータセットに対してアドホックである（と思われる）が、本論文にて提供するデータや手法はよりオープンかつリアルな問題に対して汎用的に使用できる。弱い事前知識のみでリアルデータを学習できるようにしたことも新規性として挙げられる。CVPRの査読を突破できた理由として、State-of-the-artな精度を全てのデータにて達成したことや、その学習法/アーキテクチャの提案にあると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>光の反射（Albedo）や陰影（shading）を同時に推定できる技術はよりリアルな画像の生成には重要技術なのでどんどん進んで欲しい。</p><ul><li><a href="https://arxiv.org/abs/1701.02965">論文</a></li><li><a href="http://www.cs.toronto.edu/~rgrosse/intrinsic/">MIT Intrinsic Images Dataset</a></li><li><a href="http://sintel.is.tue.mpg.de/">MPI Sintel Flow Dataset</a></li><li><a href="http://opensurfaces.cs.cornell.edu/publications/intrinsic/">Intrinsic Images in the Wild</a></li></ul></div></div><div class="slide_index">[#295]</div><div class="timestamp">2018.5.5 17:36:29</div></div></section><section id="Self-supervised_Multi-level_Face_Model_Learning_for_Monocular_Reconstruction_at_over_250_Hz"><div class="paper-abstract"><div class="title">Self-supervised Multi-level Face Model Learning for Monocular Reconstruction at over 250 Hz</div><div class="info"><div class="authors">Ayush Tewari, Michael Zollhöfer, Pablo Garrido, Florian Bernard, Hyeongwoo Kim, Patrick Perez, Christian Theobalt</div><div class="conference">CVPR 2018 Oral</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>単眼顔画像からリフレクタンス、ジオメトリー、照明情報を推定する研究。トレーニングデータには上記の情報のアノテーションを必要とせず、3D Morpahlbe Modelを使用することで高品質な3Dパラメトリックモデルを生成。
3D Morpahlbe Modelを使用することで高品質な3Dパラメトリックモデルを生成。
テスト時には250Hz以上で実行することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180505Self-supervised_Multi-level_Face_Model_Learning_for_Monocular_Reconstruction_at_over_250_Hz.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>大量のアノテーションが必要という既存手法の問題点を解決</li><li>様々な表情に対応することができ、口髭や化粧も再現することが可能。</li><li>既存のラーニングベースの手法と比較した結果、同等の実行時間でより精度の高いリコンストラクションが可能となった。最適化ベースの手法と比較すると10%ほど精度は落ちるものの、最適化ベースの手法では実行時間が120secかかるが提案手法では4msで実行可能。 </li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>目元やおでこの皺の再現には至っていない</li><li><a href="https://arxiv.org/abs/1712.02859">論文</a></li><li><a href="https://web.stanford.edu/~zollhoef/papers/CVPR18_FaceModel/page.html">Project page</a></li></ul></div></div><div class="slide_index">[#296]</div></div></section><section id="TextureGAN_Controlling_Deep_Image_Synthesis_with_Texture_Patches"><div class="paper-abstract"><div class="title">TextureGAN: Controlling Deep Image Synthesis with Texture Patches</div><div class="info"><div class="authors">W.Xian,  P.Sangkloy, V. Agrawal, A.Raj, J.Lu, C.Fang, F.Yu and J.Hays</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1706.02823</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>ユーザが色，スケッチ，テクスチャから深層画像合成を行うTextureGANを提案．既存手法では，カラーやスケッチによる制御を行っているが今回の手法ではユーザがテクスチャパチをスケッチ上に配置することによってテクスチャによる制御を実現．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/TextureGAN_Controlling_Deep_Image_Synthesis_with_Texture_Patches.png" alt="画像"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>深層画像合成における細かいテクスチャ制御の妥当性を初めて実証</li><li>ユーザが特定のテクスチャをスケッチの境界に「ドラック＆ドロップ」するテクスチャインタフェースの提案.</li><li>生成ネットワークで既存のオブジェクトに見られないテキスチャであった場合でも扱うようにする局所テクスチャロスを定義．</li></ul></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><ul><li><p>TextureGANをローカルテクスチャで制約することにより，テクスチャとスケッチベースの画像合成の効果を実証．</p></li><li><p>別のテクスチャデータベースから抽出されたテクスチャから生成されたスケッチを用いて実験を行い、提案アルゴリズムがユーザコントロールに忠実な妥当な画像を生成されることを確認．</p></li><li><p><a href="https://arxiv.org/pdf/1706.02823.pdf">Paper</a></p></li></ul></div></div><div class="slide_index">[#297]</div><div class="timestamp">2018.5.5 01:54:38</div></div></section><section id="Learning_Deep_Models_for_Face_Anti-Spoofing"><div class="paper-abstract"><div class="title">Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision</div><div class="info"><div class="authors">Yaojie Liu, Amin Jourabloo, Xiaoming Liu</div><div class="conference">CVPR2018 SPOTLIGHT</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>入力された動画が生身の人間によるものか、あるいはそれ以外のspoofing（撮影された動画や顔のプリントなど）を判定する研究。空間的な情報として顔のデプスマップ、時間的な情報としてrPPG（信号のパルス信号）。
CNN-RNNモデルを使用しCNNでデプスマップと顔の特徴量マップを、RNNは各時刻でCNNによって推定された顔の特徴量マップを入力としてrPPGを推定する。
既存研究では様々なパターンのspoofingがあるにも関わらずCNNによるバイナリの識別問題として捉えていたため、CNNの広すぎる空間を学習してしまい結果的に過学習をしてしまっていた。
提案手法では補助的な情報としてデプスマップ、rPPGを使用することで識別精度を向上した。
更に165の被写体に対して様々な照明環境、ポーズ、表情、顔むきごとの動画を収集し、anti-spoofingのためのSiWデータベースを構築した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180504Learning_Deep_Models_for_Face_Anti-Spoofing.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜCVPRに通ったか？</h1><ul><li>提案手法では既存研究のようにバイナリの識別問題とはとらえず、デプスマップとrPPGを使用することで学習したパターンのspoofingを確実に検出できることを目的とした。</li><li>既存研究とAPCER、BPCER、ACER、HTER値における比較を行なった結果、提案手法優位な結果となった。識別精度は約72%、state-of-the-artの研究では約34%。</li><li>165の被写体に対して様々な照明環境、ポーズ、表情、顔むきごとの動画を収集し、anti-spoofingのためのSiWデータベースを構築。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.11097">論文</a></li></ul></div></div><div class="slide_index">[#298]</div></div></section><section id="Adversarially_Learned_One-Class_Classifier_for_Novelty_Detection"><div class="paper-abstract"><div class="title">Adversarially Learned One-Class Classifier for Novelty Detection</div><div class="info"><div class="authors">M.Sabokrou, M.Khalooei, M.Fathy and E.Adeli</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1802.09088</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>1クラス分類の際のノベリティ検出のために2段階のネットワークを構築．1つのネットワークはノベリティの検出をし，もう1つでは，inlierを強化しoutlierを歪ませる．
画像と動画で検証．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Adversarially_Learned_One-Class_Classifier_for_Novelty_Detection.png" alt="概要図"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>1クラス分類のためのend to endネットワークを導入したもの</li><li>GANを用いた手法では学習後に片方のモデルのみが使われるが，今回の手法ではテストの際に両方のモデルを掛け合わせることで効率化を図る</li></ul></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><ul><li>inlierとoutlierの分類は元のクラスのサンプルの決定よりも優れている．</li><li>ノベリティクラスのサンプルが無くても学習し，動画や画像の異常を検知でき，様々なアプリケーションで高いパフォーマンスを示す．</li><li><a href="https://arxiv.org/pdf/1802.09088.pdf">Paper</a></li></ul></div></div><div class="slide_index">[#299]</div><div class="timestamp">2018.5.4 03:50:34</div></div></section><section id="Feature_Space_Transfer_for_Data_Augmentation"><div class="paper-abstract"><div class="title">Feature Space Transfer for Data Augmentation</div><div class="info"><div class="authors">Bo Liu, Mandar Dixit, Roland Kwitt, Nuno Vasconcelos</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像空間上ではなく、特徴空間上でデータ拡張（Data Augmentation）を行う研究である。この課題に対して著者らは特徴空間上で物体姿勢/見え方のバリエーションを多様体として考慮するFeature Transfer Network (FATTEN)を提案。従来の特徴空間上でのデータ拡張とは異なり、提案法であるFATTENはEnd-to-Endでの学習が可能であり、より効果的にデータ拡張を実行可能である。同ネットワークは姿勢やカテゴリの多タスク学習により学習を行う。図は直感的な特徴空間上での挙動を示したもので、Pose/Appearanceにおける特徴空間の動線を把握した上でデータ拡張を行うことができる。One-/Few-shot学習でも効果を発揮し、特にOne-shotでは他を大きく離して優れていることを示した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180504FATTEN.png" alt="180504FATTEN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>新規性としては複数の属性（ここでは姿勢・アピアランス）を同時に考慮しながら特徴空間上でデータ拡張を行える点が新規性としてあげられ、さらに関連研究と異なるのはEnd-to-Endで学習できる点も優れている。直感的にはビューポイントの違いとそれに対応するアピアランスを拡張する形で特徴学習ができていると言える。FATTENを適用しModelNet/SUN-RGBDのデータセットにてデータ拡張を行った結果、はっきりとした精度向上を確認した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>RotationNetとの比較や統合（RotationNet+FATTEN）が気になる。もともとこの論文で扱っている問題に対して精度が高いRotationNetに本論文のデータ拡張手法を使用するとさらに大きく精度向上するのでは？</p><ul><li><a href="https://arxiv.org/abs/1801.04356">論文</a></li><li><a href="https://github.com/kanezaki/rotationnet">RotationNet</a></li></ul></div></div><div class="slide_index">[#300]</div><div class="timestamp">2018.5.4 00:21:12</div></div></section><section id="Deep_Extreme_Cut_From_Extreme_Points_to_Object_Segmentation"><div class="paper-abstract"><div class="title">Deep Extreme Cut: From Extreme Points to Object Segmentation</div><div class="info"><div class="authors">Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, Luc Van Gool</div><div class="conference">CVPR2018, arXiv:1711.09081</div><div class="paper_id">88</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>Extreme pointを用いた物体セマンティックセグメンテーション法．このExtreme pointは，セグメンテーションの上端，下端，右端，左端を使用している．
4つのExtreme pointは，物体の大まかな形状の情報を取り込みながらCNNを学習することができる．
Pascal VOC, COCO, DAVIS2016, DAVIS2017, Grabcutで評価し，どのベンチマークにおいても高い性能を示している．
また，セマンティックセグメンテーションのアノテーションツールとして応用できることも示している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/88_overview.png" alt="88_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>使用するネットワークは，ResNet101をBackboneにしたDeepLab-v2である．提案手法のDeep Extreme Cutでは，Extreme pointを有効的に学習するために，点にガウシガウシアンを施してヒートマップを作成し，そのヒートマップを入力画像のチャンネルに追加している．
この学習方法は，様々なタスクのセグメンテーションに有効であり，セマンティックセグメンテーション，動画のセグメンテーション，インスタンスセグメンテーション，インタラクションセグメンテーションに応用することができる．
また，セグメンテーションのアノテーションツールにも応用でき，従来のアノテーションコストを10分の1まで削減できていることを示している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09081">論文リンク</a></li><li><a href="http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr/">プロジェクト＆コードリンク</a></li></ul></div></div><div class="slide_index">[#301]</div><div class="timestamp">2018.5.3 23:45:06</div></div></section><section id="Detail-Preserving_Pooling_in_Deep_Networks"><div class="paper-abstract"><div class="title">Detail-Preserving Pooling in Deep Networks</div><div class="info"><div class="authors">Faraz Saeedan, Nicolas Weber, Michael Goesele, Stefan Roth</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>徐々にダウンサイジングしながらも詳細な情報は保持するという問題設定を解決するDNN、特に微分可能なプーリング手法であるDetail-Preserving Pooling（DPP）を提案する。同ネットワークでは隠れ層にて徐々にダウンスケールを行う。図にはフローチャートが示されている。このように線形ダウンスケーリングを施した画像に対して、出力が情報量をできる限り失わないように学習できるプーリングを提案することで任意の畳み込みネットに対して性能向上を見込める手法とした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180503DPP.png" alt="180503DPP"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>データセットにより最良なプーリングの手法が異なるという欠点を解決するべくDPPを提案した。また、グラフィクスの分野にて提案されているDPID（文献31）を参考にして微分可能（学習可能）なプーリング手法を提案した。このようにして作成されたプーリングはあらゆるネットワークに対し有効にフィットし、（max/average poolingなどより）精度向上を保証すると主張した。例として単純にResNet-101のアーキテクチャのプーリングを置き換えてもCIFAR10にてエラー率が下がっている。このように学習可能であり、汎用的に使用できて高精度が期待できるプーリング手法を提案したことが採択された理由であると考える。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>本手法が汎用的に使用できるのであれば、早い段階でDLフレームワーク（e.g. PyTorch, TensorFlow）などに実装されて使用されるかも？実装面の難しさがどの程度あるか次第か。</p><ul><li><a href="https://arxiv.org/abs/1804.04076">論文</a></li><li><a href="www">Project</a></li><li><a href="https://github.com/visinf/dpp">GitHub</a></li></ul></div></div><div class="slide_index">[#302]</div><div class="timestamp">2018.5.3 23:36:27</div></div></section><section id="Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations"><div class="paper-abstract"><div class="title">Learning a Single Convolutional Super-Resolution Network for Multiple Degradations</div><div class="info"><div class="authors">Kai Zhang, Wangmeng Zuo and Lei Zhang</div><div class="conference">CVPR2018</div></div><div class="slide_editor">kodai nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>従来の単一画像の超解像手法では，低解像度の画像は，高解像度の画像からのバイキュービック的にダウンサンプリングされたものであるという仮定を置いている．そのため，この仮定に従わない場合，性能が低下する．さらに，複数の劣化に対処するスケーラビリティーも欠けている．本論文ではこれらの問題に対処するため，畳み込み超解像ネットーワークに低解像度画像とdegradation map（ブラーカーネルとノイズレベルから作成）を入力する方法を提案している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations.PNG" alt="Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations.PNG"></p><p><img src="slides/figs/Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations_1.PNG" alt="Learning_a_Single_Convolutional_Super-Resolution_Network_for_Multiple_Degradations_1.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>畳み込み超解像ネットワークにブラーカーネルやノイズレベルも入力しようとすると，低解像度画像とのサイズの違いによりネットワークの設計が困難になる．本論文では，dimensionality stretcing strategyを導入することによりこの問題を解決した点が新しい．</p><p>劣化されたSet5などのデータセットに対して，従来法や提案手法を適用し，PSNRとSSIMにより評価した結果，提案手法が最も良い結果を示した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.06116.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#303]</div><div class="timestamp">2018.5.3 15:51:30</div></div></section><section id="Super-FAN_Integrated_facial_landmark_localization_and_super-resolution_of_real-world_low_resolution_faces_in_arbitrary_poses_with_GANs"><div class="paper-abstract"><div class="title">Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs</div><div class="info"><div class="authors">Adrian Bulat, Georgios Tzimiropoulos</div><div class="conference">CVPR2018 SPOTLIGHT</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>任意の向きの低解像度顔画像に対して超解像度化する研究。生成された超解像度画像に対してランドマーク推定を同時に行うことで画像の精度が良くなることを主張。顔画像の高解像度化の際にランドマークを特定することは有用であることはすでに示されていたが、低解像度かつ任意の顔向きの際にはランドマークを使用して高解像度化することが難しかった。提案手法ではGANによって低解像度顔画像から超解像度化された顔画像を生成し、生成された顔画像に対してランドマークのヒートマップを推定を推定することでネットワークの学習を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180503Super-FAN.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜCVPRに通ったか？</h1><ul><li>解像度はそれぞれ入力画像が16x16、出力画像が64x64</li><li>生成された顔画像の評価指標としてPSNR、SSIMを、ランドマーク推定の評価指標としてAUCを使用し、 顔向きが30・60・90度の顔画像に対してどちらも既存研究より良い顔画像を生成することが可能となった。</li><li>トレーニングの際に複数のロス関数を提案しているが、各ロス関数ごとの結果に関しても議論を行っている。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1712.02765">論文</a></li></ul></div></div><div class="slide_index">[#304]</div></div></section><section id="Image_Correction_via_Deep_Reciprocating_HDR_Transfromation"><div class="paper-abstract"><div class="title">Image Correction via Deep Reciprocating HDR Transfromation</div><div class="info"><div class="authors">Xin Yang, Ke Xu, Yibing Song, Qiang Zhang, Xiaopeng Wei, Rynson W.H.Lau</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>入力されたLDR画像に対する露光量の調節をend-to-endに行う研究。２つのU-Netを使用し、LDR画像からHDR画像の推定と、推定されたHDR画像からLDRドメインへの変換、という２つ学習によって実現する。LDR画像に内包されている問題として、露光量が少ない箇所ではピクセルが黒く塗りつぶされてしまい、実際のシーンにおける色の推定が難しいという問題がある。そこで、LDR画像から一度HDR画像を生成することで、塗りつぶされた領域を修復する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180503Image_Correction_via_Deep_Reciprocating_HDR_Transfromation.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜCVPRに通ったか？</h1><ul><li>入力LDR画像の露光量が多い部分や少ない部分に対しても適切な画像修復が可能となった。</li><li>同様の問題を扱う最新手法と比較した結果、提案手法優位な結果となった。主な理由としてはHDR画像からLDR画像へ変換する際に画像の局所的な詳細情報を保てていることをあげている。</li><li>定量評価として画像の質を表す数値であるPSNR、SSIM、FSIM、Q-scoeによる評価を行った。</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.04371">論文</a></li><li><a href="https://ybsong00.github.io/cvpr18_imgcorrect/index">Project page</a></li></ul></div></div><div class="slide_index">[#305]</div></div></section><section id="Visual_Question_Answering_with_Memory_Augmented_Networks"><div class="paper-abstract"><div class="title">Visual Question Answering with Memory-Augmented Networks</div><div class="info"><div class="authors">Chao Ma, Chunhua Shen, Anthony Dick, Qi Wu, Peng Wang, Anton van den Hengel, Ian Reid</div><div class="conference">CVPR2018, arXive: 1707.04968</div><div class="paper_id">875</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>学習サンプルに少ないような質問に対しても回答ができるような手法を提案．ベースはMemory-Augmented Network (One-shot learningを導入したMemory Network)であり，記憶ブロックとAttentionの機能により，稀に発生する質問に対しても正確に回答をすることができる．
VQA benchmark datasetとCOCOのVQAタスクで評価し，高い性能を示している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/875_overview.png" alt="875_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>この手法の大まかな構造はMemory-Augmented Networkになっており，特徴抽出部分が質問文と画像特徴である．画像特徴はVGGやResNetの特徴マップを使用しており，質問文はLSTMの特徴ベクトルを使用している．
この2つの特徴ベクトルは結合され，質問と画像特徴の2つのAttentionがそれぞれ与えられてAugmented memoryに格納される．
そして，Augmented memoryを用いて最終的な回答が出力される．
提案手法では，右下図のように，稀に存在する困難な質問に対しても正確な回答を得ることができる．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1707.04968">論文リンク</a></li></ul></div></div><div class="slide_index">[#306]</div><div class="timestamp">2018.5.2 14:29:29</div></div></section><section id="Deep_Layer_Aggregation"><div class="paper-abstract"><div class="title">Deep Layer Aggregation</div><div class="info"><div class="authors">Fisher Yu, Dequan Wang, Evan Shelhamer, Trevor Darrell</div><div class="conference">CVPR2018, arXive: 1707.06484</div><div class="paper_id">272</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>Deep Neural Networkにおける，層間の結合に関して様々な検討を行った論文．従来のネットワーク(ResNet, DenseNet, FCN, U-Net等)のスキップ結合は，”浅い”結合しか適用されていなかった．
この論文では，より”深い” 結合をネットワークに取り入れ，少パラメータかつ高精度なネットワークモデルを構築している．
画像分類をはじめ，様々な認識タスクで実験を行い，高精度化を実現している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/272_overview.png" alt="272_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>この論文では，右図のような4つのモデルを検討している(c~f)．(c)のようにシンプルに特定の層を集約して連鎖的に入力していくモデルから，(d~f)のように様々な層を集約して連鎖的に集約していくモデルを検討しており，上位層と下位層の層を効率的に伝播することで，認識精度を向上させている．
また，(c)と(f)のモデルを組み合わせることで，より性能を向上させることも可能である．
画像分類，Fine-grained Recognition，物体検出，セマンティックセグメンテーションで実験を行っており，全ての認識タスクにおいて高い性能を示している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Deep CNNの次期モデルを検討しているような論文．結局，画像分類，検出，セグメンテーションではスキップ結合が重要であることを再確認できる．</p><ul><li><a href="https://arxiv.org/abs/1707.06484">論文リンク</a></li></ul></div></div><div class="slide_index">[#307]</div><div class="timestamp">2018.5.2 14:05:11</div></div></section><section id="Data_Distillation_Towards_Omni_Supervised_Learning"><div class="paper-abstract"><div class="title">Data Distillation: Towards Omni-Supervised Learning</div><div class="info"><div class="authors">Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari, Kaiming He</div><div class="conference">CVPR2018, arXive: 1712.04440</div><div class="paper_id">536</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベル付きとラベルなしデータを用いることで画像認識の精度を向上させるData Distillationを提案．この手法では，self-trainingとHinton先生のKnowledge distributionをベースに提案されている．
この手法は，インターネット上のラベルなしデータを大量に学習できる．
この論文では，Mask R-CNNによる人のKeypoint検出と，FPNをbackboneにしたFaster R-CNNによる物体検出で高精度化を実現している．
(COCOをラベル付き，Sports-1M statistic framesとCOCO2017unlabel imagesをラベルなしデータとして使用．)</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/536_overview.png" alt="536_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>一般的なラベルなしデータを扱うModel Distillationとは異なり，Data Distillationは1つのteacher modelとstudent modelを用いる．構造としては，1つの画像を複数の単純な変形を加え，それぞれの認識結果を得る．
そして，それぞれの認識結果を統合し，統合した認識結果をラベルとしてstudent modelを学習する．
ここで，学習に使用するラベルは”soft”なラベルではなく，”hard”なラベル．COCOをベースに実験をしており，ラベルなしデータを併用することで人のKeypoint検出と物体検出で高精度化を実現している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>シンプルかつ少量データの学習にも応用できるできるので，今後これをベースにした手法が増えそう．</p><ul><li><a href="https://arxiv.org/abs/1712.04440">論文リンク</a></li></ul></div></div><div class="slide_index">[#308]</div><div class="timestamp">2018.5.2 14:10:01</div></div></section><section id="Actor_and_Observer_Joint_Modeling_of_First_and_Third-Person_Videos"><div class="paper-abstract"><div class="title">Actor and Observer: Joint Modeling of First and Third-Person Videos</div><div class="info"><div class="authors">Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, Karteek Alahari</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>一人称（First Person View; 頭部にカメラを装着して撮影）かつ三人称（Third Person View; 環境に設置したカメラから撮影）の視点から人物行動や操作している物体を撮影したデータセットCharades-Egoを提供する。一人称/三人称視点は互いに対応付けされており、実に157の行動カテゴリ、112人の実演、4,000の動画ペア、全8,000動画を保有するデータベースの構築に成功した。手法の側面ではTripletによる弱教師付き学習（Weakly-supervised Learning）により一人称/三人称から抽出した複数の特徴量を評価する枠組みActorObserverNetを提案する。さらには、三人称から一人称視点への知識転換（Transferring Knowledge）をZero-shot行動認識の枠組みで実行する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180503CharadesEgo.png" alt="180503CharadesEgo"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>一人称/三人称は従来独立に撮影されて、それぞれのデータベースを構築して来たが、ここでは同時解析することにより行動に関するより詳細な考察（e.g. 間接的に行動を観察した方が良い vs. 操作している物体で行動を認識する方が良い）を行えるようにした。また、弱教師付き学習により特徴学習できるActorObserverNetを提案した。CVPRに通った理由はなんといってもデータベース（とそのベンチマーキング）、弱教師付き学習によるものである。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Hollywood in HomesのようにAMT（クラウドソーシング）にてユーザがフリーで使用を許可した動画を収集するのはアリにしている。公開してフリーにしても良い人だけの動画を効率良く集める仕組みが今後流行ってくるか？（ただ日本だと難しいかも？）データベースに対するベンチマーキングは若干少ない印象を受けるが、データベースの意義自体が優れているため査読を突破したと思われる。</p><ul><li><a href="https://arxiv.org/abs/1804.09627">論文</a></li><li><a href="https://www.youtube.com/watch?v=JkBFE2pzJkw&amp;feature=youtu.be">YouTube</a></li><li><a href="http://www.cs.cmu.edu/~gsigurds/">著者</a></li><li><a href="http://allenai.org/plato/charades/">Project/Database</a></li><li><a href="https://github.com/gsig/charades-algorithms">GitHub</a></li></ul></div></div><div class="slide_index">[#309]</div><div class="timestamp">2018.5.3 02:45:18</div></div></section><section id="The_Best_of_Both_Worlds_Combining_CNNs_and_Geometric_Constraints_for_Hierarchical_Motion_Segmentation"><div class="paper-abstract"><div class="title">The Best of Both Worlds: Combining CNNs and Geometric Constraints for Hierarchical Motion Segmentation</div><div class="info"><div class="authors">Pia Bideau et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>モーションセグメンテーションの問題を扱う。従来のモーションセグメンテーションは幾何的制約を設けることで効果的に動作をセグメントして来たが、高次なセグメントに失敗していた。一方でCNNについては従来方とは逆の特性があった。この両者の特性を活かして、両者にとって良いところどり（The Best of Both Worlds）することでモーションセグメンテーションの性能を向上させた。手法は図に示すようにオプティカルフローを用いた剛体の動き推定（Perspective Projection Constraints）、変形可能でより複雑な物体形状を推定できるようCNNによるセマンティックセグメンテーションを実行。物体のモーションモデルを形成するために、SharpMask（論文中文献35）による物体候補も導入し物体に関する知識を導入した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180503MotionSegmentation.png" alt="180503MotionSegmentation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>クラシカルなフローによる剛体モーション推定とCNNによる物体セグメンテーションを統合、両者の良い部分を引き出しているところが評価に値した。アブストラクト/図１が非常にわかりやすくこの２つで問題設定を把握できるところもグッド。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://vis-www.cs.umass.edu/motionSegmentation/website_CVPR18/cvpr18-bideau.pdf">論文</a></li><li><a href="http://vis-www.cs.umass.edu/">UMASS CV Lab.</a></li><li><a href="http://vis-www.cs.umass.edu/motionSegmentation/website_CVPR18/cvpr18-bideau-suppl.pdf">SupplementaryMaterial</a></li></ul></div></div><div class="slide_index">[#310]</div><div class="timestamp">2018.5.3 01:36:43</div></div></section><section id="Regularizing_RNNs_for_Caption_Generation_by_Reconstructing_The_Past_with_The_Present"><div class="paper-abstract"><div class="title">Regularizing RNNs for Caption Generation by Reconstructing The Past with The Present</div><div class="info"><div class="authors">Xi.Cheny, L.Mazx, W.Jiangzx, J.Yaoy and W.Liuz</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1803.11439</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>encorder/decorderモデルにhiden stateと過去のhiden stateを再構成することによって隣接するhiden stateの接続を強化するためのARNetを導入．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Regulari zing_RNNs_for_Caption_Generation_by_Reconstructing_The_Past_with_The_Present.png" alt="概要図"></p></div></div><div class="item3"><div class="text"><h1>従来手法問題点</h1><ul><li>従来のRNNのtrainとinferenceの間にはexposure biasと呼ばれる相違が存在する．</li><li>decorderはの入力に依存する演算子を用いて，キャプション生成する．</li></ul></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><ul><li>RNNにおけるtransition dynamicsの正則化を助け，シーケンス予測の不一致の緩和が見られた．</li><li>ソースコードキャプション，イメージキャプションの両方で精度の向上が見られた．</li><li><a href="https://arxiv.org/pdf/1803.11439.pdf">Paper</a></li><li><a href="https://github.com/chenxinpeng/ARNet">github</a></li></ul></div></div><div class="slide_index">[#311]</div><div class="timestamp">2018.5.2 23:09:07</div></div></section><section id="Repulsion_Loss_Detecting_Pedestrian_in_a_Crowd"><div class="paper-abstract"><div class="title">Repulsion Loss : Detecting Pedestrian in a Crowd</div><div class="info"><div class="authors">Xinlong Wang, Tete Xiau, Yuning Jiang, Shuai Shao, Jian Sun and Chunhua Shen</div><div class="conference">CVPR2018, arXive:1711.07752</div><div class="paper_id">1005</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>群衆に頑健な歩行者検出法を提案．Faster R-CNNで群衆を検出したとき，歩行者同士の間にBounding Boxが出現しやすい．
これは，Bounding Box回帰の誤差を算出する時に誤差を最小にしようとして歩行者同士の間にBounding Boxが発生してしまう．
この現象を解決するために，新たにRepulsion Lossを導入し，群衆に対しても高精度な歩行者検出を実現している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1005_overview_repLoss.png" alt="1005_overview_repLoss.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>Repulsion Lossの中身は， L1 smooth lossをベースにしたL_RepGTとL_RepBoxから構成されている．L_RepGTは，targetの歩行者付近から最も近いGTとの誤差を示しており，targetと最も近いGTにBounding Boxが検出されると誤差が大きくなるように誤差が設計されている．
L_RepBoxは，複数のBounding Boxが特定の箇所に集中するように誤差を設定している．
L_RepBoxの目的は，NMSの割合の影響を減らすためである．
歩行者検出のCaltech, CityPerson(Cityscape)でstate-of-the-artな性能を出しており，Pascal VOCにおいても有効であることを示している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>歩行者検出のベンチマークにおいて非常に高い性能を示しており，ResNetベースのFaster R-CNNに対してDilated Conv.を導入する等のちょっとしたテクニックも色々導入されている．</p><ul><li><a href="https://arxiv.org/abs/1711.07752">論文リンク</a></li></ul></div></div><div class="slide_index">[#312]</div><div class="timestamp">2018.5.2 12:15:00</div></div></section><section id="PackNet_Adding_Multiple_Tasks_to_a_Single_Network_by_Iterative_Pruning"><div class="paper-abstract"><div class="title">PackNet : Adding Multiple Tasks to a Single Network by Iterative Pruning</div><div class="info"><div class="authors">Arun Mallya, Svetlana Lazebnik</div><div class="conference">CVPR2018, arXive:1711.05769</div><div class="paper_id">1004</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>複数のデータセットを1つのネットワークで学習する場合，通常は過去に学習したデータセットは段々と精度が低下していく．これは，全てのパラメータに対して更新するため，過去に学習したデータセットの特徴を抽出できなくなっていくのが原因である．
この論文で着目していることは，大規模なネットワークは特定のパラメータは学習をサボる傾向があるところであり，このサボっているパラメータを使って効率よく学習させて複数のデータセットを学習させている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1004_overview_packnet.png" alt="1004_overview_packnet.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>手法自体は非常にシンプルであり，特定のパラメータをプルーリング(右上図の白領域)して再学習する．そして，プルーリングしたパラメータのプルーリングを解放してパラメータをアップデートする．
特定のタスク(データセット)を学習した後は同じ要領でまたプルーリングと再学習を行う．
特定のパラメータを特定のタスクに割り当てるような学習をすることで，複数タスクに対応している．
結果としては，右図のようにタスクが追加されても性能がほとんど低下していない．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>単純な手法でありながら，非常に強力な手法．図2のインパクトがすごかった．様々な応用にも繋げれそう(Transfer Learning, Domain Adaptation等)</p><ul><li><a href="https://arxiv.org/abs/1711.05769">論文リンク</a></li><li><a href="https://github.com/arunmallya/packnet">コードリンク</a></li></ul></div></div><div class="slide_index">[#313]</div><div class="timestamp">2018.5.2 13:23:59</div></div></section><section id="Tell_Me_Where_to_Look_Guided_Attention_Inference_Network"><div class="paper-abstract"><div class="title">Tell Me Where to Look : Guided Attention Inference Network</div><div class="info"><div class="authors">Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, Yun Fu</div><div class="conference">CVPR2018, arXive: 1802.10171</div><div class="paper_id">1247</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>弱教師あり学習で得られる物体のローカライゼーションを高精度にする研究．方法としては2つ提案しており，</p><ol><li>GAPのローカライゼーションを用いて物体の領域と背景の領域を明示的に学習させる方法と，</li><li>セマンティックセグメンテーションのラベルを用いて物体の詳細な領域を学習させる方法がある．セマンティックセグメンテーションと視覚的解釈に対する評価をしており，どちらのタスクも高い性能を示している．</li></ol></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1247_overview.png" alt="1247_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>1)の方法では，2streamなCNNをベースにしており，入力はそれぞれ通常の画像と，GAPのローカライゼーションから物体領域を排除した画像を入力する．この処理により，物体と背景を明示的に学習できる．
そして，セマンティックセグメンテーションでは，
1)のネットワークに加えて，セマンティックセグメンテーションのラベルと出力したAttention mapとの誤差を算出させることで，Attention mapを最適化させる．
Pascal VOCのweakly-supervisedによるセマンティックセグメンテーションのタスクで評価し，高い性能を示している．
また，発生するAttention mapの領域に対してオリジナルのデータセットを作成して評価している．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.10171">論文リンク</a></li></ul></div></div><div class="slide_index">[#314]</div><div class="timestamp">2018.5.2 13:37:25</div></div></section><section id="Beyond_Trade_off_Accelerate_FCN_based_Face_Detector_with_Higher_Accuracy"><div class="paper-abstract"><div class="title">Beyond Trade-off: Accelerate FCN-based Face Detector with Higher Accuracy</div><div class="info"><div class="authors">Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, Yun Fu</div><div class="conference">CVPR2018, arXive: 1802.10171</div><div class="paper_id">1003</div></div><div class="slide_editor">Hiroshi Fukui</div><div class="item1"><div class="text"><h1>概要</h1><p>一般的な顔(物体)検出法(Faster R-CNN, FPN, SSD, YOLO等)は，Backboneな部分がFCNベースで構築されているため，各ピクセルを密に畳み込んで検出結果を出力する．しかし，顔検出では背景領域を大量に含んでおり，検出に必要な領域はごく僅かである．
本論文では，顔検出を効率化するために，2つのAttentionを適応して高速化を試みており，左上図のように高い性能を維持しつつ，4倍以上の高速化を実現している．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1003_overview.png" alt="1003_overview.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>本手法で適応しているAttentionは，右上図のようなspatial attentionとscale attentionである．spatial attentionは2次元上における顔の位置を示しており，scale attentionは出力されたスケールピラミッドから最適な特徴マップをAttentionで表現している．
spatial attentionは2次元の位置のattentionから探索する領域を制限するために使用し，scale attentionは探索するスケールピラミッドを制限するために使用する．
ネットワークは下図のようになっており，2つのAttentionにより背景と判定された領域は，マスクされた状態で後段のMask FCNに入力される．
AFW, FDDB, MALFでstate-of-the-artな性能かつ，高速な検出が可能(最速で14.2ms)．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Attentionを計算コスト削減に適応した物体検出法．顔検出や車載系の物体検出等の背景領域を多く含む問題設定では非常に効果的に使えそうな手法．
(COCO, VOCではあまりコストに対しては言及していない)</p><ul><li><a href="https://arxiv.org/abs/1804.05197">論文リンク</a></li></ul></div></div><div class="slide_index">[#315]</div><div class="timestamp">2018.5.2 13:55:50</div></div></section><section id="Deep_Marching_Cubes_Learning_Explicit_Surface_Representations"><div class="paper-abstract"><div class="title">Deep Marching Cubes: Learning Explicit Surface Representations</div><div class="info"><div class="authors">Y. Liao, S. Donné and A. Geiger</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>既存の学習ベースの3D面推定方法は，End-to-Endでの学習ができないが，本研究では，end-to-endでの学習を可能にした．3D面推定手法の一つのマーチングキューブは微分不可．そこで，代替の微分可能定式化を行い，これを3DNNの最終層として追加する．
また，疎な点群で学習が行えるようにロス関数群を提案．
サブボクセル精度での3D形状を推定可能であることを確認した．
本モデルは形状エンコーダ・推論と組み合わせられる柔軟さがある．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Deep_Marching_Cubes_Learning_Explicit_Surface_Representations_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>End-to-endで行われたものはない．適用範囲が広そう．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="http://www.cvlibs.net/publications/Liao2018CVPR.pdf">論文</a></li></ul></div></div><div class="slide_index">[#316]</div><div class="timestamp">2018.5.2 14:41:51</div></div></section><section id="Convolutional_Image_Captioning"><div class="paper-abstract"><div class="title">Convolutional Image Captioning</div><div class="info"><div class="authors">J.Aneja, A.Deshpande and A.Schwing</div><div class="conference">CVPR2018</div><div class="paper_id">arXiv:1711.09151v1</div></div><div class="slide_editor">KotaYoshida</div><div class="item1"><div class="text"><h1>概要</h1><p>近年，条件付き画像生成や機械翻訳において畳み込みニューラルネットの功績は大きい，これを画像キャプションに応用してみた．ベースラインであるLSTMモデルと同等の精度を示し，パラメータ数ごとの学習時間の短縮をすることができた．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Convolutional_Image_Captioning.png" alt="概要図"></p></div></div><div class="item3"><div class="text"><h1>従来手法の問題提起</h1><p> <ul><li>RNNは学習プロセスが逐次的</li><li>LSTM，RNNは画像の分類精度が低い</li></ul></p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p><ul><li>RNNとCNNのアプローチを分析し，CNNを用いたアプローチは出力確率分布のエントロピーの増大，単語予測精度の向上，消失勾配の影響の低下を示すことができた．</li><li><a href="https://arxiv.org/pdf/1711.09151.pdf">論文</a></li><li><a href="https://github.com/aditya12agd5/convcap">github</a></li></ul></p></div></div><div class="slide_index">[#317]</div><div class="timestamp">2018.5.1 18:06:38</div></div></section><section id="Are_You_Talking_to_Me_Reasoned_Visual_Dialog_Generation_through_Adversarial_Learning"><div class="paper-abstract"><div class="title">Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning</div><div class="info"><div class="authors">Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, Anton Van den Hengel</div><div class="conference">CVPR 2018</div><div class="paper_id">741</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・強化学習とGANを用いたVisual Dialog回答文を自動生成する手法の提案．・従来のVisual Dialogシステムは画像とDialog履歴に基づきMLEにより回答文の予測を行う．こういった手法では回答文が短い，バリエーションが少ないなどの問題点がある．そこで， co-attentionを利用したジョイントで画像， Dialog履歴をreasonできる回答文生成器を提案した．提案モデルはsequential co-attention生成器と回答文が“human”からか“生成された”かを弁別できる弁別で構成される．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Generate_visual_dialog.png" alt="Generate_visual_dialog"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・GANを用いた提案手法はVisual Dialogタスク従来の学習データの不足，簡潔な回答しか生成できないなどの問題点を改善した．・attentionをGANと組み合わせ， 生成回答文のinterpretabilityを向上した
・ VisDial データセットにおいて,従来の手法より高い精度を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・interactive環境でVisual Dialog回答文の生成ができたら更に様々な場面で応用できる</p><ul><li><a href="https://arxiv.org/pdf/1711.07613.pdf">論文</a></li></ul></div></div><div class="slide_index">[#318]</div><div class="timestamp">2018.5.2 13:13:20</div></div></section><section id="Density_Adaptive_Point_Set_Registration"><div class="paper-abstract"><div class="title">Density Adaptive Point Set Registration</div><div class="info"><div class="authors">Felix Järemo Lawin, Martin Danelljan, Fahad Khan, Per-Erik Forssen, Michael Felsberg</div><div class="conference">CVPR 2018</div><div class="paper_id">464</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・ 三次元センサーにより取得したPoint Set の密度の変動を対応できるPoint Set Registrationの手法を提案した．・従来の三次元センサー(例Lidar)により取得できるPoint Setの密度が均一ではない，一方，従来の確率的Point Set Registrationの手法は高密度の部分を対応させ，低密度の箇所の対応が重視されない問題点がある．提案手法はシーン構造の確率分布をモデリングすることにより，密度の変化にロバストに対応できる．
・提案手法は3次元シーンの構造及びフレーム間のカメラ移動量を同時にモデリングし， EMベースなフレームワークに基づきKL divergenceを最小化によりパラメータの最適化を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Density-Adaptive-Point-Set-Registration.png" alt="Density-Adaptive-Point-Set-Registration"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・Lidarを用いたregistrationシステムのPoint Setの密度変化をロバストで対応できた．・ DAR-ideal、 VPS and TLS ETH datasetsなどのLidarデータセットで従来の確率的マルチビューRegistration手法より良い性能を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p>・deep learningを用いていない手法</p><ul><li><a href="https://arxiv.org/pdf/1804.01495.pdf">論文</a></li></ul></div></div><div class="slide_index">[#319]</div><div class="timestamp">2018.5.2 10:39:57</div></div></section><section id="pOSE_Pseudo_Object_Space_Error_for_Initialization-Free_Bundle_Adjustment"><div class="paper-abstract"><div class="title">pOSE: Pseudo Object Space Error for Initialization-Free Bundle Adjustment</div><div class="info"><div class="authors">J. Hong and C. Zach</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>カメラ姿勢推定，3次元復元に使われるバンドル調整では，適した初期値を与える必要があるが，初期値を与える必要を無くす提案をする．</p><p>アフィンバンドル調整問題においては，任意の初期化から到達可能な使いやすいminimaがあることが知られているが，その主な要因は，収束のワイドな領域を持つことで知られているVariable Projection（VarPro）法の導入によるものである．本研究ではPseudo Object Space Error（pOSE）を提案する．これは，アフィンと射影のモデルのハイブリッドで表現される複数カメラにおける目的関数である．
この定式化で，VarPro法に適したバイリニア問題構造となり，真の射影復元と近い3D復元結果を得られる．
実験では，ランダムな初期化から高い成功率で正しい3D復元を得られることを確認した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/pOSE_Pseudo_Object_Space_Error_for_Initialization-Free_Bundle_Adjustment_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>ランダム初期値でもメトリックの正しい3D復元が行える．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://github.com/jhh37/pose/blob/master/Documents/hong_and_zach_cvpr18.pdf">論文</a></li></ul></div></div><div class="slide_index">[#320]</div><div class="timestamp">2018.5.2 10:31:48</div></div></section><section id="Finding_Tiny_Faces_in_the_Wild_with_Generative_Adversarial_Network"><div class="paper-abstract"><div class="title">Finding Tiny Faces in the Wild with Generative Adversarial Network</div><div class="info"><div class="authors">Yancheng Bai, Yongqiang Zhang, Mingli Ding, Bernard Ghanem</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kazuki Inoue</div><div class="item1"><div class="text"><h1>概要</h1><p>GANを用いて画像中の顔を検出する研究。検出が難しい顔として小さくかつボケている顔が挙げられるが、これらの顔をGANによって高解像度かつはっきりとした顔にすることで検出精度を向上させる手法を提案。
generatorは高解像度にするsuper resolution network(SRN)と顔の詳細な情報を復元するrefinment network(RN)を結合したネットワークである。
discriminatorはVGG19であり、ロスとしてデータセットの顔/generatorによる顔、顔/顔ではないモノを同時に行うロスを導入。
またよりはっきりとした顔を生成するために、generatorのロスとして物体識別のロスを導入。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Finding_Tiny_Faces_in_the_Wild_with_Generative_Adversarial_Network.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜCVPRに通ったか？</h1><ul><li>GANによって画像中の顔から高解像度かつはっきりとした顔を生成することで高精度な顔検出手法を提案。</li><li>GANの導入による精度の向上、導入したロスの有効性を確認している。</li><li>state-of-the-artと比較して、最も高い検出精度を達成</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li>検出精度が非常に高く、データセットではアノテーションし忘れている顔すらも検出してしまい、これによって精度が悪いように見えてしまうと主張している。</li><li>テスト時も学習時と同様に画像全体ではなくROIを与えているため、実行時間はそれなりにかかりそう。</li><li><a href="https://ivul.kaust.edu.sa/Documents/Publications/2018/Finding%20Tiny%20Faces%20in%20the%20Wild%20with%20Generative%20Adversarial%20Network.pdf">論文</a></li><li><a href="https://ivul.kaust.edu.sa/Pages/pub-tiny-faces.aspx">Project page</a></li></ul></div></div><div class="slide_index">[#321]</div></div></section><section id="Context_Encoding_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Context Encoding for Semantic Segmentation</div><div class="info"><div class="authors">Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, Amit Agrawal</div><div class="conference">CVPR 2018</div><div class="paper_id">893</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・コンテキスト情報の抽出を利用したセマンティックセグメンテーションの効率を上げられるContext Encoding Moduleを提案した．・従来の階層式シーンの高レベルから低レベル特徴の抽出を行うネットワーク(eg. PSPNet)にはシーンのコンテキスト情報の抽出がexplicitではない問題点があり，従来のグローバル特徴抽出ネットワークの知識から，シーンのコンテキスト情報を抽出することにより，セマンティックセグメンテーションの効率を上げられるモジュールを提案した．
・具体的には：Encodingによりシーンのコンテキスト情報をキャプチャーし，クラス依存の特徴マップを選択的に強調表示できるContext Encoding Moduleを提案した； Semantic Encoding Loss (SE-loss)を提案した； Context Encoding Moduleを利用したセマンティックセグメンテーションネットワークEncNetを提案した</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Context-Encoding-for-Semantic-Segmentation.png" alt="Context-Encoding-for-Semantic-Segmentation"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・ PASCAL VOC 2012において85.9% mIoUを達成した・提案ネットワークをCIFAR-10 datasetに応用し，14層だけのネットワークで100層超えのネットワークと同じレベルの精度を実現した</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・シンプルなネットワークでstate-of-the-artな精度を実現したので，将来的に広く用いられそう</p><ul><li><a href="https://arxiv.org/pdf/1803.08904.pdf">論文</a></li></ul></div></div><div class="slide_index">[#322]</div><div class="timestamp">2018.5.1 17:39:13</div></div></section><section id="Video_Based_Reconstruction_of_3D_People_Models"><div class="paper-abstract"><div class="title">Video Based Reconstruction of 3D People Models</div><div class="info"><div class="authors">Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, Gerard Pons-Moll</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Shusuke Shigenaka</div><div class="item1"><div class="text"><h1>概要</h1><p>人間が動いている単眼のRGB映像から、正確な3次元物体モデルと任意の人物テクスチャを得る研究。仮想現実や拡張現実、監視やゲームなどの人間の追跡にはアニメーション可能な人間行動の3Dモデルが必要である。この研究では、動的な人間のシルエットに対応するシルエット形状を見つけ出し、テクスチャや骨格を推定して、アニメーション可能なデジタルダブルを作成することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1803.04758_1.png" alt="1803.04758_1"><img src="slides/figs/1803.04758_2.png" alt="1803.04758_2"></p></div></div><div class="item3"><div class="text"><h1>手法・新規性・結果</h1><p>(a). SMPLモデルを用いてポーズを計算(b). シルエットの赤で描かれていないシルエットを取り除く
(c). 正規のTポーズで被写体の形状を最適化
(d). ティクスチャを計算しパーソナライズされた好みの形状を生成
・単眼のRGBビデオから髪や衣服を含む現実的なアバターを抽出
・被服を含む4.5mmの精度で人体形状を再構成</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><p><a href="http://link.com/link1/">link</a></p></div></div><div class="slide_index">[#323]</div><div class="timestamp">2018.5.1 16:26:45</div></div></section><section id="Relation_Networks_for_Object_Detection"><div class="paper-abstract"><div class="title">Relation Networks for Object Detection</div><div class="info"><div class="authors">Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei</div><div class="conference">CVPR 2018</div><div class="paper_id">439</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・マルチオブジェクトのアピアランス特徴及び幾何情報間の関係を取り扱える，様々なタスク（物体検出，VQAなど）に用いられるObject Relation Moduleを提案した．・最近attentionに関する研究が発展し，著者たちがattentionモジュールがelement間の依頼性を学習できる面から，物体検出に応用できるアテンションモジュールを提案した．
・提案モジュールを物体検出の2つの段階に応用できる：インスタンス認識段階で提案モジュールによりオブジェクト間の関係を習得でき，精度を上げられる；duplicate removal段階で提案モジュールにより有効的に物体領域を抽出できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Relation-Networks-for-Object-Detection.png" alt="Relation-Networks-for-Object-Detection"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・従来の物体検出手法は物体ごとに推定を行い，物体間の関係を利用しない．提案手法はObject Relation Moduleを提案し，物体間の関係を学習することで，物体検出の精度を更に向上した．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・提案モジュールが付加の監督信号不要，既存なネットワークに追加しやすい特徴があるため，様々なタスクでの応用が期待される</p><ul><li><a href="https://arxiv.org/abs/1711.11575">論文</a></li></ul></div></div><div class="slide_index">[#324]</div><div class="timestamp">2018.5.1 16:37:43</div></div></section><section id="PPFNet_Global_Context_Aware_Local_Features_for_Robust_3D_Point_Matching"><div class="paper-abstract"><div class="title">PPFNet: Global Context Aware Local Features for Robust 3D Point Matching</div><div class="info"><div class="authors">Haowen Deng, Tolga Birdal, Slobodan Ilic</div><div class="conference">CVPR2018</div><div class="paper_id">45</div></div><div class="slide_editor">Shuichi Akizuki</div><div class="item1"><div class="text"><h1>概要</h1><p>点群データから直接3Dの局所特徴量を抽出するネットワークを提案．N-Tuple loss(Triplet lossの拡張)によって，
対応点間の特徴量が近く，それ以外の特徴量間の距離が遠くなるような変換を学習する．
PPFNetの入力は局所パッチ内の点の座標，法線，Point Pair Featureをまとめたデータ．
ネットワークの内部ではPointNetを利用する．
大域的な情報を得るために，各パッチから取得した局所特徴量を
Max poolingによって大域特徴量化し，局所特徴と結合する工夫も入れている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/45.png" alt="45"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>局所特徴量を生成するネットワークを構築した点，N-Tuple lossによる学習法を提案した点が新しい．
キーポイントマッチングのベンチマークでRecall rateが向上．
オーバーラップが少ないシーンでのレジストレーションも可能になっている．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.02669">Paper</a></li></ul></div></div><div class="slide_index">[#325]</div><div class="timestamp">2018.5.1 15:53:31</div></div></section><section id="Geometry-Aware_Generative_Adversarial_Networks"><div class="paper-abstract"><div class="title">GAGAN: Geometry-Aware Generative Adversarial Networks</div><div class="info"><div class="authors">Jean Kossaifi, Linh Tran, Yannis Panagakis and Maja Pantic</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yuta Matsuzaki</div><div class="item1"><div class="text"><h1>概要</h1><p>既存のGANでは考慮されていなかった形状や位置といった幾何学的情報をGANの生成プロセスに組み込んだGeometry-Aware Generative Adversarial Networks (GAGAN) を提案．具体的にGAGANでは，ジェネレータで統計的情報な形状モデルの確率空間から潜在関数をサンプリングする．次にジェネレータの出力値を微分可能な幾何学変換を介して標準座標系にマッピングすることで，物体の形状や位置といった情報を強制し，生成を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/20180501_gagan.png" alt="Item3Image"><img src="slides/figs/20180501_gagan2.png" alt="Item4Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><ul><li>GAGANのような幾何学的情報を考慮した生成モデルはなく，GAGANが初</li><li>入力画像の属性の形状に合わせて，画像を生成することが可能</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>今後は，(i)より大きな画像の生成，(ii)アフィン変換によって起こりうる変形を緩和するより複雑な幾何学的変換の探索およびそれによるGAGANの拡張，(iii)顔のランドマーク検出のための従来CNNアーキテクチャの拡張に取り組む予定</p><ul><li><a href="https://arxiv.org/pdf/1712.00684.pdf">論文</a></li></ul></div></div><div class="slide_index">[#326]</div><div class="timestamp">2018.5.1 14:41:53</div></div></section><section id="IQA_Visual_Question_Answering_in_Interactive_Environments"><div class="paper-abstract"><div class="title">IQA: Visual Question Answering in Interactive Environments</div><div class="info"><div class="authors">Daniel Gordon, Ali Farhadi, Aniruddha Kembhavi, Dieter Fox, Mohammad Rastegari, Joe Redmon</div><div class="conference">CVPR2018</div><div class="paper_id">533</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・新たな問題設定ー動的環境とインターアクトしながら視覚質問に答える(IQA)を提案した．・具体的には， IQAには4つの設定がある：環境でナビゲートする能力；環境中のオブジェクト，アクション及びアフォーダンスの理解；環境中のオブジェクトとインターアクトする能力；質問文に応じで環境での行動を計画する能力．
・提案の問題設定を解決するために，階層的マルチレベルで行動計画及びコントロールするネットワークHIMN及び空間的かつセマンティックなメモリを実現できる新たなrecurrent layer形式Egocentric Spatial GRUを提案した．
・更に，75000質問及びCGシーンを含んだデータセットIQUAD V1を提案した．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Visual-Question-Answering-in-Interactive-Environments.png" alt="Visual-Question-Answering-in-Interactive-Environments"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・従来のVQAタスクをCGシーンでの自己ナビゲーションと組み合わせた新たな問題設定を提案した．・IQUAD V1で従来の手法よりstate-of-the-artな精度</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・従来のVQAタスクに更に環境での探索および環境中オブジェクトとのインターアクトを取り入れ，従来の問題設定より一層現実に近づいている．・質問文の自動生成にも応用できそう
・特に色々なタスクを取り扱えているので，技術の面では向上する空間がありそう</p><ul><li><a href="https://arxiv.org/pdf/1712.03316.pdf">論文</a></li></ul></div></div><div class="slide_index">[#327]</div><div class="timestamp">2018.5.1 15:29:03</div></div></section><section id="On_the_Robustness_of_Semantic_Segmentation_Models_to_Adversarial_Attacks"><div class="paper-abstract"><div class="title">On the Robustness of Semantic Segmentation Models to Adversarial Attacks</div><div class="info"><div class="authors">Anurag Arnab, Ondrej Miksik and Philip H.S. Torr</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">kodai nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>adversarial attackに対するロバスト性の評価を, semantic segmentationにおいてstate-of-the-artな性能を持つネットワークを用いて実験した.Pascal VOCとCityscapesのデータセットに対して, FGSM, Interative FGSM, FGSM II, Interative FGSM IIで攻撃したときのIoU Ratioによりロバスト性を評価した.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/On_the_Robustness_of_Semantic_Segmentation_Models_to_Adversarial_Attacks.PNG" alt="On_the_Robustness_of_Semantic_Segmentation_Models_to_Adversarial_Attacks.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>ResNetをバックボーンに持つネットワークがロバストであることがわかった. 中でもDeeplab v2が最もロバスト.</li><li>multi-scale processingやmean field CRFによりロバストになる.</li><li>画像分類の分野で一般的なロバスト性やモデルサイズについての知識がsemantic segmentationでも有用とは限らない.</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1711.09856.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#328]</div><div class="timestamp">2018.5.1 14:32:41</div></div></section><section id="CodeSLAM_---_Learning_a_Compact_Optimisable_Representation_for_Dense_Visual_SLAM"><div class="paper-abstract"><div class="title">CodeSLAM --- Learning a Compact, Optimisable Representation for Dense Visual SLAM</div><div class="info"><div class="authors">Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, Andrew Davison</div><div class="conference">CVPR  2018</div><div class="paper_id">288</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・RGB画像の強度データと少数のパラメータを条件に，ほぼリアルタイムで行えるデンスなシーン幾何を推定手法を提案した．・提案手法UNet構造により強度画像の特徴抽出を行い，更に抽出特徴をauto-encoder構造を用いたデプス情報推定ネットワークに入力することで階層的にデプス情報推定を行う．また，カメラ移動中得られるマルチフレームに対し，フレームごとのデプス推定及びフレーム間のカメラモーションをジョイントで最適化を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/codeSLAM.png" alt="codeSLAM"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・デンスなデプス情報推定を行うことでSLAMシステムの更なる精度向上できると宣言した．・初めてのほぼリアルタイムで行えるカメラモーションとシーンのデンス幾何をジョイントで推定する研究である．</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>・著者たちは将来のワークとして，提案手法をリアルタイムでデンスなSLAMシステムの構築に拡張すると指摘し,将来的な研究を期待している．</p><ul><li><a href="https://arxiv.org/abs/1804.00874">論文</a></li></ul></div></div><div class="slide_index">[#329]</div><div class="timestamp">2018.5.1 14:08:56</div></div></section><section id="Learning_by_asking_questions"><div class="paper-abstract"><div class="title">Learning by asking questions</div><div class="info"><div class="authors">Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta, Laurens van der Maaten</div><div class="conference">CVPR 2018</div><div class="paper_id">3</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>・VQAタスクに用いられる新たなインターアクティブ学習フレームワークを提案した．・提案フレームワークは入力画像から，question proposal moduleにより問題集を生成し，画像との相関性を基準に問題集をフィルタリングし，残った問題をVQAにより解く．予測した答え，自己の知識及び過去の知識から質問を1つ選び，oracleにより答える．
・提案フレームワークにより，効率高い学習サンプルを得られる．また，従来のVQAネットワークで用いられるstate-of-the-artな問題集を生成できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/learning_by_asking_questions.png" alt="learning_by_asking_questions"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・なぜ通ったか？</h1><p>・従来のあらゆるフレームワークは学習データから学習を行う．この論文で，質問文の自動生成できる及び質問を選択する構造を導入し，自動的でインターアクティブで環境から情報を獲得することを可能にした．・実験を通し，提案手法により質問を選択する規制がsampleの効率を高められる．（従来と同じ精度の場合，学習データ量を40％減らせる）</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>real-worldバージョンのLBAシステムが実現されたら，機械で学習することは更に人の学習システムに近づく．</p><ul><li><a href="https://arxiv.org/abs/1712.01238">論文</a></li></ul></div></div><div class="slide_index">[#330]</div><div class="timestamp">2018.5.1 12:10:26</div></div></section><section id="Learning_Spatial-Temporal_Regularized_Correlation_Filters_for_Visual_Tracking"><div class="paper-abstract"><div class="title">Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking</div><div class="info"><div class="authors">Feng Li, Cheng Tian, Wangmeng Zuo, Lei Zhang, Ming-Hsuan Yang</div><div class="conference">CVPR 2018</div><div class="paper_id">1353</div></div><div class="slide_editor"><Takahiro>Itazuri</Takahiro></div><div class="item1"><div class="text"><h1>概要</h1><p>Spatially Regularized Discriminative Correlation Filters (SRDCF)に空間正則化を導入した一般物体追跡手法Spatial-Temporal Regularized Correlation Filters (STRCF)を提案. SRDCFは複数学習画像を利用するため, 計算量が大きくなってしまうことに着目し, 単一学習画像に対するSRDCFにonline Passive-Aggresive learningの考えに基づいて時間正則化を導入. STRCFはADMMで直接解くことができるため, DCFの高速性を保持したまま高い精度で追跡が可能となっている.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/STRCF.png" alt="STRCF"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>単一学習画像に対するSRDCFに時間正則化を導入することで, 複数学習画像に対するSRDCFを近似したSRTCFを定式化</li><li>online Passive-Aggresive learningを拡張することで, STRCFは大きな見た目の変化に対して頑健である</li><li>SRTCFはADMMを用いて, 3つの部分問題に帰着させ, Eckstein-Bertsekas条件を満たし, 大域的最適解への収束性を保証している</li><li>OTB-2015, Temple-Color, VOT-2016データセットにおいてSRDCFより精度も計算速度も向上させた</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.08679.pdf">論文</a></li><li><a href="https://github.com/lifeng9472/STRCF">コード</a></li></ul></div></div><div class="slide_index">[#331]</div></div></section><section id="Learning_Spatial-Aware_Regressions_for_Visual_Tracking"><div class="paper-abstract"><div class="title">Learning Spatial-Aware Regressions for Visual Tracking</div><div class="info"><div class="authors">Chong Sun, Huchuan Lu, Ming-Hsuan Yang</div><div class="conference">CVPR 2018</div><div class="paper_id">1676</div></div><div class="slide_editor"><Takahiro>Itazuri</Takahiro></div><div class="item1"><div class="text"><h1>概要</h1><p>一般物体追跡手法の二大手法であるカーネルリッジ回帰（相関フィルタを含む）とCNNのハイブリッドな手法を提案した.カーネルリッジ回帰は全体的な情報に,CNNは局所的な情報に注目するように設計している.それぞれの導入がどの精度向上に結びついているかも検討している.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/LSART.png" alt="LSART"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>cross-patch similarityを用いたカーネルリッジ回帰モデルを提案し,それをニューラルネットに再定式化.</li><li>spatially reguralized kernelとdistance transform pool layerを用いて,出力の各チャンネルが特定の領域に反応するようなCNN提案.</li><li>提案したカーネルリッジ回帰とCNNを相補的に用いることで,OTB-2013,OTB-2015,VOT-2016データセットでstate-of-the-artな精度を達成.		</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="http://lanl.arxiv.org/pdf/1706.07457v1">論文1</a></li><li><a href="https://drive.google.com/file/d/1Lls2CK-yTkeOcOarNGapmcz4h2HQj6gf/edit">論文2</a></li></ul></div></div><div class="slide_index">[#332]</div></div></section><section id="Improved_Fusion_of_Visual_and_Language_Representations_by_Dense_Symmetric_Co-Attention_for_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering</div><div class="info"><div class="authors">Nguyen Duy Kien, Takayuki Okatani</div><div class="conference">CVPR 2018</div><div class="paper_id">739</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>VQAタスクに用いられるattentionメカニズム“Dense Co-attention Network”(DCN)を提案した．DCNはfully対称的で，階層的にスタックできるため，マルチステップで視覚及び言語特徴のインターアクションを可能にする．具体的には，まず言語から画像の注目マップ及び画像から言語の注目マップを生成し，そして連結によりマルチモデルの特徴を融合する（dense co-attention layer)．そして階層的にdense co-attention layerをスタックにより，さらにマルチモデル特徴を深く探る．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Co-attention_VQA.png" alt="Co-attention_VQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・従来のattention for VQAタスクより，有効的でデンスな視覚と言語モデルの特徴の融合メカニズムDCN（構造的にも簡潔で拡張しやすい）を提案し，将来の様々なVQAタスクに用いられる．・VQA, VQA2.0データセットで2017 VQA優勝したモデルより良い精度を達成した．
・定性的な実験により，提案モデルが有効的にattentionを抽出できることを証明した</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.00775.pdf">論文</a></li></ul></div></div><div class="slide_index">[#333]</div></div></section><section id="Deep_Voting_A_Robust_and_Explainable_Deep_Network_for_Semantic_Part_Detection_under_Partial_Occlusion"><div class="paper-abstract"><div class="title">DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection under Partial Occlusion</div><div class="info"><div class="authors">Z. Zhang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><div class="text"><h1>概要</h1><p>画像中から物体のパーツ（車のタイヤなど）を検出するための新しい手法を提案．投票ベースの手法でオクルージョンへの頑健性を持つ．
Visual ConceptというMid-levelな特徴をベースにして，
個々のMid-level特徴から推定されるパーツの位置推定結果を積み重ねていくことでパーツを検出する．
Visual Conceptの検出とそれに基づく投票処理はConvolutionによって実装されており，
End-to-Endでの学習が可能になっているところがポイント．
Faster-RCNNといった物体検出アプローチよりもオクルージョンに頑健なことが実験的に確認できている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/DeepVoting_A_Robust_and_Explainable_Deep_Network_for_Semantic_Part_Detection_under_Partial_Occlusion..png" alt="DeepVoting_A_Robust_and_Explainable_Deep_Network_for_Semantic_Part_Detection_under_Partial_Occlusion..png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>CNNベースのVotingによるオクルージョンに頑健なパーツ検出手法を提案</li><li>Visual Conceptの検出から投票までConvolutionで実装</li><li>人工的なオクルージョン環境下での有効性を確認</li></ul></div></div><div class="item4"><div class="text"><h1>コメント・リンク</h1><ul><li>投票処理までConvolutionで表現されているのが面白い</li><li><a href="https://arxiv.org/abs/1709.04577">論文</a></li><li><a href="http://lingxixie.com/PDFs/Zhang_CVPR18_DeepVoting_SuppMat.pdf">Supplementary Material</a></li></ul></div></div><div class="slide_index">[#334]</div><div class="timestamp">2018.4.23 06:07:59</div></div></section><section id="Feature_Mapping_for_Learning_Fast_and_Accurate_3D_Pose_Inference_from_Synthetic_Images"><div class="paper-abstract"><div class="title">Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images </div><div class="info"><div class="authors">Mahdi Rad   et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>合成データを利用した、6D pose estimationとdepth based 3D hand pose estimationの研究。</p>埋め込み空間内で、合成データから実データへのマッピング関数を学習する。その関数の学習のためには実データに対応する(grand truthが同じ)合成データが必要であるので、教師あり実データがある程度あることが前提としてある。</div></div><div class="item2"><div class="text"><p><img src="slides/figs/Mahdi.png" height="1000"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>残差構造を持つmapping netを対応するペアを用いて学習する。従来のドメイン適応手法と比較しても提案手法の精度が良く、適応の有無による性能の差も非常に大きい。</p></div></div><div class="item4"><div class="text"><h1>メモ・リンク</h1><p>手法としてはかなりstraight forwardな印象。実データの量を変化させた時の精度変化の結果はあったが、合成データの量を変化させた時の精度変化が気になる。</p><ul><li><a href="https://arxiv.org/pdf/1712.03904.pdf">論文</a></li></ul></div></div><div class="slide_index">[#335]</div></div></section><section id="Embodied_Question_Answering"><div class="paper-abstract"><div class="title">Embodied Question Answering</div><div class="info"><div class="authors">Abhishek Das et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>3次元空間において、エージェントに質問の答え（例：車の色は？）を探させる研究。初期位置における視覚情報だけでは答えに行きつかないためにエージェントは移動しながら答えを探していく。
エージェントの移動には、どの方向（forward, rightなど)に進むかを決定するplannerとどこまで進むかを決定するcontrolerによって行う。
目的地(正解が分かる場所)にたどり着いた時点で、最後の5フレームを用いて172の選択肢から正解を出力する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Embodied_Question_Answering.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>LSTMを使った場合の方が目的地により近付けるという結果が得られた。強化学習なしのものは目的地により近づいている一方、ファインチューニング＋強化学習の方が正解率は高いという結果となった。
また、最短経路を与えてVQAによって答えさせる場合でも精度が悪く、答えを導くにあたってどの方向から目的地に近づくかも重要であるということが分かった。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://embodiedqa.org/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#336]</div><div class="timestamp">2018.4.23 12:59:26</div></div></section><section id="Learning_from_Synthetic_Data_Addressing_Domain_Shift_for_Semantic_Segmentation"><div class="paper-abstract"><div class="title">Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation</div><div class="info"><div class="authors">Sankaranarayanan et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>GANによる画像生成の枠組みを中間的に取り入れることでSemantic segmentationにおけるドメイン適応を行う研究。</p>従来の特徴ベクトルに対する敵対的学習によって埋め込み空間におけるdomain gapを縮める手法に対して、この研究では特徴ベクトルから画像を復元し、その画像が識別器によってどのドメインからの復元か識別できないように埋め込み関数を学習させる。
合成データからのドメイン適応で最も良い精度を達成。</div></div><div class="item2"><div class="text"><p><img src="slides/figs/Swami.png" height="1000"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p> Source(S)は教師ありデータ、Target(T)は教師なしデータ。学習のフローは以下である: 
(1)識別器(D)は入力画像に対してpixel-wiseにsource real(SR), source fake(SF), target real(TR), target fake(TF)の4値分類を学習。(2)生成器(G)は入力特徴ベクトルからDによってSからの特徴はSRに、 Sからの特徴はTRに分類されるよう学習。
(+入力との担保を取るL2Loss)(3)埋め込み関数(F)はSからの入力はTRに、Tからの入力はSRに分類されるように学習。さらにSからのサンプルに対してはFからの特徴マップを入力としてsegmentation taskを解くCNNを学習。</p></div></div><div class="item4"><div class="text"><h1>メモ・リンク</h1><p>論文内にこの手法がうまくいく理由の裏付け的実験や考察が詳細にはなかったが、特徴量から画像再生成を行うことによる入力情報の保存とS/T間の敵対的学習による分布の混合が一つのフローで行えていることが効いているように思えた。実際特徴量に対するS/T間の敵対的学習のみの場合よりも大きく精度が向上している。</p><ul><li><a href="https://arxiv.org/abs/1711.06969">論文</a></li></ul></div></div><div class="slide_index">[#337]</div></div></section><section id="Natural_and_Effective_Obfuscation_by_Head_Inpainting"><div class="paper-abstract"><div class="title">Natural and Effective Obfuscation by Head Inpainting</div><div class="info"><div class="authors">Qianru Sun, Liqian Ma, Seong Joon Oh, Luc Van Gool, Bernt Schiele, Mario Fritz</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>SNSなどで共有された画像には、プライバシー保護の問題が生じる。プライバシー保護のために顔領域にぼかしや黒塗りなどの処理がされることが多いが、画像としては不自然さが残ってしまう。
そこで、塗りつぶされた領域に顔を挿入することで自然な画像ではあるが別人のためプライバシーを保護できる画像を生成する。
提案手法は、特徴点検出（生成）と顔の挿入の2つのステップに分かれる。
特徴点検出（生成）では、オリジナルの顔画像が存在する場合は既存の特徴点検出によって特徴点を検出する。
対称の画像が既に黒塗りされているなどで特徴点検出ができない場合は、GANによって特徴点を生成する。
次のステップでは、黒塗りされている顔画像と特徴点を入力し、黒塗りされた領域に顔の挿入を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Natural_and_Effective_Obfuscation_by_Head_Inpainting.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>特徴点生成器は、GANによって生成することで正解値とのノルム最小化よりも高い精度で生成することを可能にした。画像に対する処理としてぼかしと黒塗りを比較したところ、ぼかしは顔の情報が一部残るため高い精度での生成が可能である一方、元の人物の情報は黒塗りよりも多く残ることが分かった。
また、顔の形状にも個人性が含まれるためオリジナル画像から検出した特徴点よりもGANによって生成した特徴点を使用した方が個人性は損なわれることが分かった。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.09001">論文</a></li></ul></div></div><div class="slide_index">[#338]</div><div class="timestamp">2018.4.18 15:45:44</div></div></section><section id="Augmenting_Crowd-Sourced_3D_Reconstructions_using_Semantic_Detections"><div class="paper-abstract"><div class="title">Augmenting Crowd-Sourced 3D Reconstructions using Semantic Detections</div><div class="info"><div class="authors">T. Price, J. L. Schonberger, Z. Wei, M. Pollefeys and J.M. Frahm</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Ryota Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>SfMにおいて，一つの撮影にしか映らないような移動物体を考慮することで，そのシーンの絶対スケールが推定可能になるし，人混みだと見えにくい地平面の復元も成しうる．個々の撮影画像において検出された人を3次元空間に投影し，さらに物体の意味情報（本稿では背の高さの分布）から絶対スケールを推定する．
また，人検出結果を用いて地平面推定も行う．
ランダムなインターネット画像で手法をデモンストレーションし，量的評価を行う．</p><p>人検出はトルソモデルのフィッティングに基づく．画像における肩，腰の位置が推定でき，おおよその立ち位置も分かるということ．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Augmenting_Crowd-Sourced_3D_Reconstructions_using_Semantic_Detections_Figure1.png" alt="Figure1"></p></div></div><div class="item3"><div class="text"><h1>評価点</h1><p>若干SIGGRAPH的な気風のある，面白い視点を提供する論文．過去の知見に基づく高品質な人検出などを用いて成し得た，正統なアプリケーションに感じる．
動画のインパクトも大きいので，一度視聴を勧める．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://demuc.de/papers/price2018augmenting.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=eQWXuPv5eC0">動画</a></li></ul></div></div><div class="slide_index">[#339]</div><div class="timestamp">2018.4.16 17:07:53</div></div></section><section id="Single_View_Stereo_Matching"><div class="paper-abstract"><div class="title">Single View Stereo Matching</div><div class="info"><div class="authors">Yue Luo, Jimmy Ren, Mude Lin, Jiahao Pang, Wenxiu Sun, Hongsheng Li and Liang Lin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">kodai nakashima</div><div class="item1"><div class="text"><h1>概要</h1><p>従来の単眼奥行き推定法では, 推論の際に幾何的な制約を明示的に課していないことや多くのground truth labeled dataが必要といった問題があった.この研究では単眼奥行き推定問題をview synthesis問題とstereo matching問題に分けて考えることにより, 従来法の問題を解決する.
view synthesis問題では, 入力を左画像として捉え, view synthesis networkにより右画像を生成する. stereo matching問題では, 左画像を右画像を用いstereo matching networkにより奥行きを推定する.</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/single_view_stereo_matching.PNG" alt="single_view_stereo_matching.PNG"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>単眼奥行き推定問題をview synthesis問題とstereo matching問題に分けて考えた.</li><li>従来法の問題を解決.</li><li>従来のどの方法よりも精度が高い.</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.02612.pdf">論文URL</a></li></ul></div></div><div class="slide_index">[#340]</div><div class="timestamp">2018.4.30 18:04:11</div></div></section><section id="Learning_Face_Age_Progression_A_Pyramid_Architecture_of_GANs"><div class="paper-abstract"><div class="title">Learning Face Age Progression: A Pyramid Architecture of GANs</div><div class="info"><div class="authors">Hongyu Yang, Di Huang, Yunhong Wang and Anil K. Jain</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>入力画像中の人物の老化顔をGANによって生成する手法の提案。Discriminatorには生成した画像が合成画像であるか及び目標年代の特徴を保持しているかを判定させ、それに加え元の画像とのL2ノルム及び元の顔画像と同一人物であるかをロスに加えることで、同一人物性を保持している。
その際、Discriminatorの中間層の各出力を途中で取り出すことにより（ピラミッド型ネットワーク），様々な解像度からの年齢特徴の抽出を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Learning_Face_Age_Progression_A_Pyramid_Architecture_of_GANs.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>年齢推定及び個人認証タスクによって有効性を確認した。従来手法では髪や額領域は変化できなかったが、提案手法によってこれらの要素を変化させることを可能とした。
Discriminatorをピラミッド型にすることにより、従来手法に比べてより詳細な老化特徴を取り出すことに成功。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1711.10352">論文</a></li></ul></div></div><div class="slide_index">[#341]</div><div class="timestamp">2018.4.16 16:14:24</div></div></section><section id="Image_Generation_from_Scene_Graphs"><div class="paper-abstract"><div class="title">Image Generation from Scene Graphs</div><div class="info"><div class="authors">Justin Johnson et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>物体同士の関係を表すScene Graphsから画像を生成する手法の提案。従来のテキストから画像を生成する手法よりも物体の数が多く複雑なシーンの画像を生成することができる。
初めに、Scene Graphsを処理するネットワークによってScene Graphsを表現するベクトルを取得し、そこから画像のレイアウトを作成する。
次にレイアウトからCRN(参考文献)を用いて画像を作成する。
作成された画像は、画像全体のリアルさと各物体のリアルさを評価するDiscriminatorによってリアルな画像であるかを評価する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180411graph.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>ユーザースタディの結果、StackGANと比較して合成結果が良いと答えた人が68%、認識可能な物体を生成できてると答えた人が59%という結果が得られた。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.01622">論文</a></li><li><a href="https://arxiv.org/abs/1707.09405">CRN</a></li><li><a href="https://arxiv.org/abs/1612.03242">StackGAN</a></li></ul></div></div><div class="slide_index">[#342]</div><div class="timestamp">2018.4.11 15:58:22</div></div></section><section id="Bottom-Up_and_Top-Down_Attention_for_Image_Captioning_and_Visual_Question_Answering"><div class="paper-abstract"><div class="title">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</div><div class="info"><div class="authors">Peter Anderson, Xiaodong He, Chris Buehler,Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang</div><div class="conference">CVPR 2018</div><div class="paper_id">738</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>Image captioningとVQAタスクに用いられるBottom-upとtop-down attentionをコンバインするメカニズムを提案した．従来のオブジェクトレベルの領域の抽出のほか，salient 領域の抽出も行う．Faster R-CNNを利用したbottom-up的にsalient 領域を特徴ベクトルを抽出し， top-downにより特徴のウェットを決めることをベースに， Image captioningとVQAのアーキテクチャを提案し（右図），両方ともstate-of-artな性能を得られた．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Bottom_up_Top_down_VQA.png" alt="Bottom_up_Top_down_VQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・従来のVQAとImage captioningは主にタスクスペシフィックなtop-downタイプのattentionを用いる．この論文で，人の視覚attentionメカニズムから，タスクスペシフィックなtop-downタイプのattentionを及びsalient 領域に注目するBottom-upのattentionを用いることと主張した．・2017 VQA Challengeにおいて優勝した．VQA v2.0 test-standardにおいて70.3%の精度を達成した．また， Image captioning タスクに対しMSCOCO Karpathy testで従来の手法より良い性能を達成した．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1707.07998.pdf">論文</a></li></ul></div></div><div class="slide_index">[#343]</div><div class="timestamp">2018.4.27 10:27:30</div></div></section><section id="Tips_and_Tricks_for_Visual_Question_Answering_Learning_from_the_2017_Challenge"><div class="paper-abstract"><div class="title">Tips and Tricks for Visual Question Answering: Learning from the 2017 Challenge</div><div class="info"><div class="authors">Damien Teney, Peter Anderson, Xiaodong He, Anton Van den Hengel</div><div class="conference">CVPR 2018</div><div class="paper_id">547</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>2017 VQA Challengeに優勝したモデルのモデル詳細を紹介し，さらにいかにVQAモデルの精度を上げられるかのコツとテクニックを紹介した．モデルのコアなところは視覚と質問文の意味特徴をジョイントでエンベディングし，さらにマルチ-ラベル予測を行う．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Tip_Tricks_VQA.png" alt="Tip_Tricks_VQA"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>論文により，VQAの性能上げるために，以下のテクニックがある：1.sigmoid outputsを用いて，マルチアンサーをできるようにする．2．Soft scoresを用いて，分類ではなく回帰を行う．3．Bottom-up attentionから注目領域の画像特徴を用いる．4．Gated tanhを活性化関数に用いる．5．Pre-trainedウェットで初期化する．6．ミニバッチサイズを大きく設定し，training-dataにシャッフリングを用いる</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1708.02711">論文</a></li></ul></div></div><div class="slide_index">[#344]</div><div class="timestamp">2018.4.26 16:58:02</div></div></section><section id="What_Makes_a_Video_a_Video_Analyzing_Temporal_Information_in_Video_Understanding_Models_and_Datasets"><div class="paper-abstract"><div class="title">What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets </div><div class="info"><div class="authors">Xiaolong Wang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Tomoyuki Suzuki</div><div class="item1"><div class="text"><h1>概要</h1><p>「3DCNNが実は動き特徴を捉えられていないのではないか」という考えのもと、3DCNNにおける動き特徴の影響の上界を実験的に求める。提案する工夫により、この影響のかなり低い上界を得ることができ、動き特徴を捉えているのではない(例えば実は複数フレーム入力から「重要なフレーム選択」を行っているなど)ことを示唆した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/What_Makes_a_Video_a_Video_Analyzing_Temporal_Information_in_Video_Understanding_Models_and_Datasets.png"></p></div></div><div class="item3"><div class="text"><h1>検証方法</h1><p> 通常の16frames入力で学習したC3Dにおいてtest時にsub-samplingした(動き情報を無くした)設定下でできるだけ精度を上げることで結果的に動き特徴の上界を得る。Naïveにsub-samplingを行うと入力のデータ分布の明らかな違いから動き以外の精度低下への影響をもたらすと考えられるため、 sub-samplingされたclipから元clipを生成するgeneratorを構築。学習はC3Dの中間層の値をMSEで近づける。
またsampling方法によっても精度は変わるという考えから、識別confidenceが最大となるframesをsamplingする。注意として、この際動きに関しては全く考慮せずにsamplingしてきている。</p></div></div><div class="item4"><div class="text"><h5>コメント・リンク</h5><p>結果として、かなりきつい上界を求められ、論文内では3DCNNが2Dよりも精度が良いのは動き特徴ではなく、複数フレーム入力の中で最も識別しやすいフレームを選択可能になるからではと述べられている。</p><p>フレーム選択をしているという仮説は面白いし、select frameによって精度が上昇したり、動きが大きい動画はフレーム単位での推定結果の分散が大きいなどから十分ありえそう。これが本当なら、optical flowを3dCNNに導入して大きく精度が向上することともつじつまが合いそう。</p><ul><li><a href="http://ai.stanford.edu/~dahuang/papers/cvpr18-fb.pdf">論文</a></li></ul></div></div><div class="slide_index">[#345]</div></div></section><section id="Surface_Networks"><div class="paper-abstract"><div class="title">Surface Networks</div><div class="info"><div class="authors">Ilya Kostrikov, Joan Bruna, Daniele Panozzo, Denis Zorin</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>3D triangleメッシュから有用的な三次元幾何情報を抽出するネットワークSurface Networkを提案した．従来のLaplace operatorがintrinsic三次元幾何情報しか抽出できない．しかし，様々な応用場面でextrinsic情報が必要となる．この文章で主要なcurvature方向を抽出できるDirac operator を提案し，従来のLaplace operatorより幅広い場面で応用できる．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SurfaceNetwork_result.png" alt="SurfaceNetwork_result"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>・定性的および定性的な結果によりspatial-temporal predictionsタスクにおいて，従来手法より良い結果を得られている．・variationalエンコーダーを用いたメッシュ合成手法を提案し，有効的に3次元メッシュを生成できる．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1705.10819">論文</a></li></ul></div></div><div class="slide_index">[#346]</div><div class="timestamp">2018.4.13 11:16:55</div></div></section><section id="SPLATNet_Sparse_Lattice_Networks_for_Point_Cloud_Processing"><div class="paper-abstract"><div class="title">SPLATNet: Sparse Lattice Networks for Point Cloud Processing</div><div class="info"><div class="authors">Hang Su, University of Massachusetts, Amherst; Varun Jampani, NVIDIA Research; Deqing Sun, NVIDIA; Evangelos Kalogerakis, UMass; Subhransu Maji, ; Ming-Hsuan Yang, UC Merced; Jan Kautz, NVIDIA</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>概要</h1><p>点群情報を直接処理できるSPLATNet（右図）を提案した．SPLATNetは直接点群から階層的な空間情報を抽出可能．また，2D情報と3D情報のマッピングも行えるので，点群とマルチ画像の両方をSPLATNetで処理可能．従来の直接点群情報を処理するネットワークはより局所的な空間情報を損失してしまう問題点がある．提案手法はこの問題を解決するために，BCLs層を用いた． BCLs層は点群をスパースなlatticeにマッピングし，さらにそのスパースなlatticeを畳み込みできる．それにより， unordered点群情報を処理できる上に点群のより局所的な情報も抽出可能にした．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SPLATNET.png" alt="SPLATNET"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Façade segmentationタスクにおいて，点群とマルチ画像のラベリングに良い処理スピードと従来手法手法より優れた精度を得られた．ShapeNet part segmentationにおいて従来手法より優れた精度（クラスmIoU：83.7%）を得られた．</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1802.08275">論文</a></li></ul></div></div><div class="slide_index">[#347]</div><div class="timestamp">2018.4.13 10:29:26</div></div></section><section id="From_Lifestyle_Vlogs_to_Everyday_Interactions"><div class="paper-abstract"><div class="title">From Lifestyle Vlogs to Everyday Interactions</div><div class="info"><div class="authors">Fouhey et al.</div><div class="conference">CVPR 2018.</div><div class="paper_id">arXiv ID: 1712.02310</div></div><div class="item1"><h1>概要</h1><div class="text">従来のデータ取集手法（collection-by-acting）では難しいかった, バイアスの少ない, 多様で大規模な日常生活におけるインタラクションのデータベース Lifestyle VLOG dataset を公開した. </div></div><div class="item2"><img src="slides/figs/fukuhara-From-Lifestyle-Vlogs-to-Everyday-Interactions.png"></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>従来のデータセットが想定している陽的なデータ収集とは対照的に隠的なデータ収集方法を行うことで, バイアスを小さくすることに成功した.</li><li>ビデオに対してインタラクションのラベル, フレームに対してインタラクション時の手の状態のラベル付けられている.</li><li>従来のデータセットのBiasを分析するために, 従来のデータセットで訓練した手法が Lifestyle VLOG データセットに対しても上手く動作するか検証した.</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.02310.pdf" target="blank">[論文] From Lifestyle Vlogs to Everyday Interactions</a></li><li><a href="https://people.eecs.berkeley.edu/~dfouhey/2017/VLOG/index.html" target="blank">Project Page</a></li><li><a href="https://github.com/dfouhey/VLOGToolkit" target="blank">GitHub</a></li></ul></div></div><div class="slide_index">[#348]</div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="timestamp">2018.4.12.00:00:00</div></div></section><section id="Seeing_Voices_and_Hearing_Faces_Cross-modal_biometric_matching"><div class="paper-abstract"><div class="title">Seeing Voices and Hearing Faces: Cross-modal biometric matching</div><div class="info"><div class="authors">A. Nagrani et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><div class="text"><h1>概要</h1><p>ある音声と2人分の顔画像から，どちらの人物の声かを推定する課題と，ある顔画像と2人分の音声から，どちらの音声がその人物の声かを推定する課題の2つを解くという問題設定の研究．
異なるモダリティ間でのマッチングという課題ということ．
ある入力に対応するのがどちらの人物かという2クラス識別の問題設定として定式化．
この問題を解くために，3入力を扱う3-streamのネットワーク構造を持つモデルを提案．
音声もスペクトログラムの形式で画像のように扱い，顔画像，音声ともにConvolutionしていくモデル．
実験では80%程度の識別率を達成し，人と同等の結果が出ている．
二人分の選択肢の性別，国籍，年齢などが同じという設定にすると，60%程度の正答率になるが，こちらでは人 (57%) を上回る結果となっている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Seeing_Voices_and_Hearing_Faces_Cross-modal_biometric_matching.png" alt="Seeing_Voices_and_Hearing_Faces_Cross-modal_biometric_matching.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>人物の顔画像と音声の対応付けという新しい問題設定</li><li>人間レベルの高い精度を実現</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00326">論文 (arXiv)</a></li></ul></div></div><div class="slide_index">[#349]</div><div class="timestamp">2018.4.12 15:48:11</div></div></section><section id="Actor_and_Action_Video_Segmentation_from_a_Sentence"><div class="paper-abstract"><div class="title">Actor and Action Video Segmentation from a Sentence</div><div class="info"><div class="authors">Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, Cees G.M. Snoek</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>センテンスの入力から、行動者と行動（Actor and Action）を同時に特定する研究である。複数の同様の物体から特定の人物など、詳細な分類が必要になる。ここではFully-Convolutional（構造の全てが畳み込みで構成される）モデルを適用してセグメンテーションベースで出力を行うモデルを提案。図は提案モデルを示す。I3Dにより動画像のエンコーディング、自然言語側はWord2Vecの特徴をさらにCNNによりエンコーディング。その後、動画像・言語特徴を統合してDeconvを繰り返しセグメントを獲得していく。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1803ActorAction.png" alt="1803ActorAction"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>文章（と動画像）の入力から行動者と行動の位置を特定すべくセグメンテーションを実行するという問題を提起した。また、二つの有名なデータセット（A2D/J-HMDB）を拡張して7,500を超える自然言語表現を含むデータとした。同問題に対してはSoTA。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>CVxNLPの問題はここにも進出して来た。画像キャプションに限らず、この手の統合は進められるはず。</p><ul><li><a href="https://arxiv.org/pdf/1803.07485.pdf">論文</a></li></ul></div></div><div class="slide_index">[#350]</div><div class="timestamp">2018.3.24 12:47:10</div></div></section><section id="Alive_Caricature_from_2D_to_3D"><div class="paper-abstract"><div class="title">Alive Caricature from 2D to 3D</div><div class="info"><div class="authors">Qianyi Wu, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>2Dの似顔絵画像から3Dの似顔絵を作成するためのアルゴリズムの提案。似顔絵画像のテストデータとしてはカリカチュアを使用し、カリカチュア画像の3Dモデルとテクスチャ化された画像を生成する。データは、標準の3D顔の変形を座標系に配置(下図、 xは口の開き具合)し、金のオリジナルデータから線形結合によって白い顔を生成する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330AC2Dto3D_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>カリカチュアを集めたデータセットを作って学習するのではなく、標準の3D顔のデータセットから実装でき、アプリケーションの柔軟さを推している。</p><p>3DMMやFaceWareHouseなどの従来手法と比較して、形の歪みが少なく、従来のものよりも綺麗な3D顔の出力が可能。顔以外にも、概形の予測が可能なオブジェクトなら応用できる？</p><ul><li><a href="https://arxiv.org/pdf/1803.06802.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180330AC2Dto3D_2.jpg"></p></div></div><div class="slide_index">[#351]</div></div></section><section id="A_Minimalist_Approach_to_Type-Agnostic_Detection_of_Quadrics_in_Point_Clouds"><div class="paper-abstract"><div class="title">A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds</div><div class="info"><div class="authors">Tolga Birdal, Benjamin Busam, Nassir Navab, Slobodan Ilic, Peter Sturm</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>オクルージョンが発生している場合/複雑な環境下でも簡単な形状がポイントクラウドから検出できる枠組みを提案する。手法は3D楕円形状のフィッティング、3次元空間操作、4点取得により構成。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180324Quadrics.png" alt="180324Quadrics"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>タイプに依存しない3次元の二次曲面（楕円球形状）検出を点群の入力から行う手法を考案した。さらに、4点探索問題を3点探索にしてRANSACベースの手法で解を求めた。モデルベースのアプローチよりはフィッティングの性能がよいが、キーポイントベースの手法よりは劣る。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>曖昧な教示のみで3次元形状探索問題が解決できるようになる？</p><ul><li><a href="https://arxiv.org/pdf/1803.07191.pdf">論文</a></li></ul></div></div><div class="slide_index">[#352]</div><div class="timestamp">2018.3.24 13:04:44</div></div></section><section id="COCO-Stuff_Thing_and_Stuff_Classes_in_Context"><div class="paper-abstract"><div class="title">COCO-Stuff: Thing and Stuff Classes in Context</div><div class="info"><div class="authors">Holger Caesar, Jasper Uijlings, Vittorio Ferrari</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>MSCOCOデータセットに対してThing（もの）やStuff（材質）に関する追加アノテーションを行い、さらにコンテキスト情報も追加したCOCO-Stuffを提案した。このデータセットには主にシーンタイプ、そのものがどこに現れそうかという場所、物理的/材質的な属性などをアノテーションとして付与する。COCO2017をベースにして164Kに対して91カテゴリを付与し、スーパーピクセルを用いた効率的なアノテーションについてもトライした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180329COCOStuff.png" alt="180329COCOStuff"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>材質的なアノテーションは画像キャプションに対して重要であることを確認、相対的な位置関係などデータセットのリッチなアノテーションが重要であること、セマンティックセグメンテーションベースの方法により今回のアノテーションを簡易的に行えたこと、などを示した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>さらにリッチなアノテーションは今後重要になる。この論文ではスーパーピクセルという弱い知識を用い、人間のアノテーションと組み合わせることでボトムアップ・トップダウンを効果的かつ効率的に組み合わせてアノテーションを行っている点が素晴らしい。ラストオーサのVittorio Ferrariは機械と人の協調によるアノテーションが得意（なので、既存データセットへのよりリッチなアノテーションを早いペースで提案できる）。</p><ul><li><a href="https://arxiv.org/pdf/1612.03716v4.pdf">論文</a></li><li><a href="https://github.com/nightrome/cocostuff10k">GitHub</a></li></ul></div></div><div class="slide_index">[#353]</div><div class="timestamp">2018.3.29 13:59:43</div></div></section><section id="Context-aware_Synthesis_for_Video_Frame_Interpolation"><div class="paper-abstract"><div class="title">Context-aware Synthesis for Video Frame Interpolation</div><div class="info"><div class="authors">Simon Niklaus, Feng Liu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>入力フレームだけでなく、ピクセル単位の文脈情報を用いて、高品質の中間フレームを補間するためのコンテキスト認識手法の提案。まず、プレトレインモデルを使用して、入力フレームのピクセルごとのコンテキスト情報を抽出。オプティカルフローを使用して、双方向フローを推定し、入力フレームとそのコンテキストマップの両方をワープする。最後にコンテキストマップをsynthesis networkに入力し、補間フレームを生成。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401CaS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来のビデオフレーム補間アルゴリズムは、オプティカルフローまたはその変動を推定し、それを用いて2つのフレーム間の中間フレームを生成する。本手法では、 2つの入力フレーム間の双方向フローを推定し、コンテキスト認識という方式をとることで精度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>高品質のビデオフレーム補間実験において、従来を上回る性能。</p><ul><li><a href="https://arxiv.org/pdf/1803.10967.pdf">論文</a></li></ul></div></div><div class="slide_index">[#354]</div></div></section><section id="Deep_Depth_Completion_of_a_Single_RGB-D_Image"><div class="paper-abstract"><div class="title">Deep Depth Completion of a Single RGB-D Image</div><div class="info"><div class="authors">Yinda Zhang, Thomas Funkhouser</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>RGB画像から表面の法線とオクルージョン境界を予測し、 RGB-D画像と組み合わせて、欠けている奥行き情報を補完するDeep Depth Completionの提案。また、奥行き画像と対になったRGB-D画像のデータセットであるcompletion benchmark datasetを作成し、性能を評価。これは、低コストのRGB-Dカメラでキャプチャした画像と、高コストの深度センサで同時にキャプチャした画像で構成されている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401DDC.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>深度カメラは、光沢があり、明るく、透明で、遠い表面の深さを感知しないことが多い。 このような問題を解決するために、本手法ではRGB画像から得た情報と組み合わせて、 RGB-D画像の深度チャネルを完全なものにする。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>深さ修復および推定において従来よりも優れた性能。</p><ul><li><a href="https://arxiv.org/abs/1803.09326">論文</a></li><li><a href="http://deepcompletion.cs.princeton.edu/">Project webpage</a></li></ul></div></div><div class="slide_index">[#355]</div></div></section><section id="Detecting_and_Recognizing_Human-Object_Interactions"><div class="paper-abstract"><div class="title">Detecting and Recognizing Human-Object Interactions</div><div class="info"><div class="authors">Georgia Gkioxari, Ross Girshick, Piotr Dollár, Kaiming He</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人物検出と同時に人物行動やその物体とのインタラクションも含めて学習を行うモデルを提案する。本論文では物体候補の中でも特にインタラクションに関係ありそうな物体に特化して認識ができるようにする。さらに、検出された<human, verb, object>のペアを用いて学習する（図の場合には<human, cut, knnife>）。さらに、その他の行動（図の場合にはstand）を同時に推定することもできる。モデルはFaster R-CNNをベースとするが、物体検出（box, class）、行動推定（action, target）、インタラクション（action）を推定して誤差を計算する。さらに、推定した人物位置に対する対象物体の方向も確率的に計算することが可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180322HOI.png" alt="180322HOI"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>人間に特化した検出と行動推定の枠組みを提案した。V-COCO（Verbs in COCO）にて、相対的に26%精度が向上（31.8=>40.0）、HICO-DETデータセットにて27%相対的な精度向上が見られた。計算速度は135ms/imageであり、高速に計算が可能である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>単純な多タスク学習ではなく、人物に特化して対象物体の位置も確率的に推定しているところがGood。</p><ul><li><a href="https://arxiv.org/pdf/1704.07333.pdf">論文</a></li><li><a href="https://gkioxari.github.io/InteractNet/index.html">Project</a></li><li><a href="https://github.com/s-gupta/v-coco">Verbs in COCO DB</a></li></ul></div></div><div class="slide_index">[#356]</div><div class="timestamp">2018.3.22 19:55:34</div></div></section><section id="Discriminative_Learning_of_Latent_Features_for_Zero-Shot_Recognition"><div class="paper-abstract"><div class="title">Discriminative Learning of Latent Features for Zero-Shot Recognition</div><div class="info"><div class="authors">Minghui Yan Li, et al</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>Zero-shot learning(ZSL)における、視覚的および意味的インスタンスを別々に表現し学習するLatent Discriminative Features Learning(LDF)の提案。 (1)ズームネットワークにより差別的な領域を自動的に発見することができるネットワークの提案。(2)ユーザによって定義された属性と潜在属性の両方について、拡張空間における弁別的意味表現の学習。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330LDF.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>ZSLは、画像表現と意味表現の間の空間を学習することによって、見えない画像カテゴリを認識する。 既存の手法では、視覚と意味空間を合わせたマッピングマトリックスを学習することが中心的課題。提案手法では、差別的に学習するとうアプローチで識別精度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>2つのコンポーネントによって、互いに支援しながら学習することで最先端の精度に。</p><ul><li><a href="https://arxiv.org/pdf/1803.06731.pdf">論文</a></li></ul></div></div><div class="slide_index">[#357]</div></div></section><section id="Domain_Adaptive_Faster_R-CNN_for_Object_Detection_in_the_Wild"><div class="paper-abstract"><div class="title">Domain Adaptive Faster R-CNN for Object Detection in the Wild</div><div class="info"><div class="authors">Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, Luc Van Gool</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ドメイン変換について、ゲームなどのCG映像から実際の交通シーンに対応して物体検出を行うための学習方法を提案する。本論文では(i) 画像レベルのドメイン変換、(ii) インスタンス（ある物体）に対してのドメイン変換、の二種類の方法を提案し、整合性をとるように正規化する（図のConsistency Regularization; Global/Localな特徴変換を考慮）。ここで、物体検出はFaster R-CNNをベースとしてドメイン変換の手法も二種類（H-divergence、敵対的学習）用意する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314DomainFRCNN.png" alt="180314DomainFRCNN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>CGで学習し実環境における自動運転などで使えるドメイン変換の手法を提案した。実験はCityscapes, KITTI, SIM10Kなどで行い、ロバストな物体検出を実行することができた。例えばCityscapesとKITTIの相互ドメイン変換でベースラインのFaster R-CNNが30.2 (K->C)、53.5 (C->K)のところ、Domain Adaptive Faster R-CNNでは38.5 (K->C)、64.1 (C->K)であった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>データ収集は手動から自動の時代になって来た？データを手作業で集める時代からアルゴリズムを駆使して収集する時代へ移行。</p><ul><li><a href="https://arxiv.org/pdf/1803.03243.pdf">論文</a></li><li><a href="http://www.vision.ee.ethz.ch/~liwenw/">著者</a></li></ul></div></div><div class="slide_index">[#358]</div><div class="timestamp">2018.3.14 08:43:53</div></div></section><section id="Efficient_Interactive_Annotation_of_Segmentation_Datasets_with_Polygon-RNN"><div class="paper-abstract"><div class="title">Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++</div><div class="info"><div class="authors">David Acuna, Huan Ling, Amlan Kar, Sanja Fidler</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>Polygon-RNNのアイデアを踏襲し、ヒューマン・イン・ザ・ループを使って対話的にオブジェクトのポリゴンアノテーションの生成。また、新しいCNNエンコーダアーキテクチャの設計、強化学習によるモデルの効果的な学習、 Graph Neural Networkを使用した出力解像度の向上を行う。これらのアーキテクチャをPolygon-RNN ++と呼ぶ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180331PolygonRNN++_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>アノテーション作成時の負担を軽減。より正確にアノテーションを付加できるため、雑音の多いアノテーターに対しても頑健である。</p><p>高い汎化能力となり、既存のピクセルワイズメソッドよりも大幅に改善。ドメイン外のデータセットにも適応可能。</p><ul><li><a href="https://arxiv.org/pdf/1803.09693.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180331PolygonRNN++_2.jpg"></p></div></div><div class="slide_index">[#359]</div></div></section><section id="Egocentric_Basketball_Motion_Planning_from_a_Single_First-Person_Image"><div class="paper-abstract"><div class="title">Egocentric Basketball Motion Planning from a Single First-Person Image</div><div class="info"><div class="authors">Gedas Bertasius, Aaron Chan, Jianbo Shi</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>一人称視点の画像からゴールリングに到達するまでのバスケットボール選手の動線を生成する。本論文では3D位置や頭部方向も記録する。同タスクを実行するため、まずは画像空間から12Dのカメラ空間に投影を行うEgoCam CNNを学習。次に予測を行うCNN（Future CNN）を構築、さらに予測位置やゴールまでの位置が正確かどうかを検証するGoal Verifier CNNを用いることでより正確な推定を行うことができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180307EgoBasketball.png" alt="180307EgoBasketball"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>複数のネットワークの出力（ここではEgoCamCNNとFutureCNN）を検証するVerification Networkという考え方は面白い。他のネットワークの出力を、検証用のネットワークにより正すというのはあらゆる場面で用いることができる。RNN/LSTM/GANsなどよりも高度な推定ができることが判明した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>結果例は動画像を参照。未来予測・３次元投影などコンポーネントがDNNにより高度にできるようになってきたからできた研究。さらに検証用のネットワークを構築することで出力自体を操作している。</p><ul><li><a href="https://arxiv.org/pdf/1803.01413v1.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=wRRRl4QsUQg">YouTube</a></li></ul></div></div><div class="slide_index">[#360]</div><div class="timestamp">2018.3.7 09:04:15</div></div></section><section id="Fast_and_Accurate_Single_Image_Super-Resolution_via_Information_Distillation_Network"><div class="paper-abstract"><div class="title">Fast and Accurate Single Image Super-Resolution via Information Distillation Network</div><div class="info"><div class="authors">Zheng Hui, Xiumei Wang, Xinbo Gao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>元の低解像度画像から高解像度画像を再構築するための、深くてコンパクトなCNNを提案。提案モデルは、特徴抽出ブロック、積み重ね情報蒸留ブロック、再構成ブロックの3部構成。これにより、情報量が豊富かつ効率的に特徴を徐々に抽出できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180331FaASISR.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>CNNが超解像殿画像を扱うようになってきたが、ネットワークが増大するにつれて、計算上の複雑さとメモリ消費という問題が生じる。これらの問題を解決するためのコンパクトなCNN。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>PSNR、SSIM、IFCの4つのデータセットで検証し、精度向上を確認。デシジョンおよび圧縮アーチファクト低減などの他の画像修復問題にも応用可能？</p><ul><li><a href="https://arxiv.org/pdf/1803.08679.pdf">論文</a></li></ul></div></div><div class="slide_index">[#361]</div></div></section><section id="Future_Frame_Prediction_for_Anomaly_Detection_--_A_New_Baseline"><div class="paper-abstract"><div class="title">Future Frame Prediction for Anomaly Detection -- A New Baseline</div><div class="info"><div class="authors">Wen Liu, Weixin Luo, Dongze Lian, Shenghua Gao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>先の（未来の）フレーム予測と異常検知を同時に行う手法を提案する論文。予測したフレームと異常検知の正解値により誤差を計算して最適化を行う。図に本論文で提案するネットワークアーキテクチャの図を示す。U-Netにより画像予測やさらにオプティカルフロー推定を行い、RGB空間、オプティカルフロー空間にて誤差を計算しGANの枠組みでそれらがリアルかフェイクかを判定する。同フレームを用いて異常検知を実施する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180315PredictionAnomaly.png" alt="180315PredictionAnomaly"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来は現在フレームを入力として異常検知を行う手法は存在したが、未来フレームを予測して異常検知を行う枠組みは本論文による初めての試みである。異常値の正解値を与えることで画像予測にもフィードバックされるため、画像予測と異常検知の相互学習に良い影響を与える。オープンデータベースにてベンチマークした結果、何れもState-of-the-artな精度を達成。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>生成ベースで画像予測+X（Xは任意タスク）というものはSoTAが出せるくらいにはなってきた。</p><ul><li><a href="https://arxiv.org/pdf/1712.09867.pdf">論文</a></li><li><a href="https://github.com/StevenLiuWen/ano_pred_cvpr2018">Project</a></li></ul></div></div><div class="slide_index">[#362]</div><div class="timestamp">2018.3.15 09:04:03</div></div></section><section id="Guided_Labeling_using_Convolutional_Neural_Networks"><div class="paper-abstract"><div class="title">Guided Labeling using Convolutional Neural Networks</div><div class="info"><div class="authors">Sebastian Stabinger, et al. </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルの付いていないデータに対して、どの画像にラベルを付けてデータセットを構成すればよいかを判断するguided labelingの提案。ラベル付けを行う必要があるサンプルを見定めることで、データセットの量を大幅に減らすことができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180313GuidedLabeling.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>大規模データセットにおいて、手動でのラベル付けは大変。選別してラベル付けを行えば、作業を最小限に抑えられる。また、ある意味良いデータを選別できるため、場合によっては精度も向上。</p></div></div><div class="item4"><div class="text"><p>MNISTは、データセットのサイズを1/16に、CIFAR10は1/2に減らすことが可能に。また、MNISTの場合は、全部使った時よりも識別精度が向上した。普遍性を妨げる不必要なデータを取り除けたことが精度向上につながった？</p><ul><li><a href="https://arxiv.org/pdf/1712.02154.pdf">論文</a></li></ul></div></div><div class="slide_index">[#363]</div></div></section><section id="HATS_Histograms_of_Averaged_Time_Surfaces_for_Robust_Event-based_Object_Classification"><div class="paper-abstract"><div class="title">HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object Classification</div><div class="info"><div class="authors">Amos Sironi, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>イベントベースカメラにおける、識別アルゴリズムの提案。本研究では、(1)イベントベースのオブジェクト分類のための低レベル表現とアーキテクチャの欠如、(2)実世界における大きなイベントベースのデータセットの欠如、の2つの問題に取り組む。新しい機械学習アーキテクチャ、イベントベースの特徴表現(Histograms of Averaged Time Surfaces)、データセット(N-CARS)を提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330NCARS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>イベントベースのカメラは、従来のフレームベースのカメラと比較して、高時間分解能、低消費電力、高ダイナミックレンジという点で優れており、様々なシーンで応用が利く。しかし、イベントベースのオブジェクト分類アルゴリズムの精度は未だ低い。特徴表現には過去時間の情報を使用。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>過去の情報を使うことで、既存のイベントベースカメラによる認識手法よりも優れた結果となった。</p><ul><li><a href="https://arxiv.org/pdf/1803.07913.pdf">論文</a></li><li><a href="http://www.prophesee.ai/dataset-n-cars/">データセット</a></li></ul></div></div><div class="slide_index">[#364]</div></div></section><section id="Improving_Object_Localization_with_Fitness_NMS_and_Bounded_IoU_Loss"><div class="paper-abstract"><div class="title">Improving Object Localization with Fitness NMS and Bounded IoU Loss</div><div class="info"><div class="authors">Lachlan Tychsen-Smith, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>既存のNon-Max Supressionを改良したFitness NMSの提案。Soft NMSも同時に使用するとより効果的。</p><p>勾配降下法の収束特性(滑らかさ、堅牢性など)を維持しつつ、IoUを最大化するという目標により適した損失関数であるBounded IoU Loss の提案。これをRoIクラスタリングと組み合わせることで精度が向上する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314FitnessNMS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>バウンディングボックスのスコアを算出する関数を拡張する。具体的には、グランドトゥルースとのIoUと、クラスの期待値を追加する。これにより、IoUの重なり推定値と、クラス確率の両方が高いバウンディングボックスを優先して学習することができる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>MSCOCO、Titan X(Maxwell)使用時では、精度33.6％-79Hzまたは41.8％-5Hz。本論文ではDeNetでテストしたが、別の手法でも精度向上が望めるよう。</p><ul><li><a href="https://arxiv.org/pdf/1711.00164.pdf">論文</a></li><li><a href="https://github.com/lachlants/denet">ソースコード</a></li></ul></div></div><div class="slide_index">[#365]</div></div></section><section id="Independently_Recurrent_Neural_Network_IndRNN_Building_A_Longer_and_Deeper_RNN"><div class="paper-abstract"><div class="title">Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN</div><div class="info"><div class="authors">Shuai Li, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>新しいRNN手法であるindependently recurrent neural network (IndRNN)の提案。一枚のレイヤ内のニューロンが独立しており、レイヤ間で接続されている。これにより、勾配消失問題や爆発問題を防ぎ、より長期的なデータを学習することができる。また、IndRNNは複数積み重ねることができるため、既存のRNNよりも深いネットワークを構築できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314IndRNN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>本手法によって下記の従来手法の問題を解決。</p><p>RNNは、勾配の消失や爆発の問題、長期パターンの学習が困難である。LSTMやGRUは、上記のRNNの問題を解決すべく開発されたが、層の勾配が減衰してしまう問題がある。また、RNNは全てのニューロンが接続されているため、挙動の解釈が困難。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>かなり長いシーケンス(5000回以上の時間ステップ)を処理でき、かなり深いネットワーク（実験では21レイヤー）を構築できる。</p><ul><li><a href="https://arxiv.org/pdf/1803.04831.pdf">論文</a></li></ul></div></div><div class="slide_index">[#366]</div></div></section><section id="Iterative_Visual_Reasoning_Beyond_Convolutions"><div class="paper-abstract"><div class="title">Iterative Visual Reasoning Beyond Convolutions</div><div class="info"><div class="authors">Xinlei Chen, Li-Jia Li, Li Fei-Fei, Abhinav Gupta</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>CNNのような理由を突き止める能力がない認識システムを超えた、反復的なvisual reasoningのための新しいフレームワークの提案。畳み込みベースのローカルモジュールとグラフベースのグローバルモジュールの2コアで構成。2つのモジュールのを繰返し展開し、予測結果を相互にクロスフィードして絞り込む。最後に、両方のモジュールの最高値をアテンションベースのモジュールと組み合わせてプレディクト。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401IVRBC_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>ただ畳み込むだけでなく、Spatial(空間的)およびSemanticの空間を探索することができる。下図のように、「人」は「車」を運転するというSpatialとSemanticの双方を兼ね備えた認識を行うことで精度向上を図る。</p><p>通常のCNNと比較して、ADEで8.4％、COCOで3.7％の精度向上。</p><ul><li><a href="https://arxiv.org/pdf/1803.11189.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180401IVRBC_2.jpg"></p></div></div><div class="slide_index">[#367]</div></div></section><section id="LayoutNet_Reconstructing_the_3D_Room_Layout_from_a_Single_RGB_Image"><div class="paper-abstract"><div class="title">LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image</div><div class="info"><div class="authors">Chuhang Zou, Alex Colburn, Qi Shan, Derek Hoiem</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>単一のパースペクティブまたはパノラマ画像から屋内3Dルームレイアウトを推定するLayoutNetの提案。最初に、消失点を分析し、水平になるように画像を整列。これにより、壁と壁の境界が垂直になり、ノイズ低減。画像からコーナー(レイアウト接合点)と境界を、エンコーダ/デコーダ構造のCNNで出力。最後に、3D Layoutパラメータを、予測したコーナーと境界に適合するように最適化する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401LayoutNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>アーキテクチャはRoomNetと似ているが、消失点に基づいて画像を整列させ、複数のレイアウト要素（コーナー、境界線、サイズ、平行移動）を予測し、 “L”形の部屋のような非直方体のマンハッタンレイアウトに対しても適応できる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>従来手法と比較して、処理速度と正確さにおいて性能の向上。</p><ul><li><a href="https://arxiv.org/pdf/1803.08999.pdf">論文</a></li><li><a href="https://github.com/zouchuhang/LayoutNet">ソースコード</a></li></ul></div></div><div class="slide_index">[#368]</div></div></section><section id="Learning_to_Localize_Sound_Source_in_Visual_Scenes"><div class="paper-abstract"><div class="title">Learning to Localize Sound Source in Visual Scenes</div><div class="info"><div class="authors">Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, In So Kweon</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像と音声の入力から、音が画像のどこで鳴っているか（鳴りそうか？）を推定した研究。さらに、人の声なら人の領域、車の音なら車の領域にアテンションがあたるなど物体と音声の対応関係も学習することができる。学習には音源とその対応する物体の位置を対応づけたデータセット（144Kのペアが含まれるSound Source Localization Dataset）を準備した。さらに既存の物体認識と音声を対応づけて（？）Unsupervised/Semi-supervisedに学習することにも成功した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180322LocalizeSound.png" alt="180322LocalizeSound"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>教師あり、教師なし、半教師あり、いずれの枠組みでも音声ー物体の対応関係を学習することができるようにした。音源とそれに対応する物体領域の尤度がヒートマップにて高く表示されている。結果はビデオを参照されたい。教師なし学習はTriplet-lossにより構成され、ビデオと近い/遠い音声の誤差により計算。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>非常に面白い問題設定、プラス誤差関数を柔軟に抽出可能というところが上手。精読しても良いと感じた論文。CVにおいてビデオの音声は今まで使用しないことも多かった（もしくは精度向上のために活用していた）が、これからは使用方法を見直してもよいと感じた。</p><ul><li><a href="https://arxiv.org/pdf/1803.03849.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=UyairkbzR_Y">YouTube</a></li></ul></div></div><div class="slide_index">[#369]</div><div class="timestamp">2018.3.22 19:18:32</div></div></section><section id="Learning_to_Segment_Every_Thing"><div class="paper-abstract"><div class="title">Learning to Segment Every Thing</div><div class="info"><div class="authors">Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルが完全に手に入らない際にでも転移学習が可能なセグメンテーション手法（論文中ではPartially Supervised Training Paradigm, weight transfer functionを紹介）を提案する。条件として、bboxが手に入っている物体に対してセグメンテーション領域を学習可能。Mask R-CNNをベースとしているが、Weight Transfer Functionを追加、セグメントの重みを学習・推定して誤差計算と学習繰り返し。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180303SegmentEverything.png" alt="180303SegmentEverything"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Visual Genome Datasetから3,000の視覚的概念を獲得、MSCOCOから80のマスクアノテーションを獲得した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>弱教師付き学習が現実的な精度で動作するようになってきた？アノテーションはお金や知識があっても非常に大変なタスクであり、いかに減らすかという方向に研究が進められている。（What's next?ー弱教師/教師なしの先とは？）</p><ul><li><a href="https://arxiv.org/pdf/1711.10370.pdf">論文</a></li><li><a href="http://ronghanghu.com/">著者</a></li><li><a href="http://kaiminghe.com/">Kaiming He</a></li></ul></div></div><div class="slide_index">[#370]</div><div class="timestamp">2018.3.3 10:46:40</div></div></section><section id="MakeupGAN_Makeup_Transfer_via_Cycle-Consistent_Adversarial_Networks"><div class="paper-abstract"><div class="title">MakeupGAN: Makeup Transfer via Cycle-Consistent Adversarial Networks</div><div class="info"><div class="authors">Huiwen Chang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ソース画像のメイクをターゲット画像へ転写やメイクの除去をする研究。ターゲット画像とメイク済み画像の2枚を入力としメイクを転写するネットワークGとメイク済み画像らメイクを取り除くネットワークFを考え、2つのネットワークによって元の画像に戻るように学習していく。
その際、Fによってxに付与されたメイクがyのメイクと同じものであるかを評価するロスを加えることでメイクの特徴を捉える。
従来手法ではメイク転写・除去を独立した問題として考えていたが、この研究ではセットとして考えている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180408make.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Youtubeのメイクチュートリアルの動画から、1148枚のメイクなし画像と1044枚のメイクあり画像を収集。ユーザースタディによって2つの既存手法と比較し、提案手法が一番いいと答えた人が65.7％（2番目と答えた人が31.4％）
従来手法では肌の色や表情の違いがあると上手くいかないのに対し、ソースとターゲット間でこれらが違ってもうまく転写できる。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://research.adobe.com/project/makeupgan-makeup-transfer-via-cycle-consistent-adversarial-networks/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#371]</div><div class="timestamp">2018.4.8 01:45:44</div></div></section><section id="Motion-Appearance_Co-Memory_Networks_for_Video_Question_Answering"><div class="paper-abstract"><div class="title">Motion-Appearance Co-Memory Networks for Video Question Answering</div><div class="info"><div class="authors">Jiyang Gao, Runzhou Ge, Kan Chen, Ram Nevatia</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ビデオQAのための、 Dynamic Memory Network(DMN) のコンセプトに基づいたmotion-appearance comemory networkの提案。本研究の特徴は次の3つである。(1)アテンションを生成するために動きと外観情報の両方を手がかりとして利用する共メモリアテンションメカニズム。(2) multi-level contextual factを生成するための時間的conv-deconv network。(3)異なる質問に対して動的な時間表現を構成するdynamic fact ensemble method。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401MACoMN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>本手法は、次のようなvideo QA特有の属性に基づいている。(1)豊富な情報を含む長い画像シーケンスを扱う。(2)動き情報と出現情報を相互に関連付け、アテンションキューを他の情報に応用できる。(3)答えを推論するために必要なフレーム数は質問によって異なる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>TGIF-QAの4つのタスクすべてにおいて、最先端技術よりも優れている。</p><ul><li><a href="https://arxiv.org/pdf/1803.10906.pdf">論文</a></li></ul></div></div><div class="slide_index">[#372]</div></div></section><section id="Multi-Frame_Quality_Enhancement_for_Compressed_Video"><div class="paper-abstract"><div class="title">Multi-Frame Quality Enhancement for Compressed Video</div><div class="info"><div class="authors">Ren Yang, Mai Xu, Zulin Wang, Tianyi Li</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>圧縮した動画像に対して画質を向上させる研究。Peak Quality Frames (PQFs)を用いたSVMベースの手法やMulti-Frame CNN (MF-CNN)を提案。提案法により、圧縮動画における連続フレームからアーティファクトを補正するような働きが見られた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180324PQF.png" alt="180324PQF"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>動画の画質改善手法においてState-of-the-art。動画に対する画質改善の結果は図を参照。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.04680">論文</a></li><li><a href="https://github.com/ryangBUAA/MFQE">GitHub</a></li></ul></div></div><div class="slide_index">[#373]</div><div class="timestamp">2018.3.24 15:14:35</div></div></section><section id="Multi-Level_Factorisation_Net_for_Person_Re-Identification"><div class="paper-abstract"><div class="title">Multi-Level Factorisation Net for Person Re-Identification</div><div class="info"><div class="authors">Xiaobin Chang, Timothy M. Hospedales, Tao Xiang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>人間の視覚的外観を、人の手によるアノテーションなしかつ、複数のセマンティックレベルで識別因子に分解する Multi-Level Factorisation Net(MLFN)の提案。 MLFNは、複数のブロックで構成されており、各ブロックには、複数の因子モジュールと、各入力画像の内容を解釈するための因子選択モジュールが含まれている。 </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180331MLFN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>効果的なRe-IDを目指すには、高低のセマンティックレベルでの人の差別化かつ視界不変性をモデル化することである。 近年(2018)のdeep Re-IDモデルは、セマンティックレベルの特徴表現を学習するか、アノテーション付きデータが必要となる。MLFNではこれらを改善する。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>3つのRe-IDと、CIFAR-100の結果で最先端。</p><ul><li><a href="https://arxiv.org/pdf/1803.09132.pdf">論文</a></li></ul></div></div><div class="slide_index">[#374]</div></div></section><section id="Non-local_Neural_Networks"><div class="paper-abstract"><div class="title">Non-local Neural Networks </div><div class="info"><div class="authors">Xiaolong Wang et al.</div><div class="conference">CVPR 2018</div></div><div class="item1"><h1>概要</h1><div class="text">NLPなどで効果を発揮しているself-attentionを多次元に一般化し、2D/3DCNNに導入することで新たな「non-local block」を形成し、画像や動画での実験を行った。
行動認識＠Kineticsでは非常に高い精度を達成。Instance segmentationやkey point detectionなどのタスクでも汎用的に効果を発揮。
</div></div><div class="item2"><img src="slides/figs/non_local.png"></div><div class="item3"><h1>手法</h1><div class="text">位置jと位置iに依存してアテンションを出力する関数f(.)とjのみに依存する関数g(.)の積を入力位置jに関して和をとることによって位置iの出力値を決定する。
位置情報の保存、可変入力サイズ、などの性質を持ち、全結合、畳み込みを特殊な形として含む。またf(.)の定義の仕方によってはself-attentionと一致する。
f(.)は様々な形が提案されているが、種類によらず効果を発揮している。実際に使用する場合は図のような残差構造を使用している。

</div></div><div class="item4"><div class="text"><h1>コメント・リンク</h1><p>効果のインパクトがすごい。学習曲線からもうまくいっていることが明らか。C2Dに対してspace-timeにnon-local blockを適用すると3Dconvよりも時系列方向への拡大として効果があったのが興味深い。
結局残差を用いたnon-local blockを使用していたので、単純にnon-local layerのみでの性能もきになる。
位置情報の保存は重要でも、局所性はあまり重要ではなかったのかと感じられる。</p><ul><li><a href="https://arxiv.org/abs/1711.07971">論文</a></li></ul></div></div><div class="slide_index">[#375]</div><div class="slide_editor">Tomoyuki Suzuki</div></div></section><section id="Pose-Robust_Face_Recognition_via_Deep_Residual_Equivariant_Mapping"><div class="paper-abstract"><div class="title">Pose-Robust Face Recognition via Deep Residual Equivariant Mapping</div><div class="info"><div class="authors">Kaidi Cao, Yu Rong, Cheng Li, Xiaoou Tang, Chen Change Loy</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>横顔の認識精度を高めるためにDeep Residual EquivAriant Mapping (DREAM)の提案。正面と側面の顔間のマッピングを行うことで特徴空間を対応付ける。これにより、横顔を正面の姿勢に変換して認識を単純化。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180313DREAM_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・手法・リンク集</h1><p>正面と側面のトレーニング数の不均衡から、現代の顔認識モデルの多くは、正面と比べて横顔を処理するのが比較的貧弱。本手法は姿勢変動を伴う顔認識に限定されない顔認識が可能で、横顔のデータを増やさなくても精度向上。</p><p>上図より、DREAMをCNNに追加し、入力に残差を動的に追加。下図はマッピングによる姿勢変換の例。</p><ul><li><a href="https://arxiv.org/pdf/1803.00839.pdf">論文</a></li><li><a href="http://mmlab.ie.cuhk.edu.hk/projects/DREAM">ソースコード</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180313DREAM_2.jpg"></p></div></div><div class="slide_index">[#376]</div></div></section><section id="Pyramid_Stereo_Matching_Network"><div class="paper-abstract"><div class="title">Pyramid Stereo Matching Network</div><div class="info"><div class="authors">Jia-Ren Chang, Yong-Sheng Chen</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>空間ピラミッドプーリングと3D CNNの2つのモジュールから構成された、ステレオ画像対からの奥行き推定を行うPyramid Stereo Matching Network(PSMNet)の提案。空間ピラミッドプーリングは、異なるスケールおよび位置でコンテキストを集約し、コストボリュームを形成する。 3D CNNは、複数のhourglass networksを重ねて、コストボリュームを規則化することを学習。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330PSMN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>現在(2018)ではステレオ画像からの奥行き推定を、CNNの教師あり学習で解決されてきている。 コンテキスト情報を利用することで精度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>最先端の手法よりも優れている結果。</p><ul><li><a href="https://arxiv.org/pdf/1803.08669.pdf">論文</a></li><li><a href="https: //github.com/JiaRenChang/PSMNet">ソースコード</a></li></ul></div></div><div class="slide_index">[#377]</div></div></section><section id="Referring_Relationships"><div class="paper-abstract"><div class="title">Referring Relationships</div><div class="info"><div class="authors">Ranjay Krishna, Ines Chami, Michael Bernstein, Li Fei-Fei</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>referring relationshipsを利用して同カテゴリのエンティティ間の曖昧さを解消するタスクの提案。特徴抽出後、アテンションを生成。述語を使用することで、アテンションをシフトさせる。この述語シフトモジュールを介して、subjectとobjectの間でメッセージを反復的に渡すことで、2つのエンティティをローカライズ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401ReferringRelationships_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>画像中のエンティティ間の関係にはそれぞれ意味があり、画像の理解に役立つ。例えば、図のサッカーの試合の画像では、複数の人写っているが、それぞれは異なる関係を持っている。一人はボールを蹴っており、もう一人はゴールを守っている。 <person-kicking-ball>に着目すると、述語の”kick”を理解することにより、画像内のどの人物が”ball”を蹴っているのかを正しく識別する。</p><ul><li><a href="https://arxiv.org/pdf/1803.10362.pdf">論文</a></li><li><a href="https://github.com/StanfordVL/ReferringRelationships">ソースコード</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180401ReferringRelationships_2.jpg"></p></div></div><div class="slide_index">[#378]</div></div></section><section id="Rethinking_Feature_Distribution_for_Loss_Functions_in_Image_Classification"><div class="paper-abstract"><div class="title">Rethinking Feature Distribution for Loss Functions in Image Classification</div><div class="info"><div class="authors">Weitao Wan, Yuanyi Zhong, Tianpeng Li, Jiansheng Chen</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>本論文ではLarge-margin Gaussian Mixture (L-GM) Lossを提案して画像識別タスクに応用する。Softmax Lossとの違いは、学習セットにおけるディープ特徴の混合ガウス分布をフォローしつつ仮説を設定するところである。識別境界や尤度正則化においてL-GM Lossは非常に高いパフォーマンスを実現している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314LGM.png" alt="180314LGM"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>L-GM Lossは画像識別においてSoftmax Lossよりも精度が高いことはもちろん、特徴分布を考慮するため例えばAdversarial Examples（摂動ノイズ）などにおいても対応できる。MNIST, CIFAR, ImageNet, LFWにおける識別や摂動ノイズを加えた実験においても良好な性能を確かめた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Softmax Lossよりも有意に精度向上が見られている。導入が簡単なら取り入れて精度向上したい。</p><ul><li><a href="https://arxiv.org/pdf/1803.02988.pdf">論文</a></li></ul></div></div><div class="slide_index">[#379]</div><div class="timestamp">2018.3.14 11:04:45</div></div></section><section id="Robust_Depth_Estimation_from_Auto_Bracketed_Images"><div class="paper-abstract"><div class="title">Robust Depth Estimation from Auto Bracketed Images</div><div class="info"><div class="authors">Sunghoon Im, Hae-Gon Jeon, In So Kweon</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>HDRの画像の明るさを補正するためのブラケット撮影からの距離画像やカメラ姿勢を同時推定する手法を提案する論文。ブラケット撮影とは通常の露出撮影以外に意図的に「少し明るめの写真」と「少し暗めの写真」を同時に撮影。距離画像推定は幾何変換をResidual-flow Networkに統合したモデルにより行う。ここでは学習ベースのMulti-view stereo手法（Deep Multi-View Stereo; DMVS）を幾何推定（Structure-from-Small-Motion; SfSM）と組み合わせる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180323BracketedImages.png" alt="180323BracketedImages"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>距離画像推定において、スマートフォンやDSLRカメラなど種々のデータセットにてSoTAな精度を達成。モバイル環境でも動作するような小さなネットワークと処理速度についても同時に実現した。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.07702">論文</a></li><li><a href="https://sunghoonim.github.io/">著者</a></li></ul></div></div><div class="slide_index">[#380]</div><div class="timestamp">2018.3.23 19:11:04</div></div></section><section id="Rotation-Sensitive_Regression_for_Oriented_Scene_Text_Detection"><div class="paper-abstract"><div class="title">Rotation-Sensitive Regression for Oriented Scene Text Detection</div><div class="info"><div class="authors">Minghui Liao, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>自然画像から文字を検出する。単なる検出ではなく、文字の方向を考慮したバウンディングボックスによる検出手法であるRotation-sensitive Regression Detector (RRD)の提案。回帰ブランチによって、畳み込みフィルタを回転させて回転感知特徴を抽出。分類ブランチによって、回転感性特徴をプーリングすることによって回転不変特徴を抽出。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180329RRD.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>文字をテーマにした研究では(1)テキストの向きを無視した分類方法と，(2)向きを考慮したバウンディングボックスによる回帰がある。従来研究では、両方のタスクの共有の特徴を使用していたが、互換性がなかったためにパフォーマンスが低下(図b)。そこで、異なる2つのネットワークから抽出した、異なる特性の特徴を分類および回帰することを提案(図d,e)。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>ICDAR 2015、MSRA-TD500、RCTW-17およびCOCO-Textを含む3つのシーンテキストのデータセットで最先端のパフォーマンスを達成。向きがある一般物体検出にも応用可能？</p><ul><li><a href="https://arxiv.org/pdf/1803.05265.pdf">論文</a></li></ul></div></div><div class="slide_index">[#381]</div></div></section><section id="SketchMate_Deep_Hashing_for_Million-Scale_Human_Sketch_Retrieval"><div class="paper-abstract"><div class="title">SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval</div><div class="info"><div class="authors">Peng Xu, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>スケッチ検索のためのディープハッシングフレームワークの提案。3.8mの大規模スケッチデータセットを構築。CNNでスケッチの特徴抽出。RNNでペンストロークの時間情報をモデル化。CNN-RNNでエンコードすることで、スケッチ性質に対応した新しいhashing lossを導入。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180408SkechMate.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・差分</h1><p>従来のスケッチ認識タスクに従う代わりに、より困難な問題のスケッチハッシュ検索を行う。ネットワークをスケッチ認識のために再利用することもでき、どちらも高パフォーマンス。大規模なデータセットを利用することで、従来の文献ではあまり研究されていなかった、スケッチのユニークな特性を見出す。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.01401.pdf">論文</a></li><li><a href="http://sketchx.eecs.qmul.ac.uk/downloads/">ソースコード/データセット</a></li></ul></div></div><div class="slide_index">[#382]</div></div></section><section id="Style_Aggregated_Network_for_Facial_Landmark_Detection"><div class="paper-abstract"><div class="title">Style Aggregated Network for Facial Landmark Detection</div><div class="info"><div class="authors">Xuanyi Dong, Yan Yan, Wanli Ouyang, Yi Yang, University of Technology Sydney, The University of Sydney</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>顔のランドマーク検出。顔そのもののばらつきの他に、グレースケールやカラー画像、明暗などの画像スタイルが変わっても同様に検出できるStyle Aggregated Network(SAN)の提案。まず、(1)入力画像をさまざまなスタイルに変換し、スタイルを集約し、(2)顔のランドマーク予測する。(2)は、元画像とスタイルを集約した特徴の両方を入力し、融合してカスケード式のヒートマップ予測を生成する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330SAN_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>結果・リンク集</h1><p>Flickr8kとFlickr30kを使った実験において、最先端モデルと同等かそれ以上の結果。より正確で、より多様なキャプション生成。</p><ul><li><a href="https://arxiv.org/pdf/1803.04108.pdf">論文</a></li><li><a href="https://github.com/D-X-Y/SAN">ソースコード</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180330SAN_2.jpg"></p></div></div><div class="slide_index">[#383]</div></div></section><section id="The_Unreasonable_Effectiveness_of_Deep_Features_as_a_Perceptual_Metric"><div class="paper-abstract"><div class="title">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</div><div class="info"><div class="authors">Richard Zhang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>2枚の画像の類似度を表す指標は数多く提案されているが、その類似度は必ずしも人間の知覚と一致していない。近年はDNNにより高次の特徴を得ることが可能となっており、人間の知覚に近づいている。
そこで、既存の類似度の評価尺度とDNNベースの類似度判定を比較することでDNNベースの手法がより人間の知覚に近い類似度を表現できることを確認した。
具体的には、ある画像を異なる方法で加工したもの2つを用意し、どちらが元の画像に近いかを人間とコンピュータ両方に判定させることで検証を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180408perception.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>データセットとして、画像に様々な加工を施したデータを人間に類似度を評価してもらったものを作成。加工の例としては、ノイズの付与やオートエンコーダによる画像の復元などが挙げられる。
検証の結果、ＤＮＮベースの類似度の方が既存の尺度より人間の知覚に乗っ取ってることを示した。
また、DNNのネットワーク構造そのものは重要ではないことが分かった。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://richzhang.github.io/PerceptualSimilarity/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#384]</div><div class="timestamp">2018.4.8 01:36:55</div></div></section><section id="TOM-Net_Learning_Transparent_Object_Matting_from_a_Single_Image"><div class="paper-abstract"><div class="title">TOM-Net: Learning Transparent Object Matting from a Single Image</div><div class="info"><div class="authors">Guanying Chen, Kai Han, Kwan-Yee K. Wong</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>透明物体の切り抜き（Transparent Object Matting; TOM）と反射特性を推定することが可能なネットワークTOM-Netを提案する。TOM-Netにより、物体の反射特性を保存しながら他の画像にレンダリングして、同画像のテクスチャを反映させることができる。同問題を反射フローの推定問題と捉えてDNNのモデルを構築することで解決した。荒い部分は多階層のEncoder-Decorderで推定し、詳細な部分はResidualNetで調整する。この問題を解決するために、データセットを構築した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180324TOMNet.png" alt="180324TOMNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>178Kの画像を含むデータセットを構築した。同DBには876サンプル、14の透明物体、60種の背景を含む。透明物体の推定と反射特性のレンダリングはGitHubページを参照。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.04636.pdf">論文</a></li><li><a href="http://gychen.org/TOM-Net/">Project</a></li><li><a href="https://github.com/guanyingc/TOM-Net_Rendering">GitHub</a></li></ul></div></div><div class="slide_index">[#385]</div><div class="timestamp">2018.3.24 18:05:46</div></div></section><section id="Towards_Human-Machine_CooperationSelf-supervised_Sample_Mining_for_Object_Detection"><div class="paper-abstract"><div class="title">Towards Human-Machine Cooperation:Self-supervised Sample Mining for Object Detection</div><div class="info"><div class="authors">Keze Wang, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>物体検出の課題を考慮し、既存のActive Learning(AL)の欠点を改善することを目的とした、Self-Supervised Sample Mining(SSM)の提案。ラベルなし、もしくは一部ラベルのないデータを使って学習することができる。交差検証後のスコアによってサンプルを選別。低い場合にはユーザによってアノテーション、高い場合にはそのままラベルとして採用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330SSM.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>既存のAL法では主に、単一の画像コンテクスト内でサンプル選択基準を定義し、大規模な物体検出において最適ではなく、頑強性および非実用的である。SSMによって、ユーザが必要な部分にだけ介入し、アノテーションの作業を軽減。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>アノテーションが少ないデータセットにおいても最先端の精度。</p><ul><li><a href="https://arxiv.org/pdf/1803.09867.pdf">論文</a></li></ul></div></div><div class="slide_index">[#386]</div></div></section><section id="Towards_Open-Set_Identity_Preserving_Face_Synthesis"><div class="paper-abstract"><div class="title">Towards Open-Set Identity Preserving Face Synthesis</div><div class="info"><div class="authors">Jianmin Bao, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>顔画像からidentityとattributesを別々に再構成する、GANに基づいたOpen-Set Identity Generating Adversarial Networkの提案。 face synthesis networkは、ポーズや感情、照明、背景などをキャプチャする属性ベクトルを抽出することができる。図中の2つの入力画像AおよびBから抽出された識別を再結合することによって、A0およびB0を生成することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401OSIPFS_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>顔の正面化、顔属性モーフィング、 face adversarial example detectionなど、より広範なアプリケーションに応用可能。</p><ul><li><a href="https://arxiv.org/pdf/1803.11182.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180401OSIPFS_2.jpg"></p></div></div><div class="slide_index">[#387]</div></div></section><section id="Towards_Universal_Representation_for_Unseen_Action_Recognition"><div class="paper-abstract"><div class="title">Towards Universal Representation for Unseen Action Recognition</div><div class="info"><div class="authors">Yi Zhu, Yang Long, Yu Guan, Shawn Newsam, Ling Shao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>学習画像がなくても行動認識を実現する「Unseen Action Recognition (UAR)」についての研究。UARの問題をMIL（Multiple Instance Learning）の一般化（GMIL）として扱い、ActivityNetなど大規模動画データから分布推定して表現を獲得。図は提案手法であるCross-Domain UAR (CD-UAR)である。ビデオから抽出したDeep特徴はGMILによりカーネル化される。Word2Vecとの投稿によりURを獲得し、ドメイン変換により新しい概念を獲得する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180323UAR.png" alt="180323UAR"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来法では見た/見てないの対応関係をデータセット中に含ませていたが、本論文での提案はUniversal Representation（ユニバーサル表現）を獲得して同タスクを解決する。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.08460.pdf">論文</a></li></ul></div></div><div class="slide_index">[#388]</div><div class="timestamp">2018.3.23 19:40:06</div></div></section><section id="Unsupervised_Cross-dataset_Person_Re-identification_by_Transfer_Learning_of_Spatial-Temporal_Patterns"><div class="paper-abstract"><div class="title">Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns</div><div class="info"><div class="authors">Jianming Lv, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>歩行者の時空間パターンを用いた、教師なし学習の人物再同定アルゴリズムであるTFusionを提案。既存の人物再同定アルゴリズムのほとんどは、小サイズのラベル付きデータセットを用いた教師付き学習手法である。そのため、大規模な実世界のカメラネットワークに適応することは困難である。また、そこで、ラベルなしデータセットも用いたクロスデータセット手法によって精度向上を図る。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330TFusion.jpg"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>まず、歩行者の空間的-時間的パターンを学習するために、ラベル付きデータセットを用いて学習した視覚的分類器を、ラベルなしデータセットに転送。次に、Bayesian fusion modelによって、学習された時空間パターンを視覚的特徴と組み合わせて、分類器を改善。最後に、ラベルのないデータを用いて分類器を段階的に最適化。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>人物再同定のための、教師なしクロスデータセット学習手法の中では最先端。</p><ul><li><a href="https://arxiv.org/pdf/1803.07293.pdf">論文</a></li></ul></div></div><div class="slide_index">[#389]</div></div></section><section id="Unsupervised_Cross-dataset_Person_Re-identification_by_Transfer_Learning_of_Spatio-temporal_Patterns"><div class="paper-abstract"><div class="title">Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatio-temporal Patterns</div><div class="info"><div class="authors">Jianming, Lv and Weihang, Chen and Qing, Li and Can, Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルなし、ドメインが異なる環境に対して人物再同定を行う手法を提案する。モデルであるTFusionは４ステップにより構築（１）教師あり学習により識別器を構築（２）ターゲットであるラベルなしデータにより時空間特徴パターン（Spatio-temporal Pattern）を学習（３）統合モデルFを学習（４）ラベルなしのターゲットデータにて徐々に識別器を学習する（１〜４は図に示されている）。Bayesian Fusionを提案して、時空間特徴パターンと人物のアピアランス特徴を統合してドメイン変換を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180323CDReID.png" alt="180323CDReID"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来の人物再同定の設定では比較的小さいデータセットであり、完全に教師ありの環境を想定していたが、本論文ではラベルなし、ドメインが異なる環境に対して人物再同定を実行するため、非常に難しい問題となる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.07293.pdf">論文</a></li><li><a href="https://github.com/ahangchen/TFusion">GitHub</a></li></ul></div></div><div class="slide_index">[#390]</div><div class="timestamp">2018.3.23 20:37:22</div></div></section><section id="Unsupervised_Textual_Grounding_Linking_Words_to_Image_Concepts"><div class="paper-abstract"><div class="title">Unsupervised Textual Grounding: Linking Words to Image Concepts</div><div class="info"><div class="authors">Raymond A. Yeh, Minh N. Do, Alexander G. Schwing</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>単語を検出された画像の概念に関連付けるための、仮説検定を用いた教師なしTextual grounding手法の提案。ネットワークにはVGG-16を採用し、画像内のオブジェクト/単語の空間情報やクラス情報、およびクラス外の新しい概念を学習できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401UTG.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>Textual grounding、すなわち画像内のオブジェクトと単語をリンクさせる既存の技法は、教師付きのディープラーニングとして定式化されており、大規模なデータセットを用いてバウンディングボックスを推定する。しかし、データセットの構築には時間やコストがかかるので教師なしの手法を提案。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>ReferIt GameとFlickr30kを用いたベンチマークでそれぞれ7.98％と6.96％以上の精度。</p><ul><li><a href="https://arxiv.org/pdf/1803.11185.pdf">論文</a></li></ul></div></div><div class="slide_index">[#391]</div></div></section><section id="Vision-and-Language_Navigation_Interpreting_visually-grounded_navigation_instructions_in_real_environments"><div class="paper-abstract"><div class="title">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</div><div class="info"><div class="authors">Peter Anderson, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>自然言語のナビゲーションを入力として、実空間の中をロボットが動き目的地に到達できるかどうかを競うベンチマーク（Visually-grounded natural language navigation in real buildings）を提案。データセットは3Dのシミュレータによりキャプチャされ、22Kのナビゲーション、文章の平均単語数は29で構成される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180305R2RNavi.png" alt="180305R2RNavi"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>(1) Matterport3Dデータセットを強化学習を行えるように拡張。(2) 同タスクが行えるようなベンチマークであるRoom-to-Room (R2R)を提案して言語と視覚情報から実空間にてナビができるようにした。(3) seq-to-seqをベースとしたニューラルネットによりベンチマークを構築。VQAをベースにしていて、ナビゲーション（VQAでいう質問文）と移動アクション（VQAでいう回答）という組み合わせで同問題を解決する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>自然言語の問題はキャプションや質問回答の枠を超えて実空間、さらにいうとロボットタスクに導入されつつある。この研究はビジョン側からのアプローチだが、ロボット側のアプローチが現在どこまでできているか気になる。すでに屋内環境をある程度自由に移動するロボットが実現しているとこの実現可能性が高くなる。SLAMとの組み合わせももう実行できるレベルにある？</p><ul><li><a href="https://arxiv.org/pdf/1711.07280.pdf">論文</a></li><li><a href="https://bringmeaspoon.org/">Project</a></li><li><a href="https://github.com/peteanderson80/Matterport3DSimulator">GitHub</a></li><li><a href="https://niessner.github.io/Matterport/">Matterport3D dataset</a></li></ul></div></div><div class="slide_index">[#392]</div><div class="timestamp">2018.3.5 19:53:46</div></div></section><section id="Weakly-Supervised_Action_Segmentation_with_Iterative_Soft_Boundary_Assignment"><div class="paper-abstract"><div class="title">Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment</div><div class="info"><div class="authors">Li Ding, Chenliang Xu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>時系列の行動検出/セグメンテーション（Action Segmentation）に関する問題をWeakly-Supervised（WS学習）に解いた。ここではTemporal Convolutional Feature Pyramid Network (TCFPN)とIterative Soft Boundary Assignment (ISBA)を繰り返すことで行動に関する条件学習ができてくるという仕組み。TCFPNではフレームの行動を予測し、ISBAではそれを検証、それらを繰り返して行動間の境界線を定めながらWS学習の教師としていく。さらに、WS学習を促進するためにより弱い境界として行動間の繋がりを定義することでWS学習の精度を向上させる。学習はビデオ単位の誤差を最適化することで境界についても徐々に定まる（ここがWS学習の所以）ように学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180329ISBATCFN.png" alt="180329ISBATCFN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Breakfast dataset, Hollywood extended datasetにて弱教師付き学習とテストを行いState-of-the-artな精度を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>弱い教師データを大量に集めると、そろそろ（ある程度の）教師ありデータによる精度を超えそう？もっと汎用的に学習できる枠組みが必要か。</p><ul><li><a href="https://arxiv.org/pdf/1803.10699v1.pdf">論文</a></li></ul></div></div><div class="slide_index">[#393]</div><div class="timestamp">2018.3.29 14:27:12</div></div></section><section id="Who_Let_The_Dogs_Out_Modeling_Dog_Behavior_From_Visual_Data"><div class="paper-abstract"><div class="title">Who Let The Dogs Out? Modeling Dog Behavior From Visual Data</div><div class="info"><div class="authors">Kiana Ehsani, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>犬視点の大規模ビデオデータセットを作成し、このデータを使用した、犬の行動や行動計画のモデル化。次の3つの問題に焦点を当てる。(1)犬の行動予測。(2)入力された画像対から犬のような行動計画を見出す。(3)例えば、歩行可能な表面推定などのタスクについて、学習された表現を利用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401DogsOut.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>視覚情報からintelligent agent(知的エージェント)を直接的にモデリングするタスク。犬の視覚情報を使うことで、行動をモデル化する斬新な取り組み。得られたモデルをAIなどに応用する。特に、歩行可能な表面推定のタスクで良い結果となる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>様々なエージェントやシナリオで使用でき、ラベルがないにもかかわらず有用な情報を学習することが可能。今後は、モデルやデーセットの拡張に挑む。</p><ul><li><a href="https://arxiv.org/pdf/1803.10827.pdf">論文</a></li></ul></div></div><div class="slide_index">[#394]</div></div></section><section id="Zero-shot_Recognition_via_Semantic_Embeddings_and_Knowledge_Graphs"><div class="paper-abstract"><div class="title">Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs</div><div class="info"><div class="authors">Xiaolong Wang, Yufei Ye, Abhinav Gupta, The Robotics Institute, Carnegie Mellon University</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>カテゴリの単語の埋め込みと他のカテゴリとの関係(視覚データが提供される)を使用するだけで、学習例がないカテゴリの分類器を学習するゼロショット認識モデルを提案。 knowledge graph (KG) を入力とし、Graph Convolutional Network(GCN)を基に、セマンティック埋め込みとカテゴリの関係の両方を使用して分類器を予測する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330KG.jpg"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>学習済のKGが与えられると、各ノードに対する意味的埋め込みとして入力を得る。一連のグラフ畳み込みの後、各カテゴリの視覚的分類器を予測する。トレーニング中に、カテゴリの視覚的分類器が与えられ、GCNパラメータを学習。テスト時に、これらのフィルタを使用して、見えないカテゴリの視覚的分類器を予測する。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>KGのノイズに対してロバストであり、最先端の精度。</p><ul><li><a href="https://arxiv.org/pdf/1803.08035.pdf">論文</a></li></ul></div></div><div class="slide_index">[#395]</div></div></section><section id="Zoom_and_Learn_Generalizing_Deep_Stereo_Matching_to_Novel_Domains"><div class="paper-abstract"><div class="title">Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains</div><div class="info"><div class="authors">Jiahao Pang, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>学習済みデータと新しいドメイン(ground-truthなし)の両方を用いて、ディープステレオマッチングを行うZoom and Lean(ZOLE)の提案。これにより，他のドメインに一般化できるプレトレインモデルを作成することができる。一般化に際する不具合を抑制しながらアップサンプリングを行う、反復最適化問題を定式化する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330ZOLE.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>ground-truthデータが不足しているため、CNNを用いたステレオマッチングでは学習済みステレオモデルを新規ドメインに一般化することが困難とされていた。CNN学習時のイテレーションごとに最適化していくイメージ。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>スマートフォンで収集したデータを従来の手法に入力すると、物体のエッジがぼやけてしまうが、提案手法のZOLEではこれらを改善できる。</p><ul><li><a href="https://arxiv.org/pdf/1803.06641.pdf">論文</a></li></ul></div></div><div class="slide_index">[#396]</div></div></section></div></div><script src="lib/js/head.min.js"></script><script src="js/reveal.js"></script><script>Reveal.initialize({
  history: true,
  center: false,
  width: '100%',
  height: '100%',
  transition: 'none',
  dependencies: [
    { src: 'plugin/markdown/marked.js' },
    { src: 'plugin/markdown/markdown.js' },
    { src: 'plugin/notes/notes.js', async: true },
    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
  ]
});</script></body></html>